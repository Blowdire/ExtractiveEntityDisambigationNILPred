{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:26<00:00, 378.77it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "basePath = \"./Datasets/zeshel/documents/\"\n",
    "\n",
    "\n",
    "def load_json(file_path):\n",
    "    data = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_corpuses():\n",
    "    corpuses = os.listdir(basePath)\n",
    "    corpusesDict = {}\n",
    "    for corpus in corpuses:\n",
    "        data = []\n",
    "        with open(basePath + corpus, \"r\", encoding=\"utf-8\") as file:\n",
    "            for line in file:\n",
    "                data.append(json.loads(line))\n",
    "        corpusesDict[corpus.replace(\".json\", \"\")] = data\n",
    "    return corpusesDict\n",
    "\n",
    "\n",
    "def findDoc(document_id_to_find, corpus):\n",
    "    matching_objects = [\n",
    "        obj for obj in corpus if obj[\"document_id\"] == document_id_to_find\n",
    "    ]\n",
    "    return matching_objects[0] if matching_objects else None\n",
    "\n",
    "\n",
    "def merge_data(train_data, corpus_data):\n",
    "    merged_dataset = []\n",
    "\n",
    "    for entry in tqdm(train_data):\n",
    "        try:\n",
    "            corpuseName = entry[\"corpus\"]\n",
    "            corpusDoc = findDoc(entry[\"context_document_id\"], corpus_data[corpuseName])\n",
    "            entCorpusDoc = findDoc(entry[\"label_document_id\"], corpus_data[corpuseName])\n",
    "            startIdx = entry[\"start_index\"]\n",
    "            endIdx = entry[\"end_index\"] + 1\n",
    "            splittedText = corpusDoc[\"text\"].split(\" \")\n",
    "            n = 150  # Number of tokens to take on each side\n",
    "            entSurroundedText = \" \".join(\n",
    "                splittedText[max(0, startIdx - n) : startIdx]\n",
    "                + [\"[START_ENT]\"]\n",
    "                + splittedText[startIdx:endIdx]\n",
    "                + [\"[END_ENT]\"]\n",
    "                + splittedText[endIdx : endIdx + n]\n",
    "            )\n",
    "            # entSurroundedText = (\n",
    "            #     corpusDoc[\"text\"][:startIdx]\n",
    "            #     + \"[START_ENT]\"\n",
    "            #     + corpusDoc[\"text\"][startIdx:endIdx]\n",
    "            #     + \"[END_ENT]\"\n",
    "            #     + corpusDoc[\"text\"][endIdx:]\n",
    "            # )\n",
    "            merged_entry = {\n",
    "                \"id\": len(merged_dataset),\n",
    "                \"input\": entSurroundedText,\n",
    "                \"output\": [\n",
    "                    {\n",
    "                        \"answer\": entCorpusDoc[\"title\"],\n",
    "                        \"provenance\": [{\"title\": entCorpusDoc[\"title\"]}],\n",
    "                    }\n",
    "                ],\n",
    "                \"meta\": {\n",
    "                    \"left_context\": \" \".join(\n",
    "                        splittedText[max(0, startIdx - n) : startIdx]\n",
    "                    ),\n",
    "                    \"right_context\": \" \".join(splittedText[endIdx : endIdx + n]),\n",
    "                    \"mention\": \" \".join(splittedText[startIdx:endIdx]),\n",
    "                },\n",
    "                \"candidates\": [],\n",
    "                \"answer\": entCorpusDoc[\"text\"],\n",
    "            }\n",
    "\n",
    "            merged_dataset.append(merged_entry)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    return merged_dataset\n",
    "\n",
    "\n",
    "corpuses = load_corpuses()\n",
    "# Load data from JSON files\n",
    "train_data = load_json(\"./Datasets/zeshel/mentions/test.json\")\n",
    "# corpus_data = [load_json(entry[\"corpus\"] + \".json\") for entry in train_data]\n",
    "\n",
    "# # Merge data\n",
    "merged_dataset = merge_data(train_data, corpuses)\n",
    "\n",
    "# # Save merged dataset to a new JSON file\n",
    "with open(\"./zeshel-conv.json\", \"w\", encoding=\"utf-8\") as output_file:\n",
    "    json.dump(merged_dataset, output_file, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing NIL For zero shot without NIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def load_json(file_path):\n",
    "    data = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "ds = load_json(\"./zeshel-blink.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "785"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = []\n",
    "for item in ds:\n",
    "    answer= item[\"output\"][0][\"answer\"]\n",
    "    if answer != \"Not In Candidates\":\n",
    "        filtered.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./zeshel-blink-noNIL.jsonl\", \"w\", encoding=\"utf-8\") as output_file:\n",
    "    #write data in lines to jsonl file\n",
    "    for entry in filtered:\n",
    "        json.dump(entry, output_file)\n",
    "        output_file.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
