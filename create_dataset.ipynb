{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b17588a",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14ca09f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desired percentage of NIL mentions\n",
    "perc_nil = 0.7\n",
    "\n",
    "# Number of batch to create\n",
    "n_batch = 10\n",
    "\n",
    "# Desired NIL mentions per batch (in dev and test)\n",
    "desired_nil = 20\n",
    "\n",
    "# Desired size of a single dev and test batch\n",
    "dev_test_desired_batch_size = 100\n",
    "\n",
    "# Random state\n",
    "random_state = 1234\n",
    "\n",
    "# Output dir\n",
    "outdir = 'incremental_dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a36413",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "859e39d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm,trange\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885f0b51",
   "metadata": {},
   "source": [
    "# Download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c073b3",
   "metadata": {},
   "source": [
    "Download the original dataset from https://github.com/yasumasaonoe/ET4EL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5982edb1",
   "metadata": {},
   "source": [
    "# Extract it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8898c8",
   "metadata": {},
   "source": [
    "```\n",
    "tar -xzf unseen_mentions.tar.gz\n",
    "```\n",
    "\n",
    "You should be able to see a folder named `unseen_mentions` with the following content:\n",
    "```\n",
    "unseen_mentions/\n",
    "unseen_mentions/dev.json\n",
    "unseen_mentions/test.json\n",
    "unseen_mentions/train\n",
    "unseen_mentions/train/train_2.json\n",
    "unseen_mentions/train/train_1.json\n",
    "unseen_mentions/train/train_4.json\n",
    "unseen_mentions/train/train_3.json\n",
    "unseen_mentions/train/train_5.json\n",
    "unseen_mentions/train/train_0.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c82f0d0",
   "metadata": {},
   "source": [
    "# Load Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d2b5794",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"./msnbc-test-kilt.jsonl\", lines=True)\n",
    "df[\"answer\"] = df[\"output\"].apply(lambda x: x[0][\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da2361f",
   "metadata": {},
   "source": [
    "# Proceed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d47495",
   "metadata": {},
   "source": [
    "### Calculate the absolute frequency with which each entity is mentioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82110f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "mention_frequency = df.groupby(\"answer\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1890f190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "answer\n",
       "'N Sync                                    2\n",
       "Al Arabiya                                 1\n",
       "Al Jazeera                                 2\n",
       "Alabama                                    6\n",
       "Alabama Crimson Tide                       4\n",
       "                                          ..\n",
       "Women's National Basketball Association    2\n",
       "World Trade Center                         1\n",
       "Wyoming                                    1\n",
       "Xinhua News Agency                         1\n",
       "Yamuna                                     1\n",
       "Length: 288, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mention_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4c31b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(288, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_df = pd.DataFrame(mention_frequency)\n",
    "freq_df.columns = ['freq']\n",
    "freq_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ee70a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>'N Sync</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Al Arabiya</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Al Jazeera</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alabama</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alabama Crimson Tide</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      freq\n",
       "answer                    \n",
       "'N Sync                  2\n",
       "Al Arabiya               1\n",
       "Al Jazeera               2\n",
       "Alabama                  6\n",
       "Alabama Crimson Tide     4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b1b7d0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the median frequency with which all the entities are mentioned\n",
    "med_freq = np.median(freq_df['freq'])\n",
    "med_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed4d9b1",
   "metadata": {},
   "source": [
    "Calculate the probability that each entity is NIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae1fcb31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>freq</th>\n",
       "      <th>p_formula</th>\n",
       "      <th>p_uniform</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>'N Sync</th>\n",
       "      <td>2</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.191519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Al Arabiya</th>\n",
       "      <td>1</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.622109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Al Jazeera</th>\n",
       "      <td>2</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.437728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alabama</th>\n",
       "      <td>6</td>\n",
       "      <td>0.117649</td>\n",
       "      <td>0.785359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alabama Crimson Tide</th>\n",
       "      <td>4</td>\n",
       "      <td>0.240100</td>\n",
       "      <td>0.779976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      freq  p_formula  p_uniform\n",
       "answer                                          \n",
       "'N Sync                  2   0.490000   0.191519\n",
       "Al Arabiya               1   0.700000   0.622109\n",
       "Al Jazeera               2   0.490000   0.437728\n",
       "Alabama                  6   0.117649   0.785359\n",
       "Alabama Crimson Tide     4   0.240100   0.779976"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(random_state)\n",
    "\n",
    "freq_df['p_formula'] = perc_nil ** (freq_df['freq'] / med_freq)\n",
    "s = np.random.uniform(0, 1, freq_df.shape[0])\n",
    "freq_df['p_uniform'] = s\n",
    "freq_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf54768e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>freq</th>\n",
       "      <th>p_formula</th>\n",
       "      <th>p_uniform</th>\n",
       "      <th>NIL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>'N Sync</th>\n",
       "      <td>2</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.191519</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Al Arabiya</th>\n",
       "      <td>1</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.622109</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Al Jazeera</th>\n",
       "      <td>2</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.437728</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alabama</th>\n",
       "      <td>6</td>\n",
       "      <td>0.117649</td>\n",
       "      <td>0.785359</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alabama Crimson Tide</th>\n",
       "      <td>4</td>\n",
       "      <td>0.240100</td>\n",
       "      <td>0.779976</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      freq  p_formula  p_uniform    NIL\n",
       "answer                                                 \n",
       "'N Sync                  2   0.490000   0.191519   True\n",
       "Al Arabiya               1   0.700000   0.622109   True\n",
       "Al Jazeera               2   0.490000   0.437728   True\n",
       "Alabama                  6   0.117649   0.785359  False\n",
       "Alabama Crimson Tide     4   0.240100   0.779976  False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_df['NIL'] = freq_df['p_uniform'] < freq_df['p_formula']\n",
    "freq_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8dbfd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N# NIL: 164 percentage: 56.94444444444444 %\n"
     ]
    }
   ],
   "source": [
    "print('N# NIL:', freq_df.eval('NIL').sum(), 'percentage:', freq_df.eval('NIL').sum() / freq_df.shape[0] * 100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a651d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting NIL freq to 0 so that when we split in batches each batch has the same number of NILs\n",
    "freq_df.loc[freq_df['NIL'] == True, 'freq'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ad8732b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_merged = df.join(freq_df, how='left', on='answer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65c7ea80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>meta</th>\n",
       "      <th>candidates</th>\n",
       "      <th>answer</th>\n",
       "      <th>freq</th>\n",
       "      <th>p_formula</th>\n",
       "      <th>p_uniform</th>\n",
       "      <th>NIL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Stocks end back and forth day slightly higher ...</td>\n",
       "      <td>[{'answer': 'New York City', 'provenance': [{'...</td>\n",
       "      <td>{'left_context': 'Stocks end back and forth da...</td>\n",
       "      <td>[New York, New York City, New York (magazine),...</td>\n",
       "      <td>New York City</td>\n",
       "      <td>6</td>\n",
       "      <td>0.117649</td>\n",
       "      <td>0.734945</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Stocks end back and forth day slightly higher ...</td>\n",
       "      <td>[{'answer': 'New York Stock Exchange', 'proven...</td>\n",
       "      <td>{'left_context': 'Stocks end back and forth da...</td>\n",
       "      <td>[New York Stock Exchange, Wall Street bombing]</td>\n",
       "      <td>New York Stock Exchange</td>\n",
       "      <td>3</td>\n",
       "      <td>0.343000</td>\n",
       "      <td>0.801603</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Stocks end back and forth day slightly higher ...</td>\n",
       "      <td>[{'answer': 'NASDAQ', 'provenance': [{'title':...</td>\n",
       "      <td>{'left_context': 'Stocks end back and forth da...</td>\n",
       "      <td>[NASDAQ]</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>2</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.890557</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>day slightly higher NEW YORK Stocks finished a...</td>\n",
       "      <td>[{'answer': 'United States', 'provenance': [{'...</td>\n",
       "      <td>{'left_context': 'day slightly higher NEW YORK...</td>\n",
       "      <td>[United States, United States Reports, Billboa...</td>\n",
       "      <td>United States</td>\n",
       "      <td>14</td>\n",
       "      <td>0.006782</td>\n",
       "      <td>0.658873</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>dollar after three down days kept investors ap...</td>\n",
       "      <td>[{'answer': 'The Home Depot', 'provenance': [{...</td>\n",
       "      <td>{'left_context': 'dollar after three down days...</td>\n",
       "      <td>[The Home Depot]</td>\n",
       "      <td>The Home Depot</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>0.598237</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>down days kept investors appetite for stocks i...</td>\n",
       "      <td>[{'answer': 'Saks, Inc.', 'provenance': [{'tit...</td>\n",
       "      <td>{'left_context': 'down days kept investors app...</td>\n",
       "      <td>[Saks, Inc.]</td>\n",
       "      <td>Saks, Inc.</td>\n",
       "      <td>2</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.630314</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>investors appetite for stocks in check An eigh...</td>\n",
       "      <td>[{'answer': 'Target Corporation', 'provenance'...</td>\n",
       "      <td>{'left_context': 'investors appetite for stock...</td>\n",
       "      <td>[Target Corporation]</td>\n",
       "      <td>Target Corporation</td>\n",
       "      <td>2</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.901140</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>returns in stocks and commodities lifting shar...</td>\n",
       "      <td>[{'answer': 'ING Group', 'provenance': [{'titl...</td>\n",
       "      <td>{'left_context': 'returns in stocks and commod...</td>\n",
       "      <td>[ING Group]</td>\n",
       "      <td>ING Group</td>\n",
       "      <td>1</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.767117</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>commodities lifting share prices Trading volum...</td>\n",
       "      <td>[{'answer': 'New York City', 'provenance': [{'...</td>\n",
       "      <td>{'left_context': 'commodities lifting share pr...</td>\n",
       "      <td>[New York, New York City, New York (magazine),...</td>\n",
       "      <td>New York City</td>\n",
       "      <td>6</td>\n",
       "      <td>0.117649</td>\n",
       "      <td>0.734945</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>of the holiday shopping season Despite the dra...</td>\n",
       "      <td>[{'answer': 'Federal Reserve System', 'provena...</td>\n",
       "      <td>{'left_context': 'of the holiday shopping seas...</td>\n",
       "      <td>[Federal Reserve System, The Fed (Columbia new...</td>\n",
       "      <td>Federal Reserve System</td>\n",
       "      <td>0</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.123943</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>senior portfolio manager at ING Investment Man...</td>\n",
       "      <td>[{'answer': 'Thomson Reuters', 'provenance': [...</td>\n",
       "      <td>{'left_context': 'senior portfolio manager at ...</td>\n",
       "      <td>[Thomson Reuters, Reuters, West (publisher), I...</td>\n",
       "      <td>Thomson Reuters</td>\n",
       "      <td>0</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.058392</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>past eight months as investors anticipate a re...</td>\n",
       "      <td>[{'answer': 'United States Department of Labor...</td>\n",
       "      <td>{'left_context': 'past eight months as investo...</td>\n",
       "      <td>[United States Department of Labor, Ministry o...</td>\n",
       "      <td>United States Department of Labor</td>\n",
       "      <td>0</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.555653</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>as investors anticipate a recovery in the econ...</td>\n",
       "      <td>[{'answer': 'Producer price index', 'provenanc...</td>\n",
       "      <td>{'left_context': 'as investors anticipate a re...</td>\n",
       "      <td>[Producer price index, Producer Price Index (I...</td>\n",
       "      <td>Producer price index</td>\n",
       "      <td>1</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.945553</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>holidays A report on industrial production wei...</td>\n",
       "      <td>[{'answer': 'Oklahoma City', 'provenance': [{'...</td>\n",
       "      <td>{'left_context': 'holidays A report on industr...</td>\n",
       "      <td>[Oklahoma City, Oklahoma City University, Okla...</td>\n",
       "      <td>Oklahoma City</td>\n",
       "      <td>0</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.640880</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>factories mines and utilities rose 0 1 percent...</td>\n",
       "      <td>[{'answer': 'Dow Jones Industrial Average', 'p...</td>\n",
       "      <td>{'left_context': 'factories mines and utilitie...</td>\n",
       "      <td>[Dow Jones Industrial Average]</td>\n",
       "      <td>Dow Jones Industrial Average</td>\n",
       "      <td>3</td>\n",
       "      <td>0.343000</td>\n",
       "      <td>0.791964</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>of inflation remained muted a welcome sign for...</td>\n",
       "      <td>[{'answer': 'Dow Jones Industrial Average', 'p...</td>\n",
       "      <td>{'left_context': 'of inflation remained muted ...</td>\n",
       "      <td>[Dow, California, Bangor International Airport...</td>\n",
       "      <td>Dow Jones Industrial Average</td>\n",
       "      <td>3</td>\n",
       "      <td>0.343000</td>\n",
       "      <td>0.791964</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>October The 0 3 percent rise was smaller than ...</td>\n",
       "      <td>[{'answer': 'S&amp;P 500', 'provenance': [{'title'...</td>\n",
       "      <td>{'left_context': 'October The 0 3 percent rise...</td>\n",
       "      <td>[S&amp;P 500]</td>\n",
       "      <td>S&amp;P 500</td>\n",
       "      <td>1</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.760849</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>not an issue said Tim Courtney chief investmen...</td>\n",
       "      <td>[{'answer': 'NASDAQ', 'provenance': [{'title':...</td>\n",
       "      <td>{'left_context': 'not an issue said Tim Courtn...</td>\n",
       "      <td>[NASDAQ]</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>2</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.890557</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>remain low The Dow Jones industrial average ro...</td>\n",
       "      <td>[{'answer': 'New York Stock Exchange', 'proven...</td>\n",
       "      <td>{'left_context': 'remain low The Dow Jones ind...</td>\n",
       "      <td>[New York Stock Exchange, NYSE Euronext, Euron...</td>\n",
       "      <td>New York Stock Exchange</td>\n",
       "      <td>3</td>\n",
       "      <td>0.343000</td>\n",
       "      <td>0.801603</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>10 437 42 It was the ninth gain in 10 days for...</td>\n",
       "      <td>[{'answer': 'Dow Jones Industrial Average', 'p...</td>\n",
       "      <td>{'left_context': '10 437 42 It was the ninth g...</td>\n",
       "      <td>[Dow, California, Bangor International Airport...</td>\n",
       "      <td>Dow Jones Industrial Average</td>\n",
       "      <td>3</td>\n",
       "      <td>0.343000</td>\n",
       "      <td>0.791964</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                              input  \\\n",
       "0    0  Stocks end back and forth day slightly higher ...   \n",
       "1    1  Stocks end back and forth day slightly higher ...   \n",
       "2    2  Stocks end back and forth day slightly higher ...   \n",
       "3    3  day slightly higher NEW YORK Stocks finished a...   \n",
       "4    4  dollar after three down days kept investors ap...   \n",
       "5    5  down days kept investors appetite for stocks i...   \n",
       "6    6  investors appetite for stocks in check An eigh...   \n",
       "7    7  returns in stocks and commodities lifting shar...   \n",
       "8    8  commodities lifting share prices Trading volum...   \n",
       "9    9  of the holiday shopping season Despite the dra...   \n",
       "10  10  senior portfolio manager at ING Investment Man...   \n",
       "11  11  past eight months as investors anticipate a re...   \n",
       "12  12  as investors anticipate a recovery in the econ...   \n",
       "13  13  holidays A report on industrial production wei...   \n",
       "14  14  factories mines and utilities rose 0 1 percent...   \n",
       "15  15  of inflation remained muted a welcome sign for...   \n",
       "16  16  October The 0 3 percent rise was smaller than ...   \n",
       "17  17  not an issue said Tim Courtney chief investmen...   \n",
       "18  18  remain low The Dow Jones industrial average ro...   \n",
       "19  19  10 437 42 It was the ninth gain in 10 days for...   \n",
       "\n",
       "                                               output  \\\n",
       "0   [{'answer': 'New York City', 'provenance': [{'...   \n",
       "1   [{'answer': 'New York Stock Exchange', 'proven...   \n",
       "2   [{'answer': 'NASDAQ', 'provenance': [{'title':...   \n",
       "3   [{'answer': 'United States', 'provenance': [{'...   \n",
       "4   [{'answer': 'The Home Depot', 'provenance': [{...   \n",
       "5   [{'answer': 'Saks, Inc.', 'provenance': [{'tit...   \n",
       "6   [{'answer': 'Target Corporation', 'provenance'...   \n",
       "7   [{'answer': 'ING Group', 'provenance': [{'titl...   \n",
       "8   [{'answer': 'New York City', 'provenance': [{'...   \n",
       "9   [{'answer': 'Federal Reserve System', 'provena...   \n",
       "10  [{'answer': 'Thomson Reuters', 'provenance': [...   \n",
       "11  [{'answer': 'United States Department of Labor...   \n",
       "12  [{'answer': 'Producer price index', 'provenanc...   \n",
       "13  [{'answer': 'Oklahoma City', 'provenance': [{'...   \n",
       "14  [{'answer': 'Dow Jones Industrial Average', 'p...   \n",
       "15  [{'answer': 'Dow Jones Industrial Average', 'p...   \n",
       "16  [{'answer': 'S&P 500', 'provenance': [{'title'...   \n",
       "17  [{'answer': 'NASDAQ', 'provenance': [{'title':...   \n",
       "18  [{'answer': 'New York Stock Exchange', 'proven...   \n",
       "19  [{'answer': 'Dow Jones Industrial Average', 'p...   \n",
       "\n",
       "                                                 meta  \\\n",
       "0   {'left_context': 'Stocks end back and forth da...   \n",
       "1   {'left_context': 'Stocks end back and forth da...   \n",
       "2   {'left_context': 'Stocks end back and forth da...   \n",
       "3   {'left_context': 'day slightly higher NEW YORK...   \n",
       "4   {'left_context': 'dollar after three down days...   \n",
       "5   {'left_context': 'down days kept investors app...   \n",
       "6   {'left_context': 'investors appetite for stock...   \n",
       "7   {'left_context': 'returns in stocks and commod...   \n",
       "8   {'left_context': 'commodities lifting share pr...   \n",
       "9   {'left_context': 'of the holiday shopping seas...   \n",
       "10  {'left_context': 'senior portfolio manager at ...   \n",
       "11  {'left_context': 'past eight months as investo...   \n",
       "12  {'left_context': 'as investors anticipate a re...   \n",
       "13  {'left_context': 'holidays A report on industr...   \n",
       "14  {'left_context': 'factories mines and utilitie...   \n",
       "15  {'left_context': 'of inflation remained muted ...   \n",
       "16  {'left_context': 'October The 0 3 percent rise...   \n",
       "17  {'left_context': 'not an issue said Tim Courtn...   \n",
       "18  {'left_context': 'remain low The Dow Jones ind...   \n",
       "19  {'left_context': '10 437 42 It was the ninth g...   \n",
       "\n",
       "                                           candidates  \\\n",
       "0   [New York, New York City, New York (magazine),...   \n",
       "1      [New York Stock Exchange, Wall Street bombing]   \n",
       "2                                            [NASDAQ]   \n",
       "3   [United States, United States Reports, Billboa...   \n",
       "4                                    [The Home Depot]   \n",
       "5                                        [Saks, Inc.]   \n",
       "6                                [Target Corporation]   \n",
       "7                                         [ING Group]   \n",
       "8   [New York, New York City, New York (magazine),...   \n",
       "9   [Federal Reserve System, The Fed (Columbia new...   \n",
       "10  [Thomson Reuters, Reuters, West (publisher), I...   \n",
       "11  [United States Department of Labor, Ministry o...   \n",
       "12  [Producer price index, Producer Price Index (I...   \n",
       "13  [Oklahoma City, Oklahoma City University, Okla...   \n",
       "14                     [Dow Jones Industrial Average]   \n",
       "15  [Dow, California, Bangor International Airport...   \n",
       "16                                          [S&P 500]   \n",
       "17                                           [NASDAQ]   \n",
       "18  [New York Stock Exchange, NYSE Euronext, Euron...   \n",
       "19  [Dow, California, Bangor International Airport...   \n",
       "\n",
       "                               answer  freq  p_formula  p_uniform    NIL  \n",
       "0                       New York City     6   0.117649   0.734945  False  \n",
       "1             New York Stock Exchange     3   0.343000   0.801603  False  \n",
       "2                              NASDAQ     2   0.490000   0.890557  False  \n",
       "3                       United States    14   0.006782   0.658873  False  \n",
       "4                      The Home Depot    20   0.000798   0.598237  False  \n",
       "5                          Saks, Inc.     2   0.490000   0.630314  False  \n",
       "6                  Target Corporation     2   0.490000   0.901140  False  \n",
       "7                           ING Group     1   0.700000   0.767117  False  \n",
       "8                       New York City     6   0.117649   0.734945  False  \n",
       "9              Federal Reserve System     0   0.700000   0.123943   True  \n",
       "10                    Thomson Reuters     0   0.700000   0.058392   True  \n",
       "11  United States Department of Labor     0   0.700000   0.555653   True  \n",
       "12               Producer price index     1   0.700000   0.945553  False  \n",
       "13                      Oklahoma City     0   0.700000   0.640880   True  \n",
       "14       Dow Jones Industrial Average     3   0.343000   0.791964  False  \n",
       "15       Dow Jones Industrial Average     3   0.343000   0.791964  False  \n",
       "16                            S&P 500     1   0.700000   0.760849  False  \n",
       "17                             NASDAQ     2   0.490000   0.890557  False  \n",
       "18            New York Stock Exchange     3   0.343000   0.801603  False  \n",
       "19       Dow Jones Industrial Average     3   0.343000   0.791964  False  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_merged.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e905eb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of NIL mentions in train: 37.19512195121951\n"
     ]
    }
   ],
   "source": [
    "print('Percentage of NIL mentions in train:', train_df_merged.eval('NIL').sum() / train_df_merged.shape[0] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9fee4fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame()\n",
    "results = []\n",
    "for index, row in train_df_merged.iterrows():\n",
    "    if row[\"NIL\"] == True:\n",
    "        candidates = row[\"candidates\"]\n",
    "        filtered_cands = list(\n",
    "            filter(\n",
    "                lambda x: row[\"answer\"] not in x,\n",
    "                candidates,\n",
    "            )\n",
    "        )\n",
    "        row[\"candidates\"] = filtered_cands\n",
    "        row[\"output\"][0][\"answer\"] = \"Not In Candidates\"\n",
    "        results.append(row.to_dict())\n",
    "    else:\n",
    "        results.append(row.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d363c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to jsonl file\n",
    "with open(outdir + '/msnbc-test-kilt-nil.jsonl', 'w') as outfile:\n",
    "    for entry in results:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "31b71fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_merged.to_csv(os.path.join(outdir, 'msnbc_test_nilled_good.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f46ae9",
   "metadata": {},
   "source": [
    "# Propagate NIL mentions in test and dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d4e994",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df_merged = dev_df.join(freq_df, how='left', on='wikiId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e9783a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df_merged['p_formula'] = dev_df_merged['p_formula'].fillna(-1)\n",
    "dev_df_merged['p_uniform'] = dev_df_merged['p_uniform'].fillna(-1)\n",
    "dev_df_merged['NIL'] = dev_df_merged['NIL'].fillna(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafa0322",
   "metadata": {},
   "source": [
    "Compute entity frequencies in dev for a correct sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b022384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mention_frequency_dev = dev_df_merged.query('~NIL').groupby('wikiId').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91336d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mention_frequency_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deeb44b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_df_dev = pd.DataFrame(mention_frequency_dev)\n",
    "freq_df_dev.columns = ['freq']\n",
    "freq_df_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3647986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_df_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6850e900",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df_merged = dev_df_merged.drop(columns=['freq']).join(freq_df_dev, how='left', on='wikiId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153b45c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set NIL entities freq to 0 for stratifying the NIL class\n",
    "dev_df_merged.loc[dev_df_merged['NIL'], 'freq'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2289da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df_merged['freq'] = dev_df_merged['freq'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f24c2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not dev_df_merged['NIL'].isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5c2534",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('dev NIL N#:', dev_df_merged.eval('NIL').sum(), 'Percentage:', dev_df_merged.eval('NIL').sum() / dev_df_merged.shape[0] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825422ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_merged = test_df.join(freq_df, how='left', on='wikiId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051b6da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_merged['p_formula'] = test_df_merged['p_formula'].fillna(-1)\n",
    "test_df_merged['p_uniform'] = test_df_merged['p_uniform'].fillna(-1)\n",
    "test_df_merged['NIL'] = test_df_merged['NIL'].fillna(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3122b16",
   "metadata": {},
   "source": [
    "Compute entity frequencies in test for a correct sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c86357",
   "metadata": {},
   "outputs": [],
   "source": [
    "mention_frequency_test = test_df_merged.query('~NIL').groupby('wikiId').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b69f4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mention_frequency_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ee52d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_df_test = pd.DataFrame(mention_frequency_test)\n",
    "freq_df_test.columns = ['freq']\n",
    "freq_df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efdcd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dbe2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_merged = test_df_merged.drop(columns=['freq']).join(freq_df_test, how='left', on='wikiId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2af0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set NIL entities freq to 0 for stratifying the NIL class\n",
    "test_df_merged.loc[test_df_merged['NIL'], 'freq'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c306ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_merged['freq'] = test_df_merged['freq'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04136fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not test_df_merged['NIL'].isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c797eca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test NIL N#:', test_df_merged.eval('NIL').sum(), 'Percentage:', test_df_merged.eval('NIL').sum() / test_df_merged.shape[0] * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfe1203",
   "metadata": {},
   "source": [
    "# Transplant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaf69dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want 500 NIL in dev and test\n",
    "# I want 100k total in both dev and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4082c893",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unique = train_df[['word', 'wikiId']].drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36db344a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_unique = dev_df[['word', 'wikiId']].drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a34a5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_unique = test_df[['word', 'wikiId']].drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fbce08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure no duplicates\n",
    "assert not train_unique.duplicated().any()\n",
    "assert not dev_unique.duplicated().any()\n",
    "assert not test_unique.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde9161c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure unseen mentions holds\n",
    "assert not pd.concat([train_unique, dev_unique]).duplicated().any()\n",
    "assert not pd.concat([train_unique, test_unique]).duplicated().any()\n",
    "assert not pd.concat([dev_unique, test_unique]).duplicated().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60c91d6",
   "metadata": {},
   "source": [
    "## NIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780a2bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prendo n menzioni NIL dal train che non siano nel dev/test e ce le metto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518aaf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_nil = train_df_merged.query('NIL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2db8196",
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_nil_total = desired_nil * n_batch\n",
    "nil_to_add_dev = desired_nil_total - dev_df_merged.eval('NIL').sum()\n",
    "nil_to_add_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba18553",
   "metadata": {},
   "outputs": [],
   "source": [
    "nil_to_add_test = desired_nil_total - test_df_merged.eval('NIL').sum()\n",
    "nil_to_add_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6272f9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subset_sum_ge(df, sum_, random_state):\n",
    "    \"\"\"\n",
    "    Gets a subset whose sum is greater than sum_\n",
    "    \"\"\"\n",
    "    \n",
    "    #print(sum_)\n",
    "    \n",
    "    if df.shape[0] == 0:\n",
    "        return None\n",
    "    \n",
    "    # get a sample\n",
    "    item = df.sample(n=1, random_state=random_state)\n",
    "    random_state += 1\n",
    "    df = df.drop(item.index) # remove just extracted item\n",
    "    \n",
    "    subset = item.index\n",
    "\n",
    "    current_sum = item['count'].values[0]\n",
    "    while current_sum < sum_:\n",
    "        print(f'\\r{current_sum}/{sum_}',end='')\n",
    "        # get a sample\n",
    "        item = df.sample(n=1, random_state=random_state)\n",
    "        random_state += 1\n",
    "        df = df.drop(item.index) # remove just extracted item\n",
    "\n",
    "        current_sum += item['count'].values[0]\n",
    "        \n",
    "        subset = subset.union(item.index)\n",
    "    print()\n",
    "    return subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44de2e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_nil_unique_count = pd.DataFrame(\n",
    "    train_df_nil[['wikiId', 'word']].value_counts(), columns=['count']).reset_index()\n",
    "train_df_nil_unique_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d52a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_transplant_to_dev = get_subset_sum_ge(train_df_nil_unique_count, nil_to_add_dev, random_state)\n",
    "print(train_df_nil_unique_count.loc[sample_transplant_to_dev]['count'].sum(), '>=', nil_to_add_dev)\n",
    "assert train_df_nil_unique_count.loc[sample_transplant_to_dev]['count'].sum() >= nil_to_add_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6e637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove sample_transplant to dev before extracting a sample for test\n",
    "train_df_nil_unique_count_no_dev = train_df_nil_unique_count.drop(sample_transplant_to_dev)\n",
    "sample_transplant_to_test = get_subset_sum_ge(train_df_nil_unique_count_no_dev, nil_to_add_test, random_state+10)\n",
    "print(train_df_nil_unique_count_no_dev.loc[sample_transplant_to_test]['count'].sum(), '>=', nil_to_add_test)\n",
    "assert train_df_nil_unique_count_no_dev.loc[sample_transplant_to_test]['count'].sum() >= nil_to_add_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d7cfa5",
   "metadata": {},
   "source": [
    "## Transplant NIL to dev and test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93989b41",
   "metadata": {},
   "source": [
    "### Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9ff32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions_to_dev = train_df_nil.join(\n",
    "    train_df_nil_unique_count.loc[sample_transplant_to_dev].set_index(['wikiId', 'word']),\n",
    "    on=['wikiId', 'word'], how='inner')\n",
    "mentions_to_dev = mentions_to_dev.drop(columns=['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f76e43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df_transplant = pd.concat([dev_df_merged, mentions_to_dev], ignore_index=True)\n",
    "dev_df_transplant = dev_df_transplant.sample(frac=1, random_state=random_state) # randomize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c540975c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df_transplant.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ccbb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dev_df_transplant.eval('NIL').sum(), '>=', desired_nil_total)\n",
    "assert dev_df_transplant.eval('NIL').sum() >= desired_nil_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00320a01",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa24beed",
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions_to_test = train_df_nil.join(\n",
    "    train_df_nil_unique_count.loc[sample_transplant_to_test].set_index(['wikiId', 'word']),\n",
    "    on=['wikiId', 'word'], how='inner')\n",
    "mentions_to_test = mentions_to_test.drop(columns=['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228fc356",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_transplant = pd.concat([test_df_merged, mentions_to_test], ignore_index=True)\n",
    "test_df_transplant = test_df_transplant.sample(frac=1, random_state=random_state) # randomize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2936f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_transplant.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ba5c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_df_transplant.eval('NIL').sum(), '>=', desired_nil_total)\n",
    "assert test_df_transplant.eval('NIL').sum() >= desired_nil_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5377a67b",
   "metadata": {},
   "source": [
    "### Remove from train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed4748b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_transplant = train_df_merged.drop(mentions_to_dev.index.union(mentions_to_test.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb74d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unique_transplant = train_df_transplant[['word', 'wikiId']].drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68bdf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_unique_transplant = dev_df_transplant[['word', 'wikiId']].drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218e3c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_unique_transplant = test_df_transplant[['word', 'wikiId']].drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c66684d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure no duplicates\n",
    "assert not train_unique_transplant.duplicated().any()\n",
    "assert not dev_unique_transplant.duplicated().any()\n",
    "assert not test_unique_transplant.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a155314c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure unseen mentions holds\n",
    "assert not pd.concat([train_unique_transplant, dev_unique_transplant]).duplicated().any()\n",
    "assert not pd.concat([train_unique_transplant, test_unique_transplant]).duplicated().any()\n",
    "assert not pd.concat([dev_unique_transplant, test_unique_transplant]).duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af118754",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train', train_df_transplant.shape)\n",
    "print('Dev', dev_df_transplant.shape)\n",
    "print('Test', test_df_transplant.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b339808e",
   "metadata": {},
   "source": [
    "# Transplant not-NIL mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baab260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prendo n menzioni NIL dal train che non siano nel dev/test e ce le metto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c2e49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_not_nil = train_df_merged.query('~NIL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f5542b",
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_total = dev_test_desired_batch_size * n_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac66966",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_nil_to_add_dev = desired_total - dev_df_transplant.shape[0]\n",
    "not_nil_to_add_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f0bb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_nil_to_add_test = desired_total - test_df_transplant.shape[0]\n",
    "not_nil_to_add_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f69470",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_not_nil_unique_count = pd.DataFrame(\n",
    "    train_df_not_nil[['wikiId', 'word']].value_counts(), columns=['count']).reset_index()\n",
    "train_df_not_nil_unique_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a689ffd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_transplant_to_dev_not_nil = get_subset_sum_ge(train_df_not_nil_unique_count, not_nil_to_add_dev, random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b73b684",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df_not_nil_unique_count.loc[sample_transplant_to_dev_not_nil]['count'].sum(), \">=\", not_nil_to_add_dev)\n",
    "assert train_df_not_nil_unique_count.loc[sample_transplant_to_dev_not_nil]['count'].sum() >= not_nil_to_add_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de91012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove sample_transplant to dev before extracting a sample for test\n",
    "train_df_not_nil_unique_count_no_dev = train_df_not_nil_unique_count.drop(sample_transplant_to_dev_not_nil)\n",
    "sample_transplant_to_test_not_nil = get_subset_sum_ge(train_df_not_nil_unique_count_no_dev, not_nil_to_add_test, random_state+10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33df7972",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df_not_nil_unique_count_no_dev.loc[sample_transplant_to_test_not_nil]['count'].sum(), \">=\", not_nil_to_add_test)\n",
    "assert train_df_not_nil_unique_count_no_dev.loc[sample_transplant_to_test_not_nil]['count'].sum() >= not_nil_to_add_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d049444f",
   "metadata": {},
   "source": [
    "### Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bd675d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions_to_dev_not_nil = train_df_not_nil.join(\n",
    "    train_df_not_nil_unique_count.loc[sample_transplant_to_dev_not_nil].set_index(['wikiId', 'word']),\n",
    "    on=['wikiId', 'word'], how='inner')\n",
    "mentions_to_dev_not_nil = mentions_to_dev_not_nil.drop(columns=['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61dbf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not mentions_to_dev_not_nil.eval('NIL').any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbcaeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df_transplant_final = pd.concat([dev_df_transplant, mentions_to_dev_not_nil], ignore_index=True)\n",
    "dev_df_transplant_final = dev_df_transplant_final.sample(frac=1, random_state=random_state) # randomize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b7cadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df_transplant_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5001bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert dev_df_transplant_final.eval('NIL').sum() >= desired_nil_total\n",
    "assert dev_df_transplant_final.shape[0] >= desired_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4784a194",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250846ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions_to_test_not_nil = train_df_not_nil.join(\n",
    "    train_df_not_nil_unique_count.loc[sample_transplant_to_test_not_nil].set_index(['wikiId', 'word']),\n",
    "    on=['wikiId', 'word'], how='inner')\n",
    "mentions_to_test_not_nil = mentions_to_test_not_nil.drop(columns=['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d716f8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_transplant_final = pd.concat([test_df_transplant, mentions_to_test_not_nil], ignore_index=True)\n",
    "test_df_transplant_final = test_df_transplant_final.sample(frac=1, random_state=random_state) # randomize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fab6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_transplant_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5225ca66",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert test_df_transplant_final.eval('NIL').sum() >= desired_nil_total\n",
    "assert test_df_transplant_final.shape[0] >= desired_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171239ef",
   "metadata": {},
   "source": [
    "### Remove from train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31516335",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_transplant_final = train_df_transplant.drop(mentions_to_dev_not_nil.index.union(mentions_to_test_not_nil.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd23c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unique_transplant_final = train_df_transplant_final[['word', 'wikiId']].drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa2f2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_unique_transplant_final = dev_df_transplant_final[['word', 'wikiId']].drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2639993",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_unique_transplant_final = test_df_transplant_final[['word', 'wikiId']].drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d121c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure no duplicates\n",
    "assert not train_unique_transplant_final.duplicated().any()\n",
    "assert not dev_unique_transplant_final.duplicated().any()\n",
    "assert not test_unique_transplant_final.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd693a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure unseen mentions holds\n",
    "assert not pd.concat([train_unique_transplant_final, dev_unique_transplant_final]).duplicated().any()\n",
    "assert not pd.concat([train_unique_transplant_final, test_unique_transplant_final]).duplicated().any()\n",
    "assert not pd.concat([dev_unique_transplant_final, test_unique_transplant_final]).duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efe0177",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train', train_df_transplant_final.shape)\n",
    "print('Dev', dev_df_transplant_final.shape)\n",
    "print('Test', test_df_transplant_final.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365f10bc",
   "metadata": {},
   "source": [
    "# Remove samples from dev test to reach the desired number\n",
    "If required, here it is possible to remove samples from the dev and test sets to reach a precise number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cfc994",
   "metadata": {},
   "source": [
    "# Divide train and test in batches\n",
    "Train is divided in batch similarly as it is in the original dataset to avoid a single very big file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e4a5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=n_batch, shuffle=True, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a569ec",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb90381",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_transplant_final = train_df_transplant_final.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c83f0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_indexes = []                                                                                                                                              \n",
    "# the NIL class has freq = 0 so it is fairly distributed among the batches                                                                                            \n",
    "for _, _index in skf.split(np.zeros(train_df_transplant_final.shape[0]), train_df_transplant_final['freq']):                                                          \n",
    "    train_batch_indexes.append(_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64f760e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(train_batch_indexes):                                                                                                                       \n",
    "    train_df_transplant_final.loc[batch, 'batch'] = i                                                                                                                 \n",
    "train_df_transplant_final['batch'] = train_df_transplant_final['batch'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a984d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_transplant_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061592ac",
   "metadata": {},
   "source": [
    "# Reset Dev index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2b22a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df_transplant_final = dev_df_transplant_final.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924493e8",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c5d8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_transplant_final = test_df_transplant_final.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362d6a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_indexes = []\n",
    "# the NIL class has freq = 0 so it is fairly distributed among the batches\n",
    "for _, _index in skf.split(np.zeros(test_df_transplant_final.shape[0]), test_df_transplant_final['freq']):\n",
    "    test_batch_indexes.append(_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67ac96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(test_batch_indexes):\n",
    "    test_df_transplant_final.loc[batch, 'batch'] = i\n",
    "test_df_transplant_final['batch'] = test_df_transplant_final['batch'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111e63dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_transplant_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffc5c8c",
   "metadata": {},
   "source": [
    "# Prepare for BLINK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6f34a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_transplant_final = train_df_transplant_final.rename(columns = {\n",
    "    'left_context_text': 'context_left',\n",
    "    'word': 'mention',\n",
    "    'right_context_text': 'context_right',\n",
    "    'ex_id': 'query_id',\n",
    "    'url':'label_id',\n",
    "    'wikiId':'Wikipedia_ID',\n",
    "    'wikiurl':'Wikipedia_URL',\n",
    "    'y_title': 'Wikipedia_title'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951dd53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df_transplant_final = dev_df_transplant_final.rename(columns = {\n",
    "    'left_context_text': 'context_left',\n",
    "    'word': 'mention',\n",
    "    'right_context_text': 'context_right',\n",
    "    'ex_id': 'query_id',\n",
    "    'url':'label_id',\n",
    "    'wikiId':'Wikipedia_ID',\n",
    "    'wikiurl':'Wikipedia_URL',\n",
    "    'y_title': 'Wikipedia_title'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f282d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_transplant_final = test_df_transplant_final.rename(columns = {\n",
    "    'left_context_text': 'context_left',\n",
    "    'word': 'mention',\n",
    "    'right_context_text': 'context_right',\n",
    "    'ex_id': 'query_id',\n",
    "    'url':'label_id',\n",
    "    'wikiId':'Wikipedia_ID',\n",
    "    'wikiurl':'Wikipedia_URL',\n",
    "    'y_title': 'Wikipedia_title'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e4d860",
   "metadata": {},
   "source": [
    "# Save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43920e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_basedir = os.path.join(outdir, 'train')\n",
    "os.makedirs(train_basedir, exist_ok=True)\n",
    "print('Saving train...')\n",
    "for batch in range(n_batch):\n",
    "    print('Batch {} of {}'.format(batch + 1, n_batch))\n",
    "    train_batch = train_df_transplant_final[train_df_transplant_final['batch'] == batch]\n",
    "    with open(os.path.join(train_basedir, 'train_{}.jsonl'.format(batch)), 'w') as fd:\n",
    "        for i, row in tqdm(train_batch.iterrows(), total=train_batch.shape[0]):\n",
    "            fd.write(row.to_json())\n",
    "            fd.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d60a186",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_basedir = os.path.join(outdir, 'dev')\n",
    "os.makedirs(dev_basedir, exist_ok=True)\n",
    "print('Saving dev...')\n",
    "with open(os.path.join(dev_basedir, 'dev.jsonl'), 'w') as fd:\n",
    "    for i, row in tqdm(dev_df_transplant_final.iterrows(), total=dev_df_transplant_final.shape[0]):\n",
    "        fd.write(row.to_json())\n",
    "        fd.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30e51ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_basedir = os.path.join(outdir, 'test')\n",
    "os.makedirs(test_basedir, exist_ok=True)\n",
    "print('Saving test...')\n",
    "for batch in range(n_batch):\n",
    "    print('Batch {} of {}'.format(batch + 1, n_batch))\n",
    "    test_batch = test_df_transplant_final[test_df_transplant_final['batch'] == batch]\n",
    "    with open(os.path.join(test_basedir, 'test_{}.jsonl'.format(batch)), 'w') as fd:\n",
    "        for i, row in tqdm(test_batch.iterrows(), total=test_batch.shape[0]):\n",
    "            fd.write(row.to_json())\n",
    "            fd.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335ed25b",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b12554",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd67cafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in trange(n_batch):\n",
    "    test_batch = test_df_transplant_final[test_df_transplant_final['batch'] == batch]\n",
    "    \n",
    "    batch_report = {}\n",
    "    batch_report['batch'] = batch\n",
    "    \n",
    "    # mentions\n",
    "    batch_report['test_mentions'] = test_batch.shape[0]\n",
    "\n",
    "    # entities\n",
    "    batch_report['test_entities'] = test_batch['Wikipedia_ID'].drop_duplicates().shape[0]\n",
    "\n",
    "    # NIL mentions\n",
    "    batch_report['test_nil_mentions'] = test_batch.eval('NIL').sum()\n",
    "    \n",
    "    # not-NIL mentions\n",
    "    batch_report['test_not_nil_mentions'] = batch_report['test_mentions'] - batch_report['test_nil_mentions']\n",
    "\n",
    "    # NIL entities\n",
    "    batch_report['test_nil_entities'] = test_batch.query('NIL')['Wikipedia_ID'].drop_duplicates().shape[0]\n",
    "    \n",
    "    # not-NIL entities\n",
    "    batch_report['test_not_nil_entities'] = batch_report['test_entities'] - batch_report['test_nil_entities']\n",
    "    \n",
    "    # NIL entities present in a previous batch\n",
    "    test_indexes = set(test_batch.query('NIL')['Wikipedia_ID'].drop_duplicates()\n",
    "           ).intersection(\n",
    "            # the set of all the nil entities found in the previous batches\n",
    "            set(test_df_transplant_final.query('NIL and batch < {}'.format(batch))['Wikipedia_ID'].drop_duplicates())\n",
    "        )\n",
    "    batch_report['test_nil_entities_found_in_previous_batch'] = len(test_indexes)\n",
    "    \n",
    "    # NIL mentions present in a previous batch\n",
    "    batch_report['test_nil_mentions_found_in_previous_batch'] = test_batch['Wikipedia_ID'].isin(test_indexes).sum()\n",
    "    \n",
    "    report.append(batch_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34660936",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dataset = {}\n",
    "all_dataset['batch'] = 'ALL'\n",
    "\n",
    "# mentions\n",
    "all_dataset['train_mentions'] = train_df_transplant_final.shape[0]\n",
    "all_dataset['dev_mentions'] = dev_df_transplant_final.shape[0]\n",
    "all_dataset['test_mentions'] = test_df_transplant_final.shape[0]\n",
    "\n",
    "# entities\n",
    "all_dataset['train_entities'] = train_df_transplant_final['Wikipedia_ID'].drop_duplicates().shape[0]\n",
    "all_dataset['dev_entities'] = dev_df_transplant_final['Wikipedia_ID'].drop_duplicates().shape[0]\n",
    "all_dataset['test_entities'] = test_df_transplant_final['Wikipedia_ID'].drop_duplicates().shape[0]\n",
    "\n",
    "# NIL mentions\n",
    "all_dataset['train_nil_mentions'] = train_df_transplant_final.eval('NIL').sum()\n",
    "all_dataset['dev_nil_mentions'] = dev_df_transplant_final.eval('NIL').sum()\n",
    "all_dataset['test_nil_mentions'] = test_df_transplant_final.eval('NIL').sum()\n",
    "\n",
    "# not-NIL mentions\n",
    "all_dataset['train_not_nil_mentions'] = all_dataset['train_mentions'] - all_dataset['train_nil_mentions']\n",
    "all_dataset['dev_not_nil_mentions'] = all_dataset['dev_mentions'] - all_dataset['dev_nil_mentions']\n",
    "all_dataset['test_not_nil_mentions'] = all_dataset['test_mentions'] - all_dataset['test_nil_mentions']\n",
    "\n",
    "# NIL entities\n",
    "all_dataset['train_nil_entities'] = train_df_transplant_final.query('NIL')['Wikipedia_ID'].drop_duplicates().shape[0]\n",
    "all_dataset['dev_nil_entities'] = dev_df_transplant_final.query('NIL')['Wikipedia_ID'].drop_duplicates().shape[0]\n",
    "all_dataset['test_nil_entities'] = test_df_transplant_final.query('NIL')['Wikipedia_ID'].drop_duplicates().shape[0]\n",
    "\n",
    "# not-NIL entities\n",
    "all_dataset['train_not_nil_entities'] = all_dataset['train_entities'] - all_dataset['train_nil_entities']\n",
    "all_dataset['dev_not_nil_entities'] = all_dataset['dev_entities'] - all_dataset['dev_nil_entities']\n",
    "all_dataset['test_not_nil_entities'] = all_dataset['test_entities'] - all_dataset['test_nil_entities']\n",
    "\n",
    "# NIL entities present in a previous batch\n",
    "all_dataset['train_nil_entities_found_in_previous_batch'] = 0\n",
    "all_dataset['dev_nil_entities_found_in_previous_batch'] = 0\n",
    "all_dataset['test_nil_entities_found_in_previous_batch'] = 0\n",
    "\n",
    "# NIL mentions present in a previous batch\n",
    "all_dataset['train_nil_mentions_found_in_previous_batch'] = 0\n",
    "all_dataset['dev_nil_mentions_found_in_previous_batch'] = 0\n",
    "all_dataset['test_nil_mentions_found_in_previous_batch'] = 0\n",
    "\n",
    "report.append(all_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcadbfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset = {}\n",
    "original_dataset['batch'] = 'ORIGINAL'\n",
    "\n",
    "# mentions\n",
    "original_dataset['train_mentions'] = train_df.shape[0]\n",
    "original_dataset['dev_mentions'] = dev_df.shape[0]\n",
    "original_dataset['test_mentions'] = test_df.shape[0]\n",
    "\n",
    "# entities\n",
    "original_dataset['train_entities'] = train_df['wikiId'].drop_duplicates().shape[0]\n",
    "original_dataset['dev_entities'] = dev_df['wikiId'].drop_duplicates().shape[0]\n",
    "original_dataset['test_entities'] = test_df['wikiId'].drop_duplicates().shape[0]\n",
    "\n",
    "# NIL mentions\n",
    "original_dataset['train_nil_mentions'] =  0\n",
    "original_dataset['dev_nil_mentions'] = 0\n",
    "original_dataset['test_nil_mentions'] = 0\n",
    "\n",
    "# not-NIL mentions\n",
    "original_dataset['train_not_nil_mentions'] = original_dataset['train_mentions'] - original_dataset['train_nil_mentions']\n",
    "original_dataset['dev_not_nil_mentions'] = original_dataset['dev_mentions'] - original_dataset['dev_nil_mentions']\n",
    "original_dataset['test_not_nil_mentions'] = original_dataset['test_mentions'] - original_dataset['test_nil_mentions']\n",
    "\n",
    "# NIL entities\n",
    "original_dataset['train_nil_entities'] = 0\n",
    "original_dataset['dev_nil_entities'] = 0\n",
    "original_dataset['test_nil_entities'] = 0\n",
    "\n",
    "# not-NIL entities\n",
    "original_dataset['train_not_nil_entities'] = original_dataset['train_entities'] - original_dataset['train_nil_entities']\n",
    "original_dataset['dev_not_nil_entities'] = original_dataset['dev_entities'] - original_dataset['dev_nil_entities']\n",
    "original_dataset['test_not_nil_entities'] = original_dataset['test_entities'] - original_dataset['test_nil_entities']\n",
    "\n",
    "# NIL entities present in a previous batch\n",
    "original_dataset['train_nil_entities_found_in_previous_batch'] = 0\n",
    "original_dataset['dev_nil_entities_found_in_previous_batch'] = 0\n",
    "original_dataset['test_nil_entities_found_in_previous_batch'] = 0\n",
    "\n",
    "# NIL mentions present in a previous batch\n",
    "original_dataset['train_nil_mentions_found_in_previous_batch'] = 0\n",
    "original_dataset['dev_nil_mentions_found_in_previous_batch'] = 0\n",
    "original_dataset['test_nil_mentions_found_in_previous_batch'] = 0\n",
    "\n",
    "report.append(original_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa7007b",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df = pd.DataFrame(report)\n",
    "report_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9939ff",
   "metadata": {},
   "source": [
    "## Save statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60126e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_basedir = os.path.join(outdir, 'statistics')\n",
    "os.makedirs(stats_basedir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e7345d",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df.to_csv(os.path.join(stats_basedir, 'statistics.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7068b8",
   "metadata": {},
   "source": [
    " # Remove NIL entities from the KB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246231bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "NIL_entities = []\n",
    "NIL_entities.extend(\n",
    "    train_df_transplant_final.query('NIL')['Wikipedia_ID'].tolist())\n",
    "NIL_entities.extend(\n",
    "    dev_df_transplant_final.query('NIL')['Wikipedia_ID'].tolist())\n",
    "NIL_entities.extend(\n",
    "    test_df_transplant_final.query('NIL')['Wikipedia_ID'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14537966",
   "metadata": {},
   "outputs": [],
   "source": [
    "NIL_entities = set(NIL_entities) # remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78916f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_list = \",\".join(str(i) for i in NIL_entities)\n",
    "indexer = 10 # ensure to put the correct indexer\n",
    "sql_query = 'DELETE FROM entities WHERE indexer = {} AND wikipedia_id in ({});'.format(indexer, sql_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad59d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(outdir, 'delete_nil_entities.sql'), 'w') as fd:\n",
    "    fd.write(sql_query + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcce484a",
   "metadata": {},
   "source": [
    "Now run the sql query in the database (e.g.):\n",
    "```\n",
    "sudo docker-compose exec -T postgres psql -U postgres < /path/to/incremental_dataset/delete_nil_entities.sql\n",
    "```\n",
    "\n",
    "At this point NIL entities are removed from the db and even if retrieved by the faiss indexer they are discarded."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
