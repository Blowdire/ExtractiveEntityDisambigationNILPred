{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pywikibot\n",
      "  Downloading pywikibot-8.5.1-py3-none-any.whl (706 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.6/706.6 KB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting mwparserfromhell>=0.5.2\n",
      "  Downloading mwparserfromhell-0.6.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (191 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.0/191.0 KB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.21.0 in /usr/lib/python3/dist-packages (from pywikibot) (2.25.1)\n",
      "Requirement already satisfied: setuptools>=48.0.0 in /usr/lib/python3/dist-packages (from pywikibot) (59.6.0)\n",
      "Installing collected packages: mwparserfromhell, pywikibot\n",
      "Successfully installed mwparserfromhell-0.6.5 pywikibot-8.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pywikibot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.0.4) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import pywikibot\n",
    "\n",
    "\n",
    "def get_wikidata(wikipedia_title):\n",
    "    try:\n",
    "        site = pywikibot.Site(\"en\", \"wikipedia\")  # Connect to the English Wikipedia\n",
    "        page = pywikibot.Page(site, wikipedia_title)  # Get the Wikipedia page\n",
    "        item = pywikibot.ItemPage.fromPage(page)  # Get the associated Wikidata item\n",
    "        item.get()\n",
    "        to_add = \"\"\n",
    "        if \"en\" in item.descriptions:  # Fetch all data of the Wikidata item\n",
    "            to_add += item.descriptions['en'] + \" ; \"# Return the English description\n",
    "        added = 0\n",
    "        for category in page.categories()[:3]:\n",
    "            cat = category.title().split(\":\")[1]\n",
    "            if \"All articles\" in cat or \"Articles\" in cat or \"Pages\" in cat or \"Use\" in cat or \"CS1\" in cat or \"Webarchive\" in cat or \"Articles\" in cat or \"Pages\" in cat or \"Wikipedia\" in cat:\n",
    "                continue\n",
    "            else:\n",
    "                to_add += category.title() + \" ; \"\n",
    "                added += 1\n",
    "            if added == 3:\n",
    "                break\n",
    "\n",
    "    except:\n",
    "        to_add\n",
    "\n",
    "# wikipedia_title = \"KEGG\"\n",
    "# attributes = get_wikidata(wikipedia_title)\n",
    "# # print(dir(attributes))\n",
    "# print(attributes.descriptions[\"en\"])\n",
    "# for key in attributes.text:\n",
    "#     print(key + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test = get_wikidata(\"Kegg Pipe Organ Builders\")\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich(training_item):\n",
    "    candidates = training_item[\"question\"].split(\"</ec>\")\n",
    "    enriched_candidates = []\n",
    "    for candidate in candidates:\n",
    "        if candidate != \"\" and candidate.strip() != 'NIL':\n",
    "            if candidate != \"\":\n",
    "                correct_candidate = candidate.strip()\n",
    "                description = get_wikidata(correct_candidate)\n",
    "                correct_candidate += f\" : {description}\"\n",
    "                enriched_candidates.append(correct_candidate)\n",
    "    context_string = \"\"\n",
    "    for cand in enriched_candidates:\n",
    "        context_string += cand + \" </ec> \"\n",
    "    training_item[\"question\"] = context_string\n",
    "    return training_item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/18448 [00:00<?, ?it/s]Could not load cache: UnpicklingError('pickle data was truncated')\n",
      "Could not load cache: EOFError('Ran out of input')\n",
      "Could not load cache: EOFError('Ran out of input')\n",
      "Could not load cache: EOFError('Ran out of input')\n",
      " 13%|█▎        | 2310/18448 [1:22:23<11:55:25,  2.66s/it]ERROR: An error occurred for uri https://en.wikipedia.org/w/api.php?titles=English+people&inprop=protection&prop=info&action=query&indexpageids=&continue=&meta=userinfo&uiprop=blockinfo%7Chasmsg&maxlag=5&format=json\n",
      "ERROR: Traceback (most recent call last):\n",
      "  File \"/home/rub/.local/lib/python3.10/site-packages/pywikibot/data/api/_requests.py\", line 682, in _http_request\n",
      "    response = http.request(self.site, uri=uri,\n",
      "  File \"/home/rub/.local/lib/python3.10/site-packages/pywikibot/comms/http.py\", line 283, in request\n",
      "    r = fetch(baseuri, headers=headers, **kwargs)\n",
      "  File \"/home/rub/.local/lib/python3.10/site-packages/pywikibot/comms/http.py\", line 457, in fetch\n",
      "    callback(response)\n",
      "  File \"/home/rub/.local/lib/python3.10/site-packages/pywikibot/comms/http.py\", line 343, in error_handling_callback\n",
      "    raise response from None\n",
      "  File \"/home/rub/.local/lib/python3.10/site-packages/pywikibot/comms/http.py\", line 448, in fetch\n",
      "    response = session.request(method, uri,\n",
      "  File \"/usr/lib/python3/dist-packages/requests/sessions.py\", line 544, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/usr/lib/python3/dist-packages/requests/sessions.py\", line 657, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/usr/lib/python3/dist-packages/requests/adapters.py\", line 498, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "WARNING: Waiting 5.0 seconds before retrying.\n",
      " 22%|██▏       | 4064/18448 [2:25:55<8:26:54,  2.11s/it] "
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from tqdm import tqdm\n",
    "\n",
    "tokenized = []\n",
    "file_path = \"./aida_train_converted.jsonl\"\n",
    "from datasets import Dataset\n",
    "from multiprocessing import Pool\n",
    "\n",
    "original_data = []\n",
    "if __name__ == \"__main__\":\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            # Load each line as a JSON object\n",
    "            data_line = json.loads(line)\n",
    "            with Pool(10) as p:\n",
    "                original_data = list(\n",
    "                    tqdm(p.imap(enrich, data_line), total=len(data_line))\n",
    "                )\n",
    "\n",
    "    with open(f\"{file_path}enriched.jsonl\", \"w\") as f:\n",
    "        for item in original_data:\n",
    "            f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version for base dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_variant(sample):\n",
    "  candidates = sample['candidates']\n",
    "  enriched_c = []\n",
    "  for candidate in candidates:\n",
    "    if candidate != \"\" and candidate.strip() != 'NIL':\n",
    "      \n",
    "      correct_candidate = candidate.strip()\n",
    "      description = get_wikidata(correct_candidate)\n",
    "      correct_candidate += f\" : {description}\"\n",
    "      enriched_c.append(correct_candidate)\n",
    "  sample['candidates'] = enriched_c\n",
    "  return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 2788/6821 [1:27:13<56:58,  1.18it/s]  ERROR: An error occurred for uri https://en.wikipedia.org/w/api.php?titles=Greece+national+football+team&inprop=protection&prop=info&action=query&indexpageids=&continue=&meta=userinfo&uiprop=blockinfo%7Chasmsg&maxlag=5&format=json\n",
      "ERROR: Traceback (most recent call last):\n",
      "  File \"/home/rub/.local/lib/python3.10/site-packages/pywikibot/data/api/_requests.py\", line 682, in _http_request\n",
      "    response = http.request(self.site, uri=uri,\n",
      "  File \"/home/rub/.local/lib/python3.10/site-packages/pywikibot/comms/http.py\", line 283, in request\n",
      "    r = fetch(baseuri, headers=headers, **kwargs)\n",
      "  File \"/home/rub/.local/lib/python3.10/site-packages/pywikibot/comms/http.py\", line 457, in fetch\n",
      "    callback(response)\n",
      "  File \"/home/rub/.local/lib/python3.10/site-packages/pywikibot/comms/http.py\", line 343, in error_handling_callback\n",
      "    raise response from None\n",
      "  File \"/home/rub/.local/lib/python3.10/site-packages/pywikibot/comms/http.py\", line 448, in fetch\n",
      "    response = session.request(method, uri,\n",
      "  File \"/usr/lib/python3/dist-packages/requests/sessions.py\", line 544, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/usr/lib/python3/dist-packages/requests/sessions.py\", line 657, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/usr/lib/python3/dist-packages/requests/adapters.py\", line 498, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "WARNING: Waiting 5.0 seconds before retrying.\n",
      "100%|██████████| 6821/6821 [2:35:25<00:00,  1.37s/it]  \n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "file_path = \"./wiki-test-kilt.jsonl\"\n",
    "dataset = []\n",
    "if __name__ == \"__main__\":\n",
    "    with open(file_path, \"r\") as file:\n",
    "        unfolded = []\n",
    "        for line in file:\n",
    "            # Load each line as a JSON object\n",
    "            data_line = json.loads(line)\n",
    "\n",
    "            unfolded.append(data_line)\n",
    "        with Pool(10) as p:\n",
    "            dataset = list(tqdm(p.imap(enrich_variant, unfolded), total=len(unfolded)))\n",
    "    with open(f\"{file_path}enriched.jsonl\", \"w\") as f:\n",
    "      for item in dataset:\n",
    "          f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
