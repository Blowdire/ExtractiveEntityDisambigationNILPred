{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01515e8-c2c5-4f7e-bcae-6d8559f41239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # or \"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63af2596-b907-49cc-b6c3-2190e8af746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"tesi\", name=\"bigbird\")  # args = TrainingArguments(\n",
    "wandb.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2de9e6eb-0958-42eb-807b-04e51fd71df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.0.4) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, RobertaForSequenceClassification\n",
    "from datasets import Dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'roberta-large'\n",
    ")\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-large', return_dict=False, num_labels=2\n",
    ").to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dca9bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.0.4) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "You are using a model of type ctrl to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ctrl and are newly initialized: ['classifier.out_proj.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'embeddings.position_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.bias', 'classifier.out_proj.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.6.attention.self.key.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.10.attention.self.key.bias', 'classifier.dense.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.2.intermediate.dense.bias', 'classifier.dense.weight', 'encoder.layer.7.intermediate.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    RobertaForSequenceClassification,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ctrl\")\n",
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    \"ctrl\", return_dict=False, num_labels=2\n",
    ").to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1362618-8cce-4759-8214-6f77876c33d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7939\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "tokenized = []\n",
    "file_path = \"./nil_el_instanceof.jsonl\"\n",
    "from datasets import Dataset\n",
    "\n",
    "original_data = []\n",
    "with open(file_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        # Load each line as a JSON object\n",
    "        data_line = json.loads(line)\n",
    "        \n",
    "        \n",
    "        original_data.append(data_line)\n",
    "print(len(original_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f04504f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18448\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "tokenized = []\n",
    "file_path = \"./aida_train_instanceof.jsonl\"\n",
    "from datasets import Dataset\n",
    "\n",
    "original_data_aida = []\n",
    "with open(file_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        # Load each line as a JSON object\n",
    "        data_line = json.loads(line)\n",
    "        \n",
    "        # additional_question = \"<additional> \"\n",
    "        # try:\n",
    "        #     for related in data_line[\"most_related\"]:\n",
    "        #         length = len(related)\n",
    "        #         if len(related) == 4:\n",
    "        #             additional_question += related[2]\n",
    "        #             additional_question += \" \"\n",
    "        # except:\n",
    "        #     None\n",
    "        # additional_question += \"</additional>\"\n",
    "        # data_line[\"context\"] += additional_question\n",
    "        original_data_aida.append(data_line)\n",
    "print(len(original_data_aida))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db6f78a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#random.shuffle(original_data_aida)\n",
    "#concat = original_data_aida\n",
    "#concat =original_data_aida[:int(len(original_data_aida)* 0.4)] + original_data\n",
    "concat = original_data\n",
    "\n",
    "random.shuffle(concat)\n",
    "#print(len(concat))\n",
    "train_original = concat[: int(len(concat) * 0.95)]\n",
    "eval_original = concat[int(len(concat) * 0.95) : ]\n",
    "dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"context\": [item[\"question\"] for item in train_original],\n",
    "        \"question\": [item[\"context\"] for item in train_original],\n",
    "        \"answers\": [item[\"answers\"] for item in train_original],\n",
    "    }\n",
    ")\n",
    "dataset_ev = Dataset.from_dict(\n",
    "    {\n",
    "        \"context\": [item[\"question\"] for item in eval_original],\n",
    "        \"question\": [item[\"context\"] for item in eval_original],\n",
    "        \"answers\": [item[\"answers\"] for item in eval_original],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f94614f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_alignement(context, answer):\n",
    "    \"\"\"Some original examples in SQuAD have indices wrong by 1 or 2 character. We test and fix this here.\"\"\"\n",
    "    gold_text = answer[\"text\"][0]\n",
    "    \n",
    "    start_idx = context.find(gold_text)\n",
    "    end_idx = context.find(\"</ec>\", start_idx)\n",
    "    return start_idx, end_idx\n",
    "    # if context[start_idx:end_idx] == gold_text:\n",
    "    #     return start_idx, end_idx       # When the gold label position is good\n",
    "    # elif context[start_idx-1:end_idx-1] == gold_text:\n",
    "    #     return start_idx-1, end_idx-1   # When the gold label is off by one character\n",
    "    # elif context[start_idx-2:end_idx-2] == gold_text:\n",
    "    #     return start_idx-2, end_idx-2   # When the gold label is off by two character\n",
    "    # else:\n",
    "    #     raise ValueError()\n",
    "\n",
    "\n",
    "# Tokenize our training dataset\n",
    "def convert_to_features(example):\n",
    "    try:\n",
    "        # Tokenize contexts and questions (as pairs of inputs)\n",
    "\n",
    "        input_pairs = [example[\"question\"], example[\"context\"]]\n",
    "        encodings = tokenizer.encode_plus(\n",
    "            input_pairs, pad_to_max_length=True, truncation=\"only_second\"\n",
    "        )\n",
    "        try:\n",
    "            encodings.update(\n",
    "                {\n",
    "                   \"input_ids\": encodings['input_ids'],\n",
    "                    \"attention_mask\": encodings[\"attention_mask\"],\n",
    "                    \"labels\": 1 if example[\"answers\"][\"text\"][0] == \"Not In Candidates\" else 0\n",
    "                }\n",
    "            )\n",
    "            return encodings\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "       \n",
    "    except Exception as e:\n",
    "        print(\"errors\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "352ecd7d-1d89-4113-bb3c-0fd8837334a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1b3cebd973c4a009bea4ded773e3551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7542 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rub/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2393: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb10d80312074c2fad58dab46481f2ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/397 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_ds = dataset.map(convert_to_features)\n",
    "eval_ds = dataset_ev.map(convert_to_features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8bff14-4fde-4476-b465-6f6cc047553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03702e3-22f2-46b1-9387-a536b79d06e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump( train_ds, './dumps/aidaTrainIOF.dump',)\n",
    "joblib.dump( eval_ds, './dumps/aidaEvalIOF.dump',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb4bf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = joblib.load('./dumps/aidaTrainNERBigBird.dump')\n",
    "eval_ds = joblib.load('./dumps/aidaEvalNERBigBird.dump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e60b5c-b03e-4ebf-888a-d0a10390b3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "wandb.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca745971-3963-42de-af93-1282e75daab2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "from transformers import default_data_collator\n",
    "import torch\n",
    "import wandb\n",
    "torch.cuda.set_device(0)\n",
    "torch.cuda.current_device()\n",
    "\n",
    "#wandb.init(project=\"tesi\", name=\"robertaAidaLargeNERNIL\")  # args = TrainingArguments(\n",
    "#     f\"longformer-finetuned-squad\",\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=8,\n",
    "#     num_train_epochs=3,\n",
    "#     weight_decay=0.01,\n",
    "# )\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"clsRoberta\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    report_to=\"wandb\",\n",
    "    load_best_model_at_end=True,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=3e-5,\n",
    "    per_device_eval_batch_size=2,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=20,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.02,\n",
    "    fp16=True,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7ac65fb-0f2a-401f-9cb6-0c3d98ce3c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rub/.local/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de982aeb59f4487199e0dc0c47f6a707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5544, 'learning_rate': 2.6872340425531917e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26dc2fd159d54932b71dad0fd7af20db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.37234142422676086, 'eval_runtime': 5.8804, 'eval_samples_per_second': 67.512, 'eval_steps_per_second': 33.841, 'epoch': 0.53}\n",
      "{'loss': 0.3792, 'learning_rate': 2.3680851063829785e-05, 'epoch': 1.06}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bca189aaf5094b15ad9ba010c7d27745",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4165396988391876, 'eval_runtime': 5.8995, 'eval_samples_per_second': 67.293, 'eval_steps_per_second': 33.731, 'epoch': 1.06}\n",
      "{'loss': 0.3054, 'learning_rate': 2.048936170212766e-05, 'epoch': 1.59}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af91991d85084053b579b3993ca53a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.26699376106262207, 'eval_runtime': 5.902, 'eval_samples_per_second': 67.265, 'eval_steps_per_second': 33.717, 'epoch': 1.59}\n",
      "{'loss': 0.2577, 'learning_rate': 1.7297872340425532e-05, 'epoch': 2.12}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00ab69aa02ab4ad3bf3920d943c22bbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3105666935443878, 'eval_runtime': 5.8742, 'eval_samples_per_second': 67.584, 'eval_steps_per_second': 33.877, 'epoch': 2.12}\n",
      "{'loss': 0.2075, 'learning_rate': 1.4106382978723404e-05, 'epoch': 2.65}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b11c60d1f76940d2be1deef80b92a5f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.24634204804897308, 'eval_runtime': 5.7081, 'eval_samples_per_second': 69.551, 'eval_steps_per_second': 34.863, 'epoch': 2.65}\n",
      "{'loss': 0.1438, 'learning_rate': 1.0914893617021278e-05, 'epoch': 3.18}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0db3259330294141981e630a569fb16f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3406025469303131, 'eval_runtime': 5.6134, 'eval_samples_per_second': 70.723, 'eval_steps_per_second': 35.451, 'epoch': 3.18}\n",
      "{'loss': 0.1348, 'learning_rate': 7.723404255319148e-06, 'epoch': 3.71}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebac9bf75631499087ad14a8dd45f785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2822650074958801, 'eval_runtime': 5.6163, 'eval_samples_per_second': 70.688, 'eval_steps_per_second': 35.433, 'epoch': 3.71}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1536\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1537\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1538\u001b[0m )\n\u001b[0;32m-> 1539\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1540\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1541\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1542\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1543\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1544\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:1888\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1886\u001b[0m     optimizer_was_run \u001b[39m=\u001b[39m scale_before \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m scale_after\n\u001b[1;32m   1887\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1888\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m   1889\u001b[0m     optimizer_was_run \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39moptimizer_step_was_skipped\n\u001b[1;32m   1891\u001b[0m \u001b[39mif\u001b[39;00m optimizer_was_run:\n\u001b[1;32m   1892\u001b[0m     \u001b[39m# Delay optimizer scheduling until metrics are generated\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/optimizer.py:133\u001b[0m, in \u001b[0;36mAcceleratedOptimizer.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_scale \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mget_scale()\n\u001b[1;32m    132\u001b[0m     new_scale \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscaler\u001b[39m.\u001b[39;49mstep(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer, closure)\n\u001b[1;32m    134\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mupdate()\n\u001b[1;32m    135\u001b[0m scale_after \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mget_scale()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:374\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    372\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mfound_inf_per_device\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 374\u001b[0m retval \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_opt_step(optimizer, optimizer_state, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    376\u001b[0m optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mstage\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m OptState\u001b[39m.\u001b[39mSTEPPED\n\u001b[1;32m    378\u001b[0m \u001b[39mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:289\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_maybe_opt_step\u001b[39m(\u001b[39mself\u001b[39m, optimizer, optimizer_state, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    288\u001b[0m     retval \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39msum\u001b[39;49m(v\u001b[39m.\u001b[39;49mitem() \u001b[39mfor\u001b[39;49;00m v \u001b[39min\u001b[39;49;00m optimizer_state[\u001b[39m\"\u001b[39;49m\u001b[39mfound_inf_per_device\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mvalues()):\n\u001b[1;32m    290\u001b[0m         retval \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39mstep(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    291\u001b[0m     \u001b[39mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:289\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_maybe_opt_step\u001b[39m(\u001b[39mself\u001b[39m, optimizer, optimizer_state, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    288\u001b[0m     retval \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39msum\u001b[39m(v\u001b[39m.\u001b[39;49mitem() \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mfound_inf_per_device\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    290\u001b[0m         retval \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39mstep(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    291\u001b[0m     \u001b[39mreturn\u001b[39;00m retval\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "77766c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('./classifierFinal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71805d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b3f211-be57-4c73-9a66-1d65c7dade62",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3e969f",
   "metadata": {},
   "source": [
    "## Prediction and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f99facfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alzenau'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_brackets(input_string):\n",
    "    stack = []\n",
    "    brackets = {\"(\": \")\", \"{\": \"}\", \"[\": \"]\"}\n",
    "    for char in input_string:\n",
    "        if char in brackets.keys():\n",
    "            stack.append(char)\n",
    "        elif char in brackets.values():\n",
    "            if not stack or brackets[stack.pop()] != char:\n",
    "                return False\n",
    "    return not stack\n",
    "\n",
    "\n",
    "def process_answer(answer, candidates):\n",
    "    if answer == \"Not In Candidates\":\n",
    "        return answer\n",
    "    else:\n",
    "        modified_answer = answer.split(\"</ec>\")[0]\n",
    "        modified_answer = answer.split(\": instance of \")[0]\n",
    "        modified_answer = answer.split(\": instance\")[0]\n",
    "        modified_answer = modified_answer.replace(\"<s>\",\"\")\n",
    "        modified_answer = modified_answer.replace(\"</ec\",\"\")\n",
    "        modified_answer = modified_answer.replace(\"</s>\",\"\")\n",
    "        modified_answer = modified_answer.replace(\"<s\", \"\")\n",
    "        modified_answer = modified_answer.replace(\"</\", \"\")\n",
    "        modified_answer = modified_answer.replace(\" ec\", \"\")\n",
    "        modified_answer = modified_answer.replace(\">\",\"\")\n",
    "        modified_answer = modified_answer.strip()\n",
    "        if not check_brackets(modified_answer):\n",
    "            modified_answer = modified_answer.replace(\"(\", \"\")\n",
    "            modified_answer = modified_answer.replace(\")\", \"\")\n",
    "        if modified_answer == \"Not In Candidates\":\n",
    "            modified_answer = \"NIL\"\n",
    "        if modified_answer == \"\":\n",
    "            modified_answer = \"Not In Candidates\"\n",
    "        if modified_answer not in candidates:\n",
    "            modified_answer = \"Not In Candidates\"\n",
    "        return modified_answer\n",
    "process_answer(\"> Alzenau </\", [\"Alzenau\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1555cdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch.nn.functional as F\n",
    "def check_is_nil(input_pairs):\n",
    "    encodings = tokenizer.encode_plus(\n",
    "                input_pairs, return_tensors=\"pt\", truncation=\"only_second\"\n",
    "            ).to(\"cuda\")\n",
    "    outs = model(\n",
    "        encodings[\"input_ids\"], attention_mask=encodings[\"attention_mask\"]\n",
    "    )\n",
    "    outs = outs[0].cpu().detach().numpy()\n",
    "    if outs[0][0] < outs[0][1]:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "            \n",
    "\n",
    "def make_prediction(data_entry, nil_prediction):\n",
    "    with torch.no_grad():\n",
    "        question = data_entry[\"input\"]\n",
    "        context = \"\"\n",
    "        for item in data_entry[\"candidates\"]:\n",
    "            context += item + f\" </ec> \"\n",
    "        input_pairs = [question, context]\n",
    "        context += \" Not In Candidates </ec> \"\n",
    "        encodings = tokenizer.encode_plus(\n",
    "            input_pairs, return_tensors=\"pt\", truncation=\"only_second\"\n",
    "        ).to(\"cuda\")\n",
    "        outs = model(\n",
    "            encodings[\"input_ids\"], attention_mask=encodings[\"attention_mask\"]\n",
    "        )\n",
    "        answer = ''\n",
    "        is_nil = check_is_nil(input_pairs)\n",
    "        if is_nil:\n",
    "            answer = \"Not In Candidates\"\n",
    "        else:\n",
    "            answer = data_entry['output'][0]['provenance'][0]['title']\n",
    "        return {\n",
    "            \"correct\": data_entry[\"output\"],\n",
    "            \"predicted\": answer,\n",
    "            \"candidates\": context,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255c023f-a73d-4d5e-a753-418ff74c0e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee371f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "import torch\n",
    "\n",
    "results = []\n",
    "file_path = \"./msnbc_iof_nilled.jsonl\"\n",
    "dataset = []\n",
    "with open(file_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        # Load each line as a JSON object\n",
    "        data_line = json.loads(line)\n",
    "\n",
    "        dataset.append(data_line)\n",
    "\n",
    "for item in tqdm(dataset):\n",
    "    try:\n",
    "        pred = make_prediction(item, True)\n",
    "        results.append(pred)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "daca2118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5ba25a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 656/656 [00:00<00:00, 1079005.26it/s]\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "correct_trace = []\n",
    "wrong = []\n",
    "correct_nil = 0\n",
    "wrong_nil = 0\n",
    "full_data = []\n",
    "for result in tqdm(results):\n",
    "  processed = result['predicted']\n",
    "  if processed == result['correct'][0]['answer']:\n",
    "    correct += 1\n",
    "    correct_trace.append(result)\n",
    "    if processed == 'Not In Candidates':\n",
    "        correct_nil += 1\n",
    "  \n",
    "  else:\n",
    "    if result['correct'][0]['answer'] == 'Not In Candidates':\n",
    "        wrong_nil += 1\n",
    "    wrong.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2024e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of the model is 0.07164634146341463\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maccuracy of the model is \u001b[39m\u001b[39m{\u001b[39;00mcorrect\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(results)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maccuracy in nil prediction is: \u001b[39m\u001b[39m{\u001b[39;00mcorrect_nil\u001b[39m/\u001b[39;49m(correct_nil\u001b[39m+\u001b[39;49mwrong_nil)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy of the model is {correct/len(results)}\")\n",
    "print(f\"accuracy in nil prediction is: {correct_nil/(correct_nil+wrong_nil)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b285bc-679f-41a8-9859-3f594a4b276c",
   "metadata": {},
   "source": [
    "-  Base: 65.9%\n",
    "-  Large: 73.21%(enriched) - 74.65% (standard)\n",
    "-  Large EN: 76.66%\n",
    "-  large NER: 75.5%\n",
    "-  Base-NER: 66.35%\n",
    "\n",
    "## MSNBC large\n",
    "-  No EN: 91.92%\n",
    "-  No Instance Of: 92.83% - Yes: 93.44%\n",
    "-  EN: 92.8\n",
    "-  Large NER: 92.83%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b365299e-e8f0-4651-b752-ff7d6adcb5a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1197d1e9-3e52-4ca4-a959-57f6751f6c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "full_df = pd.DataFrame(full_data)\n",
    "df = pd.DataFrame(wrong)\n",
    "df_c = pd.DataFrame(correct_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43c3df84-751a-4ade-9364-a9078478e2a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correct</th>\n",
       "      <th>predicted</th>\n",
       "      <th>candidates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[{'answer': 'Los Angeles Lakers', 'provenance'...</td>\n",
       "      <td>Los Angeles Lakers</td>\n",
       "      <td>Los Angeles Lakers : instance of basketball te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[{'answer': 'Miami Heat', 'provenance': [{'tit...</td>\n",
       "      <td>Miami Heat</td>\n",
       "      <td>Heat : instance of process function &lt;/ec&gt; Miam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[{'answer': 'Opportunity (rover)', 'provenance...</td>\n",
       "      <td>Opportunity (rover)</td>\n",
       "      <td>Opportunity (rover) : instance of Mars rover &lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[{'answer': 'Saddam Hussein', 'provenance': [{...</td>\n",
       "      <td>Saddam Hussein</td>\n",
       "      <td>Saddam Hussein : instance of human &lt;/ec&gt; Halab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[{'answer': 'Hindu', 'provenance': [{'title': ...</td>\n",
       "      <td>Hindu</td>\n",
       "      <td>Hindu : instance of Unknown &lt;/ec&gt; Hindus crick...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[{'answer': 'London Heathrow Airport', 'proven...</td>\n",
       "      <td>London Heathrow Airport</td>\n",
       "      <td>London Heathrow Airport : instance of Unknown ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[{'answer': 'Grand Rapids, Michigan', 'provena...</td>\n",
       "      <td>Grand Rapids, Michigan</td>\n",
       "      <td>Grand Rapids, Michigan : instance of big city ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              correct  \\\n",
       "40  [{'answer': 'Los Angeles Lakers', 'provenance'...   \n",
       "41  [{'answer': 'Miami Heat', 'provenance': [{'tit...   \n",
       "42  [{'answer': 'Opportunity (rover)', 'provenance...   \n",
       "43  [{'answer': 'Saddam Hussein', 'provenance': [{...   \n",
       "44  [{'answer': 'Hindu', 'provenance': [{'title': ...   \n",
       "45  [{'answer': 'London Heathrow Airport', 'proven...   \n",
       "46  [{'answer': 'Grand Rapids, Michigan', 'provena...   \n",
       "\n",
       "                  predicted                                         candidates  \n",
       "40       Los Angeles Lakers  Los Angeles Lakers : instance of basketball te...  \n",
       "41               Miami Heat  Heat : instance of process function </ec> Miam...  \n",
       "42      Opportunity (rover)  Opportunity (rover) : instance of Mars rover <...  \n",
       "43           Saddam Hussein  Saddam Hussein : instance of human </ec> Halab...  \n",
       "44                    Hindu  Hindu : instance of Unknown </ec> Hindus crick...  \n",
       "45  London Heathrow Airport  London Heathrow Airport : instance of Unknown ...  \n",
       "46   Grand Rapids, Michigan  Grand Rapids, Michigan : instance of big city ...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "n = 20\n",
    "offset=2\n",
    "df_c.iloc[offset *n : offset*n + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be007697-5520-4786-b03a-4fa61bf30b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./errorsNil.csv\")\n",
    "df_c.to_csv(\"./correctNil.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67de164f-a8a4-4241-b9a6-3b88323bd64a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
