{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d01515e8-c2c5-4f7e-bcae-6d8559f41239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # or \"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63af2596-b907-49cc-b6c3-2190e8af746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"tesi\", name=\"bigbird\")  # args = TrainingArguments(\n",
    "wandb.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2de9e6eb-0958-42eb-807b-04e51fd71df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from datasets import Dataset\n",
    "\n",
    "modelName = \"./robertaLargeIofNil/checkpoint-2178/\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelName)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(modelName, return_dict=False).to(\n",
    "    \"cuda:0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1362618-8cce-4759-8214-6f77876c33d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7939\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "tokenized = []\n",
    "file_path = \"./nil_el_instanceof.jsonl\"\n",
    "from datasets import Dataset\n",
    "\n",
    "original_data = []\n",
    "with open(file_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        # Load each line as a JSON object\n",
    "        data_line = json.loads(line)\n",
    "        \n",
    "        \n",
    "        original_data.append(data_line)\n",
    "print(len(original_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f04504f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18448\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "tokenized = []\n",
    "file_path = \"./aida_train_instanceof.jsonl\"\n",
    "from datasets import Dataset\n",
    "\n",
    "original_data_aida = []\n",
    "with open(file_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        # Load each line as a JSON object\n",
    "        data_line = json.loads(line)\n",
    "        \n",
    "        # additional_question = \"<additional> \"\n",
    "        # try:\n",
    "        #     for related in data_line[\"most_related\"]:\n",
    "        #         length = len(related)\n",
    "        #         if len(related) == 4:\n",
    "        #             additional_question += related[2]\n",
    "        #             additional_question += \" \"\n",
    "        # except:\n",
    "        #     None\n",
    "        # additional_question += \"</additional>\"\n",
    "        # data_line[\"context\"] += additional_question\n",
    "        original_data_aida.append(data_line)\n",
    "print(len(original_data_aida))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db6f78a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(original_data_aida)\n",
    "#concat = original_data_aida\n",
    "concat =original_data_aida[:int(len(original_data_aida)* 0.7)] + original_data\n",
    "\n",
    "\n",
    "random.shuffle(concat)\n",
    "#print(len(concat))\n",
    "train_original = concat[: int(len(concat) * 0.95)]\n",
    "eval_original = concat[int(len(concat) * 0.95) : ]\n",
    "dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"context\": [item[\"question\"] for item in train_original],\n",
    "        \"question\": [item[\"context\"] for item in train_original],\n",
    "        \"answers\": [item[\"answers\"] for item in train_original],\n",
    "    }\n",
    ")\n",
    "dataset_ev = Dataset.from_dict(\n",
    "    {\n",
    "        \"context\": [item[\"question\"] for item in eval_original],\n",
    "        \"question\": [item[\"context\"] for item in eval_original],\n",
    "        \"answers\": [item[\"answers\"] for item in eval_original],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f94614f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_alignement(context, answer):\n",
    "    \"\"\"Some original examples in SQuAD have indices wrong by 1 or 2 character. We test and fix this here.\"\"\"\n",
    "    gold_text = answer[\"text\"][0]\n",
    "    \n",
    "    start_idx = context.find(gold_text)\n",
    "    end_idx = context.find(\"</ec>\", start_idx)\n",
    "    return start_idx, end_idx\n",
    "    # if context[start_idx:end_idx] == gold_text:\n",
    "    #     return start_idx, end_idx       # When the gold label position is good\n",
    "    # elif context[start_idx-1:end_idx-1] == gold_text:\n",
    "    #     return start_idx-1, end_idx-1   # When the gold label is off by one character\n",
    "    # elif context[start_idx-2:end_idx-2] == gold_text:\n",
    "    #     return start_idx-2, end_idx-2   # When the gold label is off by two character\n",
    "    # else:\n",
    "    #     raise ValueError()\n",
    "\n",
    "\n",
    "# Tokenize our training dataset\n",
    "def convert_to_features(example):\n",
    "    try:\n",
    "        # Tokenize contexts and questions (as pairs of inputs)\n",
    "\n",
    "        input_pairs = [example[\"question\"], example[\"context\"]]\n",
    "        encodings = tokenizer.encode_plus(\n",
    "            input_pairs, pad_to_max_length=True, truncation=\"only_second\"\n",
    "        )\n",
    "        context_encodings = tokenizer.encode_plus(example[\"context\"])\n",
    "\n",
    "        # Compute start and end tokens for labels using Transformers's fast tokenizers alignement methodes.\n",
    "        # this will give us the position of answer span in the context text\n",
    "\n",
    "        start_idx, end_idx = get_correct_alignement(\n",
    "            example[\"context\"], example[\"answers\"]\n",
    "        )\n",
    "        if end_idx != -1 and start_idx != -1:\n",
    "            try:\n",
    "                start_positions_context = context_encodings.char_to_token(start_idx)\n",
    "                end_positions_context = context_encodings.char_to_token(end_idx)\n",
    "\n",
    "                # here we will compute the start and end position of the answer in the whole example\n",
    "                # as the example is encoded like this <s> question</s></s> context</s>\n",
    "                # and we know the postion of the answer in the context\n",
    "                # we can just find out the index of the sep token and then add that to position + 1 (+1 because there are two sep tokens)\n",
    "                # this will give us the position of the answer span in whole example\n",
    "                sep_idx = encodings[\"input_ids\"].index(tokenizer.sep_token_id)\n",
    "                start_positions = start_positions_context + sep_idx\n",
    "                end_positions = end_positions_context + sep_idx + 1\n",
    "                #print(tokenizer.decode\n",
    "                #(encodings[\"input_ids\"][start_positions:end_positions]))\n",
    "                encodings.update(\n",
    "                    {\n",
    "                        \"start_positions\": start_positions,\n",
    "                        \"end_positions\": end_positions,\n",
    "                        \"attention_mask\": encodings[\"attention_mask\"],\n",
    "                    }\n",
    "                )\n",
    "                return encodings\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        else:\n",
    "            print(\"qu\")\n",
    "            encodings.update(\n",
    "                {\n",
    "                    \"start_positions\": start_idx,\n",
    "                    \"end_positions\": end_idx,\n",
    "                    \"attention_mask\": encodings[\"attention_mask\"],\n",
    "                    \"is_nil\": 1 if example[\"answers\"][\"text\"][0] == \"Not In Candidates\" else 0\n",
    "                }\n",
    "            )\n",
    "            return encodings\n",
    "    except Exception as e:\n",
    "        print(\"errors\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352ecd7d-1d89-4113-bb3c-0fd8837334a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = dataset.map(convert_to_features)\n",
    "eval_ds = dataset_ev.map(convert_to_features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8bff14-4fde-4476-b465-6f6cc047553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03702e3-22f2-46b1-9387-a536b79d06e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump( train_ds, './dumps/aidaTrainIOF.dump',)\n",
    "joblib.dump( eval_ds, './dumps/aidaEvalIOF.dump',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb4bf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = joblib.load('./dumps/aidaTrainNERBigBird.dump')\n",
    "eval_ds = joblib.load('./dumps/aidaEvalNERBigBird.dump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49e60b5c-b03e-4ebf-888a-d0a10390b3c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3qey95ft) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:3qey95ft). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.1 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">desert-silence-4</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/agazzi-ruben99/uncategorized\" target=\"_blank\">https://wandb.ai/agazzi-ruben99/uncategorized</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/agazzi-ruben99/uncategorized/runs/2yhntv94\" target=\"_blank\">https://wandb.ai/agazzi-ruben99/uncategorized/runs/2yhntv94</a><br/>\n",
       "                Run data is saved locally in <code>/home/agazzi/wandb/run-20231218_100154-2yhntv94</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(2yhntv94)</h1><iframe src=\"https://wandb.ai/agazzi-ruben99/uncategorized/runs/2yhntv94\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fceae26cfd0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca745971-3963-42de-af93-1282e75daab2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "from transformers import default_data_collator\n",
    "\n",
    "import wandb\n",
    "torch.cuda.set_device(0)\n",
    "torch.cuda.current_device()\n",
    "\n",
    "#wandb.init(project=\"tesi\", name=\"robertaAidaLargeNERNIL\")  # args = TrainingArguments(\n",
    "#     f\"longformer-finetuned-squad\",\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=8,\n",
    "#     num_train_epochs=3,\n",
    "#     weight_decay=0.01,\n",
    "# )\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"robertaLargefNil\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"wandb\",\n",
    "    load_best_model_at_end=False,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    save_total_limit=1,\n",
    "    learning_rate=3e-5,\n",
    "    per_device_eval_batch_size=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=100,\n",
    "    num_train_epochs=14,\n",
    "    weight_decay=0.02,\n",
    "    fp16=True,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7ac65fb-0f2a-401f-9cb6-0c3d98ce3c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: answers, context, question.\n",
      "***** Running training *****\n",
      "  Num examples = 19809\n",
      "  Num Epochs = 14\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 100\n",
      "  Gradient Accumulation steps = 100\n",
      "  Total optimization steps = 2772\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33magazzi-ruben99\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.1 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">robertaLargefNil</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/agazzi-ruben99/huggingface\" target=\"_blank\">https://wandb.ai/agazzi-ruben99/huggingface</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/agazzi-ruben99/huggingface/runs/zp9jbbva\" target=\"_blank\">https://wandb.ai/agazzi-ruben99/huggingface/runs/zp9jbbva</a><br/>\n",
       "                Run data is saved locally in <code>/home/agazzi/wandb/run-20231218_194507-zp9jbbva</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1171' max='2772' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1171/2772 4:01:55 < 5:31:19, 0.08 it/s, Epoch 5.91/14]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.386881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.337372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.466700</td>\n",
       "      <td>0.329125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.466700</td>\n",
       "      <td>0.356657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.466700</td>\n",
       "      <td>0.387810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/extend/lib/python3.9/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: answers, context, question.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1043\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to robertaLargefNil/checkpoint-198\n",
      "Configuration saved in robertaLargefNil/checkpoint-198/config.json\n",
      "Model weights saved in robertaLargefNil/checkpoint-198/pytorch_model.bin\n",
      "tokenizer config file saved in robertaLargefNil/checkpoint-198/tokenizer_config.json\n",
      "Special tokens file saved in robertaLargefNil/checkpoint-198/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: answers, context, question.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1043\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to robertaLargefNil/checkpoint-396\n",
      "Configuration saved in robertaLargefNil/checkpoint-396/config.json\n",
      "Model weights saved in robertaLargefNil/checkpoint-396/pytorch_model.bin\n",
      "tokenizer config file saved in robertaLargefNil/checkpoint-396/tokenizer_config.json\n",
      "Special tokens file saved in robertaLargefNil/checkpoint-396/special_tokens_map.json\n",
      "Deleting older checkpoint [robertaLargefNil/checkpoint-198] due to args.save_total_limit\n",
      "/opt/miniconda3/envs/extend/lib/python3.9/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: answers, context, question.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1043\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to robertaLargefNil/checkpoint-594\n",
      "Configuration saved in robertaLargefNil/checkpoint-594/config.json\n",
      "Model weights saved in robertaLargefNil/checkpoint-594/pytorch_model.bin\n",
      "tokenizer config file saved in robertaLargefNil/checkpoint-594/tokenizer_config.json\n",
      "Special tokens file saved in robertaLargefNil/checkpoint-594/special_tokens_map.json\n",
      "Deleting older checkpoint [robertaLargefNil/checkpoint-396] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: answers, context, question.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1043\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to robertaLargefNil/checkpoint-792\n",
      "Configuration saved in robertaLargefNil/checkpoint-792/config.json\n",
      "Model weights saved in robertaLargefNil/checkpoint-792/pytorch_model.bin\n",
      "tokenizer config file saved in robertaLargefNil/checkpoint-792/tokenizer_config.json\n",
      "Special tokens file saved in robertaLargefNil/checkpoint-792/special_tokens_map.json\n",
      "Deleting older checkpoint [robertaLargefNil/checkpoint-594] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: answers, context, question.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1043\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to robertaLargefNil/checkpoint-990\n",
      "Configuration saved in robertaLargefNil/checkpoint-990/config.json\n",
      "Model weights saved in robertaLargefNil/checkpoint-990/pytorch_model.bin\n",
      "tokenizer config file saved in robertaLargefNil/checkpoint-990/tokenizer_config.json\n",
      "Special tokens file saved in robertaLargefNil/checkpoint-990/special_tokens_map.json\n",
      "Deleting older checkpoint [robertaLargefNil/checkpoint-792] due to args.save_total_limit\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77766c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('./RoBertAAida')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71805d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b3f211-be57-4c73-9a66-1d65c7dade62",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3e969f",
   "metadata": {},
   "source": [
    "## Prediction and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f99facfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alzenau'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_brackets(input_string):\n",
    "    stack = []\n",
    "    brackets = {\"(\": \")\", \"{\": \"}\", \"[\": \"]\"}\n",
    "    for char in input_string:\n",
    "        if char in brackets.keys():\n",
    "            stack.append(char)\n",
    "        elif char in brackets.values():\n",
    "            if not stack or brackets[stack.pop()] != char:\n",
    "                return False\n",
    "    return not stack\n",
    "\n",
    "\n",
    "def process_answer(answer, candidates):\n",
    "    if answer == \"Not In Candidates\":\n",
    "        return answer\n",
    "    else:\n",
    "        modified_answer = answer.split(\"</ec>\")[0]\n",
    "        modified_answer = modified_answer.split(\": instance of \")[0]\n",
    "        modified_answer = modified_answer.split(\": instance\")[0]\n",
    "        modified_answer = modified_answer.replace(\"<s>\", \"\")\n",
    "        modified_answer = modified_answer.replace(\"</ec\", \"\")\n",
    "        modified_answer = modified_answer.replace(\"</s>\", \"\")\n",
    "        modified_answer = modified_answer.replace(\"<s\", \"\")\n",
    "        modified_answer = modified_answer.replace(\"</\", \"\")\n",
    "        modified_answer = modified_answer.replace(\" ec\", \"\")\n",
    "        modified_answer = modified_answer.replace(\">\", \"\")\n",
    "        modified_answer = modified_answer.strip()\n",
    "        if not check_brackets(modified_answer):\n",
    "            modified_answer = modified_answer.replace(\"(\", \"\")\n",
    "            modified_answer = modified_answer.replace(\")\", \"\")\n",
    "        if modified_answer == \"Not In Candidates\":\n",
    "            modified_answer = \"Not In Candidates\"\n",
    "        if modified_answer == \"\":\n",
    "            modified_answer = \"Not In Candidates\"\n",
    "        if modified_answer not in candidates:\n",
    "            modified_answer = \"Not In Candidates\"\n",
    "        return modified_answer\n",
    "\n",
    "\n",
    "process_answer(\"> Alzenau </\", [\"Alzenau\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1555cdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_context(text, n=200):\n",
    "    start_ent = \"[START_ENT]\"\n",
    "    end_ent = \"[END_ENT]\"\n",
    "\n",
    "    start_idx = text.find(start_ent)\n",
    "    end_idx = text.find(end_ent) + len(end_ent)\n",
    "\n",
    "    before_start_ent = text[max(0, start_idx - n) : start_idx]\n",
    "    mention = text[start_idx:end_idx]\n",
    "    after_end_ent = text[end_idx : end_idx + n]\n",
    "    return before_start_ent + mention + after_end_ent\n",
    "\n",
    "\n",
    "def entropy(p):\n",
    "    return -np.sum(p * np.log2(p))\n",
    "\n",
    "\n",
    "def make_prediction(data_entry, nil_prediction):\n",
    "    with torch.no_grad():\n",
    "        question = data_entry[\"input\"]\n",
    "        context = \"\"\n",
    "        # if nil_prediction:\n",
    "        #     context += \" Not In Candidates </ec> \"\n",
    "        index = 0\n",
    "        added = False\n",
    "        candidates = data_entry[\"candidates\"]\n",
    "        # random.shuffle(candidates)\n",
    "        for item in candidates:\n",
    "            context += item + f\" </ec> \"\n",
    "            index += 1\n",
    "            if index == 1 and nil_prediction:\n",
    "                context += \" Not In Candidates </ec> \"\n",
    "                added = True\n",
    "        if not added and nil_prediction:\n",
    "            context = \" Not In Candidates </ec> \" + context\n",
    "        input_pairs = [question, context]\n",
    "\n",
    "        encodings = tokenizer.encode_plus(\n",
    "            input_pairs, return_tensors=\"pt\", truncation=\"only_second\"\n",
    "        ).to(\"cuda\")\n",
    "        if len(encodings[\"input_ids\"][0]) > 512:\n",
    "            print(len(data_entry[\"candidates\"]))\n",
    "        start_scores, end_scores = model(\n",
    "            encodings[\"input_ids\"],\n",
    "            attention_mask=encodings[\"attention_mask\"],\n",
    "        )\n",
    "        start_scores = F.softmax(start_scores, dim=1)\n",
    "        end_scores = F.softmax(end_scores, dim=1)\n",
    "        startEntropy = entropy(start_scores[0].cpu().numpy())\n",
    "        endEntropy = entropy(end_scores[0].cpu().numpy())\n",
    "        # calculate mean entropy\n",
    "        meanEntropy = (startEntropy + endEntropy) / 2\n",
    "        # start_scores.to(\"cpu\")\n",
    "        # end_scores.to(\"cpu\")\n",
    "        start = torch.argmax(\n",
    "            start_scores\n",
    "        )  # Get the most likely beginning of answer with the argmax of the score\n",
    "        end = (\n",
    "            torch.argmax(end_scores) + 1\n",
    "        )  # Get the most likely end of answer with the argmax of the score\n",
    "        mean_score = 0\n",
    "        try:\n",
    "            mean_score = end_scores[0][end.item()] + start_scores[0][start.item()]\n",
    "        except:\n",
    "            None\n",
    "        answer_tokens = encodings[\"input_ids\"][0, start.item() : end.item() + 1]\n",
    "        answer = \"\"\n",
    "\n",
    "        answer = process_answer(tokenizer.decode(answer_tokens), context)\n",
    "        classified = 0\n",
    "        if (meanEntropy > 0.5785785785785785) and nil_prediction:\n",
    "            answer = \"Not In Candidates\"\n",
    "        else:\n",
    "            if answer == \"\":\n",
    "                answer = \"Not In Candidates\"\n",
    "        del encodings\n",
    "        del end_scores\n",
    "        del start_scores\n",
    "        del start\n",
    "        del end\n",
    "        score = float(mean_score)\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        return {\n",
    "            \"correct\": data_entry[\"output\"][0][\"answer\"],\n",
    "            \"non_processed\": tokenizer.decode(answer_tokens),\n",
    "            \"predicted\": answer,\n",
    "            \"input_phrase\": question,\n",
    "            \"scores\": score,\n",
    "            \"candidates\": context,\n",
    "            \"entropy\": meanEntropy,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255c023f-a73d-4d5e-a753-418ff74c0e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ee371f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/992 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 992/992 [01:46<00:00,  9.34it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "import torch\n",
    "\n",
    "results = []\n",
    "file_path = \"./nil_el_test_instanceof.jsonl\"\n",
    "dataset = []\n",
    "with open(file_path, \"r\") as file:\n",
    "    # dataset = json.load(file)\n",
    "    for line in file:\n",
    "        # Load each line as a JSON object\n",
    "        data_line = json.loads(line)\n",
    "\n",
    "        dataset.append(data_line)\n",
    "\n",
    "for item in tqdm(dataset):\n",
    "    try:\n",
    "        pred = make_prediction(item, True)\n",
    "        results.append(pred)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daca2118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'correct': 'New York City',\n",
       " 'non_processed': '> New York City : instance of city in the United States </ec',\n",
       " 'predicted': 'New York City',\n",
       " 'input_phrase': 'Stocks end back and forth day slightly higher [START_ENT] New York [END_ENT] Stocks finished an erratic session mixed Tuesday as higher commodity prices lifted energy and materials shares Major stock indexes had their third straight advance reaching new 13 month highs but the',\n",
       " 'scores': 0.9999992847442627,\n",
       " 'candidates': 'New York : instance of Wikimedia disambiguation page </ec>  Not In Candidates </ec> New York City : instance of city in the United States </ec> New York (magazine) : instance of magazine </ec> United States congressional delegations from New York : instance of Wikimedia list article </ec> New York (film) : instance of Unknown </ec> Province of New York : instance of crown colony </ec> Manhattan : instance of borough of New York City </ec> New York Knicks : instance of basketball team </ec> New York metropolitan area : instance of metropolitan statistical area </ec> New York GAA : instance of association </ec> New York Liberty : instance of basketball team </ec> New York Yankees : instance of baseball team </ec> New York Stock Exchange : instance of stock exchange </ec> Miss New York USA : instance of female beauty pageant </ec> New York (album) : instance of album </ec> New York Republican State Committee : instance of political party </ec> New York Mets : instance of baseball team </ec> New York-class battleship : instance of ship class </ec> John F. Kennedy International Airport : instance of commercial traffic aerodrome </ec> New York (Paloma Faith song) : instance of single </ec> New York (Ja Rule song) : instance of single </ec> Miss New York Teen USA : instance of teen competition </ec> Tiffany Pollard : instance of human </ec> New York Harbor : instance of administrative division </ec> Federal Reserve Bank of New York : instance of Federal Reserve Bank </ec> Pennsylvania Station (New York City) : instance of Unknown </ec> Miss New York : instance of female beauty pageant </ec> USS New York (ACR-2) : instance of ship </ec> Roman Catholic Archdiocese of New York : instance of Roman Catholic metropolitan archdiocese </ec> New York (U2 song) : instance of musical work/composition </ec> USS New York (BB-34) : instance of battleship </ec> New York University : instance of private not-for-profit educational institution </ec> New York, Texas : instance of unincorporated community in the United States </ec> New York Red Bulls : instance of association football team </ec> New York City Subway : instance of rapid transit </ec> New York Rangers : instance of ice hockey team </ec> New York Fashion Week : instance of recurring event </ec> September 11 attacks : instance of terrorist attack </ec> 1969 New York Mets season : instance of baseball team season </ec> New York State Capitol : instance of capitol building </ec> Brooklyn Navy Yard : instance of shipyard </ec> San Francisco Giants : instance of baseball team </ec> New York City Marathon : instance of recurring sporting event </ec> New York (typeface) : instance of Unknown </ec> New York Philharmonic : instance of symphony orchestra </ec> New York State Department of Transportation : instance of state department of transportation of the United States </ec> National Register of Historic Places listings in New York : instance of Wikimedia list article </ec> 1976 New York Yankees season : instance of baseball team season </ec> New York, New York (Tha Dogg Pound song) : instance of single </ec> New York gubernatorial election, 2002 : instance of Unknown </ec> Nassau County, New York : instance of county of New York </ec> East Coast hip hop : instance of musical scene </ec> WNBC : instance of television station </ec> New-York Mirror : instance of daily newspaper </ec> New York Jets : instance of American football team </ec> New York State Wildlife Management Areas : instance of wildlife refuge </ec> Rome, New York : instance of city in the state of New York </ec> 2007 New York Yankees season : instance of baseball team season </ec> New York Mountains : instance of mountain range </ec> Chuck Schumer : instance of human </ec> New York Cosmos : instance of Wikimedia disambiguation page </ec> New York, New York (Moby song) : instance of single </ec> New York Attorney General : instance of Unknown </ec> New York Sharks : instance of sports team </ec> New York State Bar Association : instance of bar association </ec> Daily Bugle : instance of fictional newspaper </ec> New York Theological Seminary : instance of private not-for-profit educational institution </ec> New York Film Festival : instance of film festival </ec> Genesee, New York : instance of town of New York </ec> New York State Route 94 : instance of road </ec> New York City bid for the 2012 Summer Olympics : instance of bids for Olympic Games </ec> New York City draft riots : instance of riot </ec> 1973 New York Mets season : instance of baseball team season </ec> Grand Central Terminal : instance of union station </ec> New York Bay : instance of bay </ec> New York, New York (So Good They Named It Twice) : instance of single </ec> United States District Court for the Southern District of New York : instance of United States district court </ec> Pulaski, New York : instance of town </ec> 1983–84 New York Knicks season : instance of basketball team season </ec> New York/New Jersey Rockers : instance of Unknown </ec> Newburgh (city), New York : instance of Unknown </ec> New York Express : instance of association football club </ec> New York locations by per capita income : instance of Unknown </ec> Madison Square Garden : instance of indoor arena </ec> Outline of New York : instance of Wikimedia outline article </ec> Koreatown, Manhattan : instance of Koreatown </ec> 1999 New York Mets season : instance of baseball team season </ec> New York Herald : instance of newspaper </ec> Metro New York : instance of daily newspaper </ec> Alexander Hamilton U.S. Custom House : instance of custom house </ec> Elmont, New York : instance of unincorporated community in the United States </ec> New York Black Yankees : instance of baseball team </ec> New York gubernatorial election, 2006 : instance of Unknown </ec> New York, New York (film) : instance of Unknown </ec> Time Warner Center : instance of Unknown </ec> New York Dragons : instance of sports team </ec> 1978 New York Yankees season : instance of baseball team season </ec> New York, North Yorkshire : instance of village </ec> 1977 New York Yankees season : instance of baseball team season </ec> New York Asian Film Festival : instance of film festival </ec> '}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5ba25a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 992/992 [00:00<00:00, 476658.22it/s]\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "correct_trace = []\n",
    "wrong = []\n",
    "correct_nil = 0\n",
    "wrong_nil = 0\n",
    "full_data = []\n",
    "for result in tqdm(results):\n",
    "    processed = result[\"predicted\"]\n",
    "    if processed == result[\"correct\"]:\n",
    "        correct += 1\n",
    "        correct_trace.append(result)\n",
    "        if processed == \"Not In Candidates\":\n",
    "            correct_nil += 1\n",
    "    else:\n",
    "        if result[\"correct\"] == \"Not In Candidates\":\n",
    "            wrong_nil += 1\n",
    "        wrong.append(result)\n",
    "    full_data.append(\n",
    "        {\n",
    "            \"score\": result[\"scores\"],\n",
    "            \"nil\": int(result[\"correct\"] == \"Not In Candidates\"),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d68f13ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of the model is 0.6381048387096774\n",
      "accuracy in nil prediction is: 0.8308157099697885\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy of the model is {correct/len(results)}\")\n",
    "print(f\"accuracy in nil prediction is: {correct_nil/(correct_nil+wrong_nil)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2939b435",
   "metadata": {},
   "source": [
    "zeshel acc = 0.4676229922214365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2024e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of the model is 0.836890243902439\n",
      "accuracy in nil prediction is: 0.5935483870967742\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy of the model is {correct/len(results)}\")\n",
    "print(f\"accuracy in nil prediction is: {correct_nil/(correct_nil+wrong_nil)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b285bc-679f-41a8-9859-3f594a4b276c",
   "metadata": {},
   "source": [
    "-  Base: 65.9%\n",
    "-  Large: 73.21%(enriched) - 74.65% (standard)\n",
    "-  Large EN: 76.66%\n",
    "-  large NER: 75.5%\n",
    "-  Base-NER: 66.35%\n",
    "\n",
    "## MSNBC large\n",
    "-  No EN: 91.92%\n",
    "-  No Instance Of: 92.83% - Yes: 93.44%\n",
    "-  EN: 92.8\n",
    "-  Large NER: 92.83%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b365299e-e8f0-4651-b752-ff7d6adcb5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1197d1e9-3e52-4ca4-a959-57f6751f6c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "full_df = pd.DataFrame(full_data)\n",
    "df = pd.DataFrame(wrong)\n",
    "df_c = pd.DataFrame(correct_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "878b3a7c-82f8-434d-8767-739109fcf191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Iraq War : instance of military offensive </ec> Iraq national football team : instance of national association football team </ec> Anglo-Iraqi War : instance of war </ec> Iraq Football Association : instance of association football federation </ec> Iraqi Army : instance of Unknown </ec> Iraq at the 2004 Summer Olympics : instance of Olympic delegation </ec> Iraq at the 2008 Summer Olympics : instance of Olympic delegation </ec> Iraq at the 1960 Summer Olympics : instance of Olympic delegation </ec> Iraq prison abuse scandals : instance of scandal </ec> Iraq at the Asian Games : instance of nation at sport competition </ec> Iraq national basketball team : instance of Unknown </ec> History of Iraq (2003–11) : instance of Unknown </ec> Iraq at the 2006 Asian Games : instance of nation at sport competition </ec> Iraqi Premier League : instance of Unknown </ec> Iraq national futsal team : instance of national futsal team </ec> Afro-Iraqi : instance of Unknown </ec> Iraq–Turkey relations : instance of bilateral relation </ec> Gulf War : instance of war </ec> Iraqi Canadian : instance of Unknown </ec> Iraqi Kurdistan : instance of geographic location </ec> Security Detachment Iraq (Australia) : instance of military unit </ec> Iraq al-Manshiyya : instance of village </ec> Opposition to the Iraq War : instance of political position </ec> Supreme Iraqi Criminal Tribunal : instance of Wikimedia list article </ec> Iraqi Air Force : instance of air force </ec> Embassy of Iraq in Washington, D.C. : instance of Unknown </ec> Operation Telic : instance of Unknown </ec> Iraq Liberation Act : instance of Act of Congress in the United States </ec> United States support for Iraq during the Iran–Iraq war : instance of Unknown </ec> Saddam Hussein : instance of human </ec> Iraq War troop surge of 2007 : instance of war </ec> Open for Business (album) : instance of Unknown </ec> `Iraq al Amir : instance of Unknown </ec> Iraq Suwaydan : instance of village </ec> Mesopotamian Arabic : instance of modern language </ec> Iraq national baseball team : instance of national sports team </ec> Iraqi revolt against the British : instance of Unknown </ec> RAF Iraq Command : instance of military unit </ec> Iraq Family Health Survey : instance of Unknown </ec> Iraqi records in athletics : instance of Unknown </ec> Iraqi people : instance of Unknown </ec> 2011 Iraqi protests : instance of protest </ec> Iraqi passport : instance of Unknown </ec> Iraq–United States relations : instance of bilateral relation </ec> Iraqi Communist Party : instance of communist party </ec> Iraq at the 1996 Summer Olympics : instance of Olympic delegation </ec> Iraqi security forces : instance of Security Forces </ec> China–Iraq relations : instance of bilateral relation </ec> Iraq at the Olympics : instance of Olympic delegation </ec> Iraqi dinar : instance of dinar </ec> Iraq at the 2009 World Championships in Athletics : instance of nation at the World Athletics Championships </ec> Denmark–Iraq relations : instance of bilateral relation </ec> Iraq and weapons of mass destruction : instance of aspect in a geographic region </ec> Iraq–Russia relations : instance of bilateral relation </ec> Iraq at the Paralympics : instance of Paralympics delegation </ec> Academi : instance of Unknown </ec> Iraq at the 2011 World Aquatics Championships : instance of nation at sport competition </ec> Iraqi Republic Railways : instance of national railway </ec> Iraq at the 1964 Summer Olympics : instance of Olympic delegation </ec> Iraq at the 2008 Summer Paralympics : instance of Paralympics delegation </ec> Iraq at the 2000 Summer Olympics : instance of Olympic delegation </ec> Iraqi Army Ranks Insignia : instance of Unknown </ec> Iraq–Pakistan relations : instance of bilateral relation </ec> Iraq at the 2010 Summer Youth Olympics : instance of nation at sport competition </ec> Iraq at the 1988 Summer Olympics : instance of Olympic delegation </ec> Iraq at the 1992 Summer Olympics : instance of Olympic delegation </ec> Iraq–United Kingdom relations : instance of bilateral relation </ec> Iraqi cuisine : instance of national cuisine </ec> Iraq–Israel relations : instance of bilateral relation </ec> Mawtini : instance of national anthem </ec> Not In Candidates </ec>  Not In Candidates </ec> '"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]['candidates']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43c3df84-751a-4ade-9364-a9078478e2a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correct</th>\n",
       "      <th>non_processed</th>\n",
       "      <th>predicted</th>\n",
       "      <th>input_phrase</th>\n",
       "      <th>scores</th>\n",
       "      <th>candidates</th>\n",
       "      <th>entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New York City</td>\n",
       "      <td>&lt;/s&gt;New York &lt;/ec</td>\n",
       "      <td>New York</td>\n",
       "      <td>Stocks end back and forth day slightly higher ...</td>\n",
       "      <td>0.947723</td>\n",
       "      <td>New York &lt;/ec&gt;  Not In Candidates &lt;/ec&gt; New Yo...</td>\n",
       "      <td>0.221581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not In Candidates</td>\n",
       "      <td>&lt;/s&gt;Reuters &lt;/ec</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>senior portfolio manager at ING Investment Man...</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>Reuters &lt;/ec&gt;  Not In Candidates &lt;/ec&gt; West (p...</td>\n",
       "      <td>0.000026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not In Candidates</td>\n",
       "      <td>&lt;/s&gt;Ministry of Employment and Labor &lt;/ec&gt;  No...</td>\n",
       "      <td>Ministry of Employment and Labor</td>\n",
       "      <td>past eight months as investors anticipate a re...</td>\n",
       "      <td>0.542618</td>\n",
       "      <td>Ministry of Employment and Labor &lt;/ec&gt;  Not In...</td>\n",
       "      <td>0.861312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Producer price index</td>\n",
       "      <td>&gt; U.S. Producer Price Index &lt;/ec</td>\n",
       "      <td>U.S. Producer Price Index</td>\n",
       "      <td>as investors anticipate a recovery in the econ...</td>\n",
       "      <td>0.999803</td>\n",
       "      <td>Producer price index &lt;/ec&gt;  Not In Candidates ...</td>\n",
       "      <td>0.005312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U.S. Steel</td>\n",
       "      <td>&lt;/s&gt; Not In Candidates &lt;/ec</td>\n",
       "      <td>Not In Candidates</td>\n",
       "      <td>a 12 year low in March A bounce in crude oil h...</td>\n",
       "      <td>0.999982</td>\n",
       "      <td>Not In Candidates &lt;/ec&gt;</td>\n",
       "      <td>0.000241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Not In Candidates</td>\n",
       "      <td>&lt;/s&gt;Russell Indexes &lt;/ec</td>\n",
       "      <td>Russell Indexes</td>\n",
       "      <td>Treasury note slipped to 3 33 percent from 3 3...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Russell Indexes &lt;/ec&gt;  Not In Candidates &lt;/ec&gt;</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Not In Candidates</td>\n",
       "      <td>&lt;/s&gt;Shinkansen &lt;/ec&gt;  Not In Candidates &lt;/ec</td>\n",
       "      <td>Shinkansen</td>\n",
       "      <td>stocks Exxon Mobil Corp rose 60 cents to 75 03...</td>\n",
       "      <td>0.519977</td>\n",
       "      <td>Shinkansen &lt;/ec&gt;  Not In Candidates &lt;/ec&gt; X-Le...</td>\n",
       "      <td>1.370051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Not In Candidates</td>\n",
       "      <td>&lt;/s&gt;United Kingdom &lt;/ec</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>03 while United States Steel Corp rose 93 cent...</td>\n",
       "      <td>0.999981</td>\n",
       "      <td>United Kingdom &lt;/ec&gt;  Not In Candidates &lt;/ec&gt; ...</td>\n",
       "      <td>0.005060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Not In Candidates</td>\n",
       "      <td>&gt; Xmas &lt;/ec</td>\n",
       "      <td>Xmas</td>\n",
       "      <td>Timberlake Diaz reportedly break up Former N S...</td>\n",
       "      <td>0.715395</td>\n",
       "      <td>Bastille Day &lt;/ec&gt;  Not In Candidates &lt;/ec&gt; Au...</td>\n",
       "      <td>1.547497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Not In Candidates</td>\n",
       "      <td>&lt;/s&gt;Vail, Arizona &lt;/ec</td>\n",
       "      <td>Vail, Arizona</td>\n",
       "      <td>Timberlake Diaz reportedly break up Former N S...</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>Vail, Arizona &lt;/ec&gt;  Not In Candidates &lt;/ec&gt; V...</td>\n",
       "      <td>0.004216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Innosense</td>\n",
       "      <td>Not In Candidates &lt;/ec</td>\n",
       "      <td>Not In Candidates</td>\n",
       "      <td>quits according to a report in Star magazine A...</td>\n",
       "      <td>0.731758</td>\n",
       "      <td>Innosense &lt;/ec&gt;  Not In Candidates &lt;/ec&gt;</td>\n",
       "      <td>0.729406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Innosense</td>\n",
       "      <td>Not In Candidates &lt;/ec</td>\n",
       "      <td>Not In Candidates</td>\n",
       "      <td>with her family in Vail Colo while Timberlake ...</td>\n",
       "      <td>0.519403</td>\n",
       "      <td>Finn (dinghy) &lt;/ec&gt;  Not In Candidates &lt;/ec&gt; F...</td>\n",
       "      <td>0.537991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Not In Candidates</td>\n",
       "      <td>&gt; Tara Conner &lt;/ec</td>\n",
       "      <td>Tara Conner</td>\n",
       "      <td>filed for business bankruptcies They are out o...</td>\n",
       "      <td>0.946796</td>\n",
       "      <td>Susie Castillo &lt;/ec&gt;  Not In Candidates &lt;/ec&gt; ...</td>\n",
       "      <td>0.656606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Not In Candidates</td>\n",
       "      <td>&gt; The Apprentice (TV series) &lt;/ec</td>\n",
       "      <td>The Apprentice (TV series)</td>\n",
       "      <td>underage drinking would keep her crown Trump i...</td>\n",
       "      <td>0.999982</td>\n",
       "      <td>The Apprentice (UK TV series) &lt;/ec&gt;  Not In Ca...</td>\n",
       "      <td>0.000890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Birmingham, Alabama</td>\n",
       "      <td>&lt;/s&gt;Birmingham &lt;/ec</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>Health insurance bridges gap for poor families...</td>\n",
       "      <td>0.998982</td>\n",
       "      <td>Birmingham &lt;/ec&gt;  Not In Candidates &lt;/ec&gt; Birm...</td>\n",
       "      <td>0.007625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Not In Candidates</td>\n",
       "      <td>&lt;/s&gt;United States &lt;/ec</td>\n",
       "      <td>United States</td>\n",
       "      <td>Health insurance bridges gap for poor families...</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>United States &lt;/ec&gt;  Not In Candidates &lt;/ec&gt; G...</td>\n",
       "      <td>0.000111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Not In Candidates</td>\n",
       "      <td>&lt;/s&gt;Blue Cross Blue Shield Association &lt;/ec</td>\n",
       "      <td>Blue Cross Blue Shield Association</td>\n",
       "      <td>Al Rohling watched parents quit their jobs whe...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Blue Cross Blue Shield Association &lt;/ec&gt;  Not ...</td>\n",
       "      <td>0.000011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Not In Candidates</td>\n",
       "      <td>&lt;/s&gt;Tuscaloosa County, Alabama &lt;/ec</td>\n",
       "      <td>Tuscaloosa County, Alabama</td>\n",
       "      <td>[START_ENT] Tuscaloosa [END_ENT] Ala Nick Saba...</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>Tuscaloosa County, Alabama &lt;/ec&gt;  Not In Candi...</td>\n",
       "      <td>0.000347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>University of Alabama</td>\n",
       "      <td>&gt; Alabama Crimson Tide football &lt;/ec</td>\n",
       "      <td>Alabama Crimson Tide football</td>\n",
       "      <td>TUSCALOOSA Ala Nick Saban couldn t escape his ...</td>\n",
       "      <td>0.995637</td>\n",
       "      <td>Alabama &lt;/ec&gt;  Not In Candidates &lt;/ec&gt; Univers...</td>\n",
       "      <td>0.038929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Not In Candidates</td>\n",
       "      <td>&lt;/s&gt;Miami &lt;/ec</td>\n",
       "      <td>Miami</td>\n",
       "      <td>TUSCALOOSA Ala Nick Saban couldn t escape his ...</td>\n",
       "      <td>0.999765</td>\n",
       "      <td>Miami &lt;/ec&gt;  Not In Candidates &lt;/ec&gt; Dolphins–...</td>\n",
       "      <td>0.112150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  correct                                      non_processed  \\\n",
       "0           New York City                                  </s>New York </ec   \n",
       "1       Not In Candidates                                   </s>Reuters </ec   \n",
       "2       Not In Candidates  </s>Ministry of Employment and Labor </ec>  No...   \n",
       "3    Producer price index                   > U.S. Producer Price Index </ec   \n",
       "4              U.S. Steel                        </s> Not In Candidates </ec   \n",
       "5       Not In Candidates                           </s>Russell Indexes </ec   \n",
       "6       Not In Candidates       </s>Shinkansen </ec>  Not In Candidates </ec   \n",
       "7       Not In Candidates                            </s>United Kingdom </ec   \n",
       "8       Not In Candidates                                        > Xmas </ec   \n",
       "9       Not In Candidates                             </s>Vail, Arizona </ec   \n",
       "10              Innosense                             Not In Candidates </ec   \n",
       "11              Innosense                             Not In Candidates </ec   \n",
       "12      Not In Candidates                                 > Tara Conner </ec   \n",
       "13      Not In Candidates                  > The Apprentice (TV series) </ec   \n",
       "14    Birmingham, Alabama                                </s>Birmingham </ec   \n",
       "15      Not In Candidates                             </s>United States </ec   \n",
       "16      Not In Candidates        </s>Blue Cross Blue Shield Association </ec   \n",
       "17      Not In Candidates                </s>Tuscaloosa County, Alabama </ec   \n",
       "18  University of Alabama               > Alabama Crimson Tide football </ec   \n",
       "19      Not In Candidates                                     </s>Miami </ec   \n",
       "\n",
       "                             predicted  \\\n",
       "0                             New York   \n",
       "1                              Reuters   \n",
       "2     Ministry of Employment and Labor   \n",
       "3            U.S. Producer Price Index   \n",
       "4                    Not In Candidates   \n",
       "5                      Russell Indexes   \n",
       "6                           Shinkansen   \n",
       "7                       United Kingdom   \n",
       "8                                 Xmas   \n",
       "9                        Vail, Arizona   \n",
       "10                   Not In Candidates   \n",
       "11                   Not In Candidates   \n",
       "12                         Tara Conner   \n",
       "13          The Apprentice (TV series)   \n",
       "14                          Birmingham   \n",
       "15                       United States   \n",
       "16  Blue Cross Blue Shield Association   \n",
       "17          Tuscaloosa County, Alabama   \n",
       "18       Alabama Crimson Tide football   \n",
       "19                               Miami   \n",
       "\n",
       "                                         input_phrase    scores  \\\n",
       "0   Stocks end back and forth day slightly higher ...  0.947723   \n",
       "1   senior portfolio manager at ING Investment Man...  0.999999   \n",
       "2   past eight months as investors anticipate a re...  0.542618   \n",
       "3   as investors anticipate a recovery in the econ...  0.999803   \n",
       "4   a 12 year low in March A bounce in crude oil h...  0.999982   \n",
       "5   Treasury note slipped to 3 33 percent from 3 3...  1.000000   \n",
       "6   stocks Exxon Mobil Corp rose 60 cents to 75 03...  0.519977   \n",
       "7   03 while United States Steel Corp rose 93 cent...  0.999981   \n",
       "8   Timberlake Diaz reportedly break up Former N S...  0.715395   \n",
       "9   Timberlake Diaz reportedly break up Former N S...  0.999993   \n",
       "10  quits according to a report in Star magazine A...  0.731758   \n",
       "11  with her family in Vail Colo while Timberlake ...  0.519403   \n",
       "12  filed for business bankruptcies They are out o...  0.946796   \n",
       "13  underage drinking would keep her crown Trump i...  0.999982   \n",
       "14  Health insurance bridges gap for poor families...  0.998982   \n",
       "15  Health insurance bridges gap for poor families...  0.999999   \n",
       "16  Al Rohling watched parents quit their jobs whe...  1.000000   \n",
       "17  [START_ENT] Tuscaloosa [END_ENT] Ala Nick Saba...  0.999992   \n",
       "18  TUSCALOOSA Ala Nick Saban couldn t escape his ...  0.995637   \n",
       "19  TUSCALOOSA Ala Nick Saban couldn t escape his ...  0.999765   \n",
       "\n",
       "                                           candidates   entropy  \n",
       "0   New York </ec>  Not In Candidates </ec> New Yo...  0.221581  \n",
       "1   Reuters </ec>  Not In Candidates </ec> West (p...  0.000026  \n",
       "2   Ministry of Employment and Labor </ec>  Not In...  0.861312  \n",
       "3   Producer price index </ec>  Not In Candidates ...  0.005312  \n",
       "4                            Not In Candidates </ec>   0.000241  \n",
       "5     Russell Indexes </ec>  Not In Candidates </ec>   0.000022  \n",
       "6   Shinkansen </ec>  Not In Candidates </ec> X-Le...  1.370051  \n",
       "7   United Kingdom </ec>  Not In Candidates </ec> ...  0.005060  \n",
       "8   Bastille Day </ec>  Not In Candidates </ec> Au...  1.547497  \n",
       "9   Vail, Arizona </ec>  Not In Candidates </ec> V...  0.004216  \n",
       "10          Innosense </ec>  Not In Candidates </ec>   0.729406  \n",
       "11  Finn (dinghy) </ec>  Not In Candidates </ec> F...  0.537991  \n",
       "12  Susie Castillo </ec>  Not In Candidates </ec> ...  0.656606  \n",
       "13  The Apprentice (UK TV series) </ec>  Not In Ca...  0.000890  \n",
       "14  Birmingham </ec>  Not In Candidates </ec> Birm...  0.007625  \n",
       "15  United States </ec>  Not In Candidates </ec> G...  0.000111  \n",
       "16  Blue Cross Blue Shield Association </ec>  Not ...  0.000011  \n",
       "17  Tuscaloosa County, Alabama </ec>  Not In Candi...  0.000347  \n",
       "18  Alabama </ec>  Not In Candidates </ec> Univers...  0.038929  \n",
       "19  Miami </ec>  Not In Candidates </ec> Dolphins–...  0.112150  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 20\n",
    "offset=0\n",
    "df.iloc[offset *n : offset*n + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "be007697-5520-4786-b03a-4fa61bf30b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./errorsNil.csv\")\n",
    "df_c.to_csv(\"./correctNil.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67de164f-a8a4-4241-b9a6-3b88323bd64a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
