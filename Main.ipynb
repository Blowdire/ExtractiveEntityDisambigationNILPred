{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d01515e8-c2c5-4f7e-bcae-6d8559f41239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # or \"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63af2596-b907-49cc-b6c3-2190e8af746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"tesi\", name=\"bigbird\")  # args = TrainingArguments(\n",
    "wandb.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2de9e6eb-0958-42eb-807b-04e51fd71df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from datasets import Dataset\n",
    "\n",
    "modelName = \"./robertaLargeIofNil/checkpoint-2178/\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelName)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(modelName, return_dict=False).to(\n",
    "    \"cuda:0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1362618-8cce-4759-8214-6f77876c33d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7939\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "tokenized = []\n",
    "file_path = \"./nil_el_instanceof.jsonl\"\n",
    "from datasets import Dataset\n",
    "\n",
    "original_data = []\n",
    "with open(file_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        # Load each line as a JSON object\n",
    "        data_line = json.loads(line)\n",
    "        \n",
    "        \n",
    "        original_data.append(data_line)\n",
    "print(len(original_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f04504f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18448\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "tokenized = []\n",
    "file_path = \"./aida_train_instanceof.jsonl\"\n",
    "from datasets import Dataset\n",
    "\n",
    "original_data_aida = []\n",
    "with open(file_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        # Load each line as a JSON object\n",
    "        data_line = json.loads(line)\n",
    "        \n",
    "        original_data_aida.append(data_line)\n",
    "print(len(original_data_aida))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db6f78a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(original_data_aida)\n",
    "#concat = original_data_aida\n",
    "concat =original_data_aida[:int(len(original_data_aida)* 0.7)] + original_data\n",
    "\n",
    "\n",
    "random.shuffle(concat)\n",
    "#print(len(concat))\n",
    "train_original = concat[: int(len(concat) * 0.95)]\n",
    "eval_original = concat[int(len(concat) * 0.95) : ]\n",
    "dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"context\": [item[\"question\"] for item in train_original],\n",
    "        \"question\": [item[\"context\"] for item in train_original],\n",
    "        \"answers\": [item[\"answers\"] for item in train_original],\n",
    "    }\n",
    ")\n",
    "dataset_ev = Dataset.from_dict(\n",
    "    {\n",
    "        \"context\": [item[\"question\"] for item in eval_original],\n",
    "        \"question\": [item[\"context\"] for item in eval_original],\n",
    "        \"answers\": [item[\"answers\"] for item in eval_original],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f94614f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_alignement(context, answer):\n",
    "    \"\"\"Some original examples in SQuAD have indices wrong by 1 or 2 character. We test and fix this here.\"\"\"\n",
    "    gold_text = answer[\"text\"][0]\n",
    "    \n",
    "    start_idx = context.find(gold_text)\n",
    "    end_idx = context.find(\"</ec>\", start_idx)\n",
    "    return start_idx, end_idx\n",
    "   \n",
    "\n",
    "\n",
    "# Tokenize our training dataset\n",
    "def convert_to_features(example):\n",
    "    try:\n",
    "        # Tokenize contexts and questions (as pairs of inputs)\n",
    "\n",
    "        input_pairs = [example[\"question\"], example[\"context\"]]\n",
    "        encodings = tokenizer.encode_plus(\n",
    "            input_pairs, pad_to_max_length=True, truncation=\"only_second\"\n",
    "        )\n",
    "        context_encodings = tokenizer.encode_plus(example[\"context\"])\n",
    "\n",
    "        # Compute start and end tokens for labels using Transformers's fast tokenizers alignement methodes.\n",
    "        # this will give us the position of answer span in the context text\n",
    "\n",
    "        start_idx, end_idx = get_correct_alignement(\n",
    "            example[\"context\"], example[\"answers\"]\n",
    "        )\n",
    "        if end_idx != -1 and start_idx != -1:\n",
    "            try:\n",
    "                start_positions_context = context_encodings.char_to_token(start_idx)\n",
    "                end_positions_context = context_encodings.char_to_token(end_idx)\n",
    "\n",
    "                # here we will compute the start and end position of the answer in the whole example\n",
    "                # as the example is encoded like this <s> question</s></s> context</s>\n",
    "                # and we know the postion of the answer in the context\n",
    "                # we can just find out the index of the sep token and then add that to position + 1 (+1 because there are two sep tokens)\n",
    "                # this will give us the position of the answer span in whole example\n",
    "                sep_idx = encodings[\"input_ids\"].index(tokenizer.sep_token_id)\n",
    "                start_positions = start_positions_context + sep_idx\n",
    "                end_positions = end_positions_context + sep_idx + 1\n",
    "               \n",
    "                encodings.update(\n",
    "                    {\n",
    "                        \"start_positions\": start_positions,\n",
    "                        \"end_positions\": end_positions,\n",
    "                        \"attention_mask\": encodings[\"attention_mask\"],\n",
    "                    }\n",
    "                )\n",
    "                return encodings\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        else:\n",
    "            print(\"qu\")\n",
    "            encodings.update(\n",
    "                {\n",
    "                    \"start_positions\": start_idx,\n",
    "                    \"end_positions\": end_idx,\n",
    "                    \"attention_mask\": encodings[\"attention_mask\"],\n",
    "                    \"is_nil\": 1 if example[\"answers\"][\"text\"][0] == \"Not In Candidates\" else 0\n",
    "                }\n",
    "            )\n",
    "            return encodings\n",
    "    except Exception as e:\n",
    "        print(\"errors\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352ecd7d-1d89-4113-bb3c-0fd8837334a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = dataset.map(convert_to_features)\n",
    "eval_ds = dataset_ev.map(convert_to_features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8bff14-4fde-4476-b465-6f6cc047553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03702e3-22f2-46b1-9387-a536b79d06e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump( train_ds, './dumps/aidaTrainIOF.dump',)\n",
    "joblib.dump( eval_ds, './dumps/aidaEvalIOF.dump',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb4bf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = joblib.load('./dumps/aidaTrainNERBigBird.dump')\n",
    "eval_ds = joblib.load('./dumps/aidaEvalNERBigBird.dump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca745971-3963-42de-af93-1282e75daab2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "from transformers import default_data_collator\n",
    "\n",
    "import wandb\n",
    "torch.cuda.set_device(0)\n",
    "torch.cuda.current_device()\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"robertaLargefNil\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"wandb\",\n",
    "    load_best_model_at_end=False,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    save_total_limit=1,\n",
    "    learning_rate=3e-5,\n",
    "    per_device_eval_batch_size=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=100,\n",
    "    num_train_epochs=14,\n",
    "    weight_decay=0.02,\n",
    "    fp16=True,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7ac65fb-0f2a-401f-9cb6-0c3d98ce3c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: answers, context, question.\n",
      "***** Running training *****\n",
      "  Num examples = 19809\n",
      "  Num Epochs = 14\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 100\n",
      "  Gradient Accumulation steps = 100\n",
      "  Total optimization steps = 2772\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33magazzi-ruben99\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.1 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">robertaLargefNil</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/agazzi-ruben99/huggingface\" target=\"_blank\">https://wandb.ai/agazzi-ruben99/huggingface</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/agazzi-ruben99/huggingface/runs/zp9jbbva\" target=\"_blank\">https://wandb.ai/agazzi-ruben99/huggingface/runs/zp9jbbva</a><br/>\n",
       "                Run data is saved locally in <code>/home/agazzi/wandb/run-20231218_194507-zp9jbbva</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1171' max='2772' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1171/2772 4:01:55 < 5:31:19, 0.08 it/s, Epoch 5.91/14]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.386881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.337372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.466700</td>\n",
       "      <td>0.329125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.466700</td>\n",
       "      <td>0.356657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.466700</td>\n",
       "      <td>0.387810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/extend/lib/python3.9/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: answers, context, question.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1043\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to robertaLargefNil/checkpoint-198\n",
      "Configuration saved in robertaLargefNil/checkpoint-198/config.json\n",
      "Model weights saved in robertaLargefNil/checkpoint-198/pytorch_model.bin\n",
      "tokenizer config file saved in robertaLargefNil/checkpoint-198/tokenizer_config.json\n",
      "Special tokens file saved in robertaLargefNil/checkpoint-198/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: answers, context, question.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1043\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to robertaLargefNil/checkpoint-396\n",
      "Configuration saved in robertaLargefNil/checkpoint-396/config.json\n",
      "Model weights saved in robertaLargefNil/checkpoint-396/pytorch_model.bin\n",
      "tokenizer config file saved in robertaLargefNil/checkpoint-396/tokenizer_config.json\n",
      "Special tokens file saved in robertaLargefNil/checkpoint-396/special_tokens_map.json\n",
      "Deleting older checkpoint [robertaLargefNil/checkpoint-198] due to args.save_total_limit\n",
      "/opt/miniconda3/envs/extend/lib/python3.9/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: answers, context, question.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1043\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to robertaLargefNil/checkpoint-594\n",
      "Configuration saved in robertaLargefNil/checkpoint-594/config.json\n",
      "Model weights saved in robertaLargefNil/checkpoint-594/pytorch_model.bin\n",
      "tokenizer config file saved in robertaLargefNil/checkpoint-594/tokenizer_config.json\n",
      "Special tokens file saved in robertaLargefNil/checkpoint-594/special_tokens_map.json\n",
      "Deleting older checkpoint [robertaLargefNil/checkpoint-396] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: answers, context, question.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1043\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to robertaLargefNil/checkpoint-792\n",
      "Configuration saved in robertaLargefNil/checkpoint-792/config.json\n",
      "Model weights saved in robertaLargefNil/checkpoint-792/pytorch_model.bin\n",
      "tokenizer config file saved in robertaLargefNil/checkpoint-792/tokenizer_config.json\n",
      "Special tokens file saved in robertaLargefNil/checkpoint-792/special_tokens_map.json\n",
      "Deleting older checkpoint [robertaLargefNil/checkpoint-594] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: answers, context, question.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1043\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to robertaLargefNil/checkpoint-990\n",
      "Configuration saved in robertaLargefNil/checkpoint-990/config.json\n",
      "Model weights saved in robertaLargefNil/checkpoint-990/pytorch_model.bin\n",
      "tokenizer config file saved in robertaLargefNil/checkpoint-990/tokenizer_config.json\n",
      "Special tokens file saved in robertaLargefNil/checkpoint-990/special_tokens_map.json\n",
      "Deleting older checkpoint [robertaLargefNil/checkpoint-792] due to args.save_total_limit\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77766c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('./RoBertAAida')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71805d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b3f211-be57-4c73-9a66-1d65c7dade62",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3e969f",
   "metadata": {},
   "source": [
    "## Prediction and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f99facfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alzenau'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_brackets(input_string):\n",
    "    stack = []\n",
    "    brackets = {\"(\": \")\", \"{\": \"}\", \"[\": \"]\"}\n",
    "    for char in input_string:\n",
    "        if char in brackets.keys():\n",
    "            stack.append(char)\n",
    "        elif char in brackets.values():\n",
    "            if not stack or brackets[stack.pop()] != char:\n",
    "                return False\n",
    "    return not stack\n",
    "\n",
    "\n",
    "def process_answer(answer, candidates):\n",
    "    if answer == \"Not In Candidates\":\n",
    "        return answer\n",
    "    else:\n",
    "        modified_answer = answer.split(\"</ec>\")[0]\n",
    "        modified_answer = modified_answer.split(\": instance of \")[0]\n",
    "        modified_answer = modified_answer.split(\": instance\")[0]\n",
    "        modified_answer = modified_answer.replace(\"<s>\", \"\")\n",
    "        modified_answer = modified_answer.replace(\"</ec\", \"\")\n",
    "        modified_answer = modified_answer.replace(\"</s>\", \"\")\n",
    "        modified_answer = modified_answer.replace(\"<s\", \"\")\n",
    "        modified_answer = modified_answer.replace(\"</\", \"\")\n",
    "        modified_answer = modified_answer.replace(\" ec\", \"\")\n",
    "        modified_answer = modified_answer.replace(\">\", \"\")\n",
    "        modified_answer = modified_answer.strip()\n",
    "        if not check_brackets(modified_answer):\n",
    "            modified_answer = modified_answer.replace(\"(\", \"\")\n",
    "            modified_answer = modified_answer.replace(\")\", \"\")\n",
    "        if modified_answer == \"Not In Candidates\":\n",
    "            modified_answer = \"Not In Candidates\"\n",
    "        if modified_answer == \"\":\n",
    "            modified_answer = \"Not In Candidates\"\n",
    "        if modified_answer not in candidates:\n",
    "            modified_answer = \"Not In Candidates\"\n",
    "        return modified_answer\n",
    "\n",
    "\n",
    "process_answer(\"> Alzenau </\", [\"Alzenau\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1555cdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_context(text, n=200):\n",
    "    start_ent = \"[START_ENT]\"\n",
    "    end_ent = \"[END_ENT]\"\n",
    "\n",
    "    start_idx = text.find(start_ent)\n",
    "    end_idx = text.find(end_ent) + len(end_ent)\n",
    "\n",
    "    before_start_ent = text[max(0, start_idx - n) : start_idx]\n",
    "    mention = text[start_idx:end_idx]\n",
    "    after_end_ent = text[end_idx : end_idx + n]\n",
    "    return before_start_ent + mention + after_end_ent\n",
    "\n",
    "\n",
    "def entropy(p):\n",
    "    return -np.sum(p * np.log2(p))\n",
    "\n",
    "\n",
    "def make_prediction(data_entry, nil_prediction):\n",
    "    with torch.no_grad():\n",
    "        question = data_entry[\"input\"]\n",
    "        context = \"\"\n",
    "        # if nil_prediction:\n",
    "        #     context += \" Not In Candidates </ec> \"\n",
    "        index = 0\n",
    "        added = False\n",
    "        candidates = data_entry[\"candidates\"]\n",
    "        # random.shuffle(candidates)\n",
    "        for item in candidates:\n",
    "            context += item + f\" </ec> \"\n",
    "            index += 1\n",
    "            if index == 1 and nil_prediction:\n",
    "                context += \" Not In Candidates </ec> \"\n",
    "                added = True\n",
    "        if not added and nil_prediction:\n",
    "            context = \" Not In Candidates </ec> \" + context\n",
    "        input_pairs = [question, context]\n",
    "\n",
    "        encodings = tokenizer.encode_plus(\n",
    "            input_pairs, return_tensors=\"pt\", truncation=\"only_second\"\n",
    "        ).to(\"cuda\")\n",
    "        if len(encodings[\"input_ids\"][0]) > 512:\n",
    "            print(len(data_entry[\"candidates\"]))\n",
    "        start_scores, end_scores = model(\n",
    "            encodings[\"input_ids\"],\n",
    "            attention_mask=encodings[\"attention_mask\"],\n",
    "        )\n",
    "        start_scores = F.softmax(start_scores, dim=1)\n",
    "        end_scores = F.softmax(end_scores, dim=1)\n",
    "        startEntropy = entropy(start_scores[0].cpu().numpy())\n",
    "        endEntropy = entropy(end_scores[0].cpu().numpy())\n",
    "        # calculate mean entropy\n",
    "        meanEntropy = (startEntropy + endEntropy) / 2\n",
    "        # start_scores.to(\"cpu\")\n",
    "        # end_scores.to(\"cpu\")\n",
    "        start = torch.argmax(\n",
    "            start_scores\n",
    "        )  # Get the most likely beginning of answer with the argmax of the score\n",
    "        end = (\n",
    "            torch.argmax(end_scores) + 1\n",
    "        )  # Get the most likely end of answer with the argmax of the score\n",
    "        mean_score = 0\n",
    "        try:\n",
    "            mean_score = end_scores[0][end.item()] + start_scores[0][start.item()]\n",
    "        except:\n",
    "            None\n",
    "        answer_tokens = encodings[\"input_ids\"][0, start.item() : end.item() + 1]\n",
    "        answer = \"\"\n",
    "\n",
    "        answer = process_answer(tokenizer.decode(answer_tokens), context)\n",
    "        classified = 0\n",
    "        if (meanEntropy > 0.5785785785785785) and nil_prediction:\n",
    "            answer = \"Not In Candidates\"\n",
    "        else:\n",
    "            if answer == \"\":\n",
    "                answer = \"Not In Candidates\"\n",
    "        del encodings\n",
    "        del end_scores\n",
    "        del start_scores\n",
    "        del start\n",
    "        del end\n",
    "        score = float(mean_score)\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        return {\n",
    "            \"correct\": data_entry[\"output\"][0][\"answer\"],\n",
    "            \"non_processed\": tokenizer.decode(answer_tokens),\n",
    "            \"predicted\": answer,\n",
    "            \"input_phrase\": question,\n",
    "            \"scores\": score,\n",
    "            \"candidates\": context,\n",
    "            \"entropy\": meanEntropy,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255c023f-a73d-4d5e-a753-418ff74c0e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ee371f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/992 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 992/992 [01:46<00:00,  9.34it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "import torch\n",
    "\n",
    "results = []\n",
    "file_path = \"./nil_el_test_instanceof.jsonl\"\n",
    "dataset = []\n",
    "with open(file_path, \"r\") as file:\n",
    "    # dataset = json.load(file)\n",
    "    for line in file:\n",
    "        # Load each line as a JSON object\n",
    "        data_line = json.loads(line)\n",
    "\n",
    "        dataset.append(data_line)\n",
    "\n",
    "for item in tqdm(dataset):\n",
    "    try:\n",
    "        pred = make_prediction(item, True)\n",
    "        results.append(pred)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daca2118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'correct': 'New York City',\n",
       " 'non_processed': '> New York City : instance of city in the United States </ec',\n",
       " 'predicted': 'New York City',\n",
       " 'input_phrase': 'Stocks end back and forth day slightly higher [START_ENT] New York [END_ENT] Stocks finished an erratic session mixed Tuesday as higher commodity prices lifted energy and materials shares Major stock indexes had their third straight advance reaching new 13 month highs but the',\n",
       " 'scores': 0.9999992847442627,\n",
       " 'candidates': 'New York : instance of Wikimedia disambiguation page </ec>  Not In Candidates </ec> New York City : instance of city in the United States </ec> New York (magazine) : instance of magazine </ec> United States congressional delegations from New York : instance of Wikimedia list article </ec> New York (film) : instance of Unknown </ec> Province of New York : instance of crown colony </ec> Manhattan : instance of borough of New York City </ec> New York Knicks : instance of basketball team </ec> New York metropolitan area : instance of metropolitan statistical area </ec> New York GAA : instance of association </ec> New York Liberty : instance of basketball team </ec> New York Yankees : instance of baseball team </ec> New York Stock Exchange : instance of stock exchange </ec> Miss New York USA : instance of female beauty pageant </ec> New York (album) : instance of album </ec> New York Republican State Committee : instance of political party </ec> New York Mets : instance of baseball team </ec> New York-class battleship : instance of ship class </ec> John F. Kennedy International Airport : instance of commercial traffic aerodrome </ec> New York (Paloma Faith song) : instance of single </ec> New York (Ja Rule song) : instance of single </ec> Miss New York Teen USA : instance of teen competition </ec> Tiffany Pollard : instance of human </ec> New York Harbor : instance of administrative division </ec> Federal Reserve Bank of New York : instance of Federal Reserve Bank </ec> Pennsylvania Station (New York City) : instance of Unknown </ec> Miss New York : instance of female beauty pageant </ec> USS New York (ACR-2) : instance of ship </ec> Roman Catholic Archdiocese of New York : instance of Roman Catholic metropolitan archdiocese </ec> New York (U2 song) : instance of musical work/composition </ec> USS New York (BB-34) : instance of battleship </ec> New York University : instance of private not-for-profit educational institution </ec> New York, Texas : instance of unincorporated community in the United States </ec> New York Red Bulls : instance of association football team </ec> New York City Subway : instance of rapid transit </ec> New York Rangers : instance of ice hockey team </ec> New York Fashion Week : instance of recurring event </ec> September 11 attacks : instance of terrorist attack </ec> 1969 New York Mets season : instance of baseball team season </ec> New York State Capitol : instance of capitol building </ec> Brooklyn Navy Yard : instance of shipyard </ec> San Francisco Giants : instance of baseball team </ec> New York City Marathon : instance of recurring sporting event </ec> New York (typeface) : instance of Unknown </ec> New York Philharmonic : instance of symphony orchestra </ec> New York State Department of Transportation : instance of state department of transportation of the United States </ec> National Register of Historic Places listings in New York : instance of Wikimedia list article </ec> 1976 New York Yankees season : instance of baseball team season </ec> New York, New York (Tha Dogg Pound song) : instance of single </ec> New York gubernatorial election, 2002 : instance of Unknown </ec> Nassau County, New York : instance of county of New York </ec> East Coast hip hop : instance of musical scene </ec> WNBC : instance of television station </ec> New-York Mirror : instance of daily newspaper </ec> New York Jets : instance of American football team </ec> New York State Wildlife Management Areas : instance of wildlife refuge </ec> Rome, New York : instance of city in the state of New York </ec> 2007 New York Yankees season : instance of baseball team season </ec> New York Mountains : instance of mountain range </ec> Chuck Schumer : instance of human </ec> New York Cosmos : instance of Wikimedia disambiguation page </ec> New York, New York (Moby song) : instance of single </ec> New York Attorney General : instance of Unknown </ec> New York Sharks : instance of sports team </ec> New York State Bar Association : instance of bar association </ec> Daily Bugle : instance of fictional newspaper </ec> New York Theological Seminary : instance of private not-for-profit educational institution </ec> New York Film Festival : instance of film festival </ec> Genesee, New York : instance of town of New York </ec> New York State Route 94 : instance of road </ec> New York City bid for the 2012 Summer Olympics : instance of bids for Olympic Games </ec> New York City draft riots : instance of riot </ec> 1973 New York Mets season : instance of baseball team season </ec> Grand Central Terminal : instance of union station </ec> New York Bay : instance of bay </ec> New York, New York (So Good They Named It Twice) : instance of single </ec> United States District Court for the Southern District of New York : instance of United States district court </ec> Pulaski, New York : instance of town </ec> 1983–84 New York Knicks season : instance of basketball team season </ec> New York/New Jersey Rockers : instance of Unknown </ec> Newburgh (city), New York : instance of Unknown </ec> New York Express : instance of association football club </ec> New York locations by per capita income : instance of Unknown </ec> Madison Square Garden : instance of indoor arena </ec> Outline of New York : instance of Wikimedia outline article </ec> Koreatown, Manhattan : instance of Koreatown </ec> 1999 New York Mets season : instance of baseball team season </ec> New York Herald : instance of newspaper </ec> Metro New York : instance of daily newspaper </ec> Alexander Hamilton U.S. Custom House : instance of custom house </ec> Elmont, New York : instance of unincorporated community in the United States </ec> New York Black Yankees : instance of baseball team </ec> New York gubernatorial election, 2006 : instance of Unknown </ec> New York, New York (film) : instance of Unknown </ec> Time Warner Center : instance of Unknown </ec> New York Dragons : instance of sports team </ec> 1978 New York Yankees season : instance of baseball team season </ec> New York, North Yorkshire : instance of village </ec> 1977 New York Yankees season : instance of baseball team season </ec> New York Asian Film Festival : instance of film festival </ec> '}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5ba25a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 992/992 [00:00<00:00, 476658.22it/s]\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "correct_trace = []\n",
    "wrong = []\n",
    "correct_nil = 0\n",
    "wrong_nil = 0\n",
    "full_data = []\n",
    "for result in tqdm(results):\n",
    "    processed = result[\"predicted\"]\n",
    "    if processed == result[\"correct\"]:\n",
    "        correct += 1\n",
    "        correct_trace.append(result)\n",
    "        if processed == \"Not In Candidates\":\n",
    "            correct_nil += 1\n",
    "    else:\n",
    "        if result[\"correct\"] == \"Not In Candidates\":\n",
    "            wrong_nil += 1\n",
    "        wrong.append(result)\n",
    "    full_data.append(\n",
    "        {\n",
    "            \"score\": result[\"scores\"],\n",
    "            \"nil\": int(result[\"correct\"] == \"Not In Candidates\"),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d68f13ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of the model is 0.6381048387096774\n",
      "accuracy in nil prediction is: 0.8308157099697885\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy of the model is {correct/len(results)}\")\n",
    "print(f\"accuracy in nil prediction is: {correct_nil/(correct_nil+wrong_nil)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2939b435",
   "metadata": {},
   "source": [
    "zeshel acc = 0.4676229922214365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2024e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of the model is 0.836890243902439\n",
      "accuracy in nil prediction is: 0.5935483870967742\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy of the model is {correct/len(results)}\")\n",
    "print(f\"accuracy in nil prediction is: {correct_nil/(correct_nil+wrong_nil)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
