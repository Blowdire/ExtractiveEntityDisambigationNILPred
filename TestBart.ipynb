{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d01515e8-c2c5-4f7e-bcae-6d8559f41239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # or \"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63af2596-b907-49cc-b6c3-2190e8af746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"tesi\", name=\"bigbird\")  # args = TrainingArguments(\n",
    "wandb.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2de9e6eb-0958-42eb-807b-04e51fd71df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.0.4) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-large and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from datasets import Dataset\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "    \"facebook/bart-large\", return_dict=False\n",
    ").to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1362618-8cce-4759-8214-6f77876c33d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7939\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "tokenized = []\n",
    "file_path = \"./nil_el_instanceof.jsonl\"\n",
    "from datasets import Dataset\n",
    "\n",
    "original_data = []\n",
    "with open(file_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        # Load each line as a JSON object\n",
    "        data_line = json.loads(line)\n",
    "        \n",
    "        \n",
    "        original_data.append(data_line)\n",
    "print(len(original_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f04504f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18448\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "tokenized = []\n",
    "file_path = \"./aida_train_instanceof.jsonl\"\n",
    "from datasets import Dataset\n",
    "\n",
    "original_data_aida = []\n",
    "with open(file_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        # Load each line as a JSON object\n",
    "        data_line = json.loads(line)\n",
    "        \n",
    "        # additional_question = \"<additional> \"\n",
    "        # try:\n",
    "        #     for related in data_line[\"most_related\"]:\n",
    "        #         length = len(related)\n",
    "        #         if len(related) == 4:\n",
    "        #             additional_question += related[2]\n",
    "        #             additional_question += \" \"\n",
    "        # except:\n",
    "        #     None\n",
    "        # additional_question += \"</additional>\"\n",
    "        # data_line[\"context\"] += additional_question\n",
    "        original_data_aida.append(data_line)\n",
    "print(len(original_data_aida))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db6f78a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(original_data_aida)\n",
    "#concat = original_data_aida\n",
    "concat =original_data_aida[:int(len(original_data_aida)* 0.4)] + original_data\n",
    "\n",
    "\n",
    "random.shuffle(concat)\n",
    "#print(len(concat))\n",
    "train_original = concat[: int(len(concat) * 0.95)]\n",
    "eval_original = concat[int(len(concat) * 0.95) : ]\n",
    "dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"context\": [item[\"question\"] for item in train_original],\n",
    "        \"question\": [item[\"context\"] for item in train_original],\n",
    "        \"answers\": [item[\"answers\"] for item in train_original],\n",
    "    }\n",
    ")\n",
    "dataset_ev = Dataset.from_dict(\n",
    "    {\n",
    "        \"context\": [item[\"question\"] for item in eval_original],\n",
    "        \"question\": [item[\"context\"] for item in eval_original],\n",
    "        \"answers\": [item[\"answers\"] for item in eval_original],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f94614f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_alignement(context, answer):\n",
    "    \"\"\"Some original examples in SQuAD have indices wrong by 1 or 2 character. We test and fix this here.\"\"\"\n",
    "    gold_text = answer[\"text\"][0]\n",
    "    \n",
    "    start_idx = context.find(gold_text)\n",
    "    end_idx = context.find(\"</ec>\", start_idx)\n",
    "    return start_idx, end_idx\n",
    "    # if context[start_idx:end_idx] == gold_text:\n",
    "    #     return start_idx, end_idx       # When the gold label position is good\n",
    "    # elif context[start_idx-1:end_idx-1] == gold_text:\n",
    "    #     return start_idx-1, end_idx-1   # When the gold label is off by one character\n",
    "    # elif context[start_idx-2:end_idx-2] == gold_text:\n",
    "    #     return start_idx-2, end_idx-2   # When the gold label is off by two character\n",
    "    # else:\n",
    "    #     raise ValueError()\n",
    "\n",
    "\n",
    "# Tokenize our training dataset\n",
    "def convert_to_features(example):\n",
    "    try:\n",
    "        # Tokenize contexts and questions (as pairs of inputs)\n",
    "\n",
    "        input_pairs = [example[\"question\"], example[\"context\"]]\n",
    "        encodings = tokenizer.encode_plus(\n",
    "            input_pairs,max_length=512, pad_to_max_length=False, truncation=\"only_second\"\n",
    "        )\n",
    "        context_encodings = tokenizer.encode_plus(example[\"context\"])\n",
    "\n",
    "        # Compute start and end tokens for labels using Transformers's fast tokenizers alignement methodes.\n",
    "        # this will give us the position of answer span in the context text\n",
    "\n",
    "        start_idx, end_idx = get_correct_alignement(\n",
    "            example[\"context\"], example[\"answers\"]\n",
    "        )\n",
    "        if end_idx != -1 and start_idx != -1:\n",
    "            try:\n",
    "                start_positions_context = context_encodings.char_to_token(start_idx)\n",
    "                end_positions_context = context_encodings.char_to_token(end_idx)\n",
    "\n",
    "                # here we will compute the start and end position of the answer in the whole example\n",
    "                # as the example is encoded like this <s> question</s></s> context</s>\n",
    "                # and we know the postion of the answer in the context\n",
    "                # we can just find out the index of the sep token and then add that to position + 1 (+1 because there are two sep tokens)\n",
    "                # this will give us the position of the answer span in whole example\n",
    "                sep_idx = encodings[\"input_ids\"].index(tokenizer.sep_token_id)\n",
    "                start_positions = start_positions_context + sep_idx\n",
    "                end_positions = end_positions_context + sep_idx + 1\n",
    "                #print(tokenizer.decode\n",
    "                #(encodings[\"input_ids\"][start_positions:end_positions]))\n",
    "                encodings.update(\n",
    "                    {\n",
    "                        \"start_positions\": start_positions,\n",
    "                        \"end_positions\": end_positions,\n",
    "                        \"attention_mask\": encodings[\"attention_mask\"],\n",
    "                    }\n",
    "                )\n",
    "                return encodings\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        else:\n",
    "            print(\"qu\")\n",
    "            encodings.update(\n",
    "                {\n",
    "                    \"start_positions\": start_idx,\n",
    "                    \"end_positions\": end_idx,\n",
    "                    \"attention_mask\": encodings[\"attention_mask\"],\n",
    "                }\n",
    "            )\n",
    "            return encodings\n",
    "    except Exception as e:\n",
    "        print(\"errors\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "352ecd7d-1d89-4113-bb3c-0fd8837334a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f74701706ea74d70bd01e5a6e22a20c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14552 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qu\n",
      "qu\n",
      "qu\n",
      "qu\n",
      "qu\n",
      "qu\n",
      "qu\n",
      "qu\n",
      "qu\n",
      "qu\n",
      "qu\n",
      "qu\n",
      "qu\n",
      "qu\n",
      "qu\n",
      "qu\n",
      "qu\n",
      "qu\n",
      "qu\n",
      "qu\n",
      "qu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae1f229f345b4dc6affdd562e69582fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/766 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qu\n"
     ]
    }
   ],
   "source": [
    "train_ds = dataset.map(convert_to_features)\n",
    "eval_ds = dataset_ev.map(convert_to_features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8bff14-4fde-4476-b465-6f6cc047553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03702e3-22f2-46b1-9387-a536b79d06e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump( train_ds, './dumps/aidaTrainIOF.dump',)\n",
    "joblib.dump( eval_ds, './dumps/aidaEvalIOF.dump',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb4bf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = joblib.load('./dumps/aidaTrainNERBigBird.dump')\n",
    "eval_ds = joblib.load('./dumps/aidaEvalNERBigBird.dump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e60b5c-b03e-4ebf-888a-d0a10390b3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca745971-3963-42de-af93-1282e75daab2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "from transformers import default_data_collator\n",
    "\n",
    "import wandb\n",
    "torch.cuda.set_device(0)\n",
    "torch.cuda.current_device()\n",
    "\n",
    "#wandb.init(project=\"tesi\", name=\"robertaAidaLargeNERNIL\")  # args = TrainingArguments(\n",
    "#     f\"longformer-finetuned-squad\",\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=8,\n",
    "#     num_train_epochs=3,\n",
    "#     weight_decay=0.01,\n",
    "# )\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"bart\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    report_to=\"wandb\",\n",
    "    load_best_model_at_end=False,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    save_total_limit=1,\n",
    "    learning_rate=3e-5,\n",
    "    per_device_eval_batch_size=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=10,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.02,\n",
    "    fp16=True,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7ac65fb-0f2a-401f-9cb6-0c3d98ce3c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rub/.local/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.0.4) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33magazzi-ruben99\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rub/Documents/Borsa/Tesi/TESTFS/wandb/run-20231222_135030-hdllai79</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/agazzi-ruben99/huggingface/runs/hdllai79' target=\"_blank\">vibrant-dust-158</a></strong> to <a href='https://wandb.ai/agazzi-ruben99/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/agazzi-ruben99/huggingface' target=\"_blank\">https://wandb.ai/agazzi-ruben99/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/agazzi-ruben99/huggingface/runs/hdllai79' target=\"_blank\">https://wandb.ai/agazzi-ruben99/huggingface/runs/hdllai79</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3ede36bf9d04516a976bcff92d020e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1455 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8309, 'learning_rate': 2.802061855670103e-05, 'epoch': 0.07}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf4dcdc7a36d48d88a8ccc96d6ce914a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/766 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_runtime': 26.559, 'eval_samples_per_second': 28.841, 'eval_steps_per_second': 28.841, 'epoch': 0.07}\n",
      "{'loss': 1.1225, 'learning_rate': 2.595876288659794e-05, 'epoch': 0.14}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "550c37481c974680869cd7547f0bb912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/766 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_runtime': 26.1439, 'eval_samples_per_second': 29.299, 'eval_steps_per_second': 29.299, 'epoch': 0.14}\n",
      "{'loss': 0.8399, 'learning_rate': 2.3896907216494846e-05, 'epoch': 0.21}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "348bf4839aa2442283e53a45c9d9f45e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/766 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_runtime': 26.3725, 'eval_samples_per_second': 29.045, 'eval_steps_per_second': 29.045, 'epoch': 0.21}\n",
      "{'loss': 0.7346, 'learning_rate': 2.1855670103092783e-05, 'epoch': 0.27}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2861e14b950f4414897ce5a38f829ffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/766 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_runtime': 26.5305, 'eval_samples_per_second': 28.872, 'eval_steps_per_second': 28.872, 'epoch': 0.27}\n",
      "{'loss': 0.7959, 'learning_rate': 1.9793814432989692e-05, 'epoch': 0.34}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d74bf17c8fd4404297903d53f37fc156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/766 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_runtime': 25.9997, 'eval_samples_per_second': 29.462, 'eval_steps_per_second': 29.462, 'epoch': 0.34}\n",
      "{'loss': 0.6692, 'learning_rate': 1.7731958762886598e-05, 'epoch': 0.41}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f5bbad0f3ae421fbfed462286268449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/766 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_runtime': 25.8819, 'eval_samples_per_second': 29.596, 'eval_steps_per_second': 29.596, 'epoch': 0.41}\n",
      "{'loss': 0.6604, 'learning_rate': 1.5670103092783507e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc8e3099ad124c5c8b7cd9e97e3654c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/766 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_runtime': 26.3322, 'eval_samples_per_second': 29.09, 'eval_steps_per_second': 29.09, 'epoch': 0.48}\n",
      "{'loss': 0.5415, 'learning_rate': 1.3608247422680413e-05, 'epoch': 0.55}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e3bf392ce904c30b870a3512745a2f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/766 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_runtime': 25.6111, 'eval_samples_per_second': 29.909, 'eval_steps_per_second': 29.909, 'epoch': 0.55}\n",
      "{'loss': 0.5353, 'learning_rate': 1.154639175257732e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "816392d323fc42f2ab6e073ff243e4b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/766 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_runtime': 26.3493, 'eval_samples_per_second': 29.071, 'eval_steps_per_second': 29.071, 'epoch': 0.62}\n",
      "{'loss': 0.5225, 'learning_rate': 9.484536082474226e-06, 'epoch': 0.69}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c34b03fb142a41e198c766807ac712c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/766 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_runtime': 26.309, 'eval_samples_per_second': 29.116, 'eval_steps_per_second': 29.116, 'epoch': 0.69}\n",
      "{'loss': 0.5418, 'learning_rate': 7.422680412371135e-06, 'epoch': 0.76}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76176f18d4914cff9380292d1648b516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/766 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_runtime': 25.6514, 'eval_samples_per_second': 29.862, 'eval_steps_per_second': 29.862, 'epoch': 0.76}\n",
      "{'loss': 0.5262, 'learning_rate': 5.360824742268042e-06, 'epoch': 0.82}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a3da531156440da94a7b9f26659e5ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/766 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_runtime': 26.1082, 'eval_samples_per_second': 29.339, 'eval_steps_per_second': 29.339, 'epoch': 0.82}\n",
      "{'loss': 0.425, 'learning_rate': 3.2989690721649484e-06, 'epoch': 0.89}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a574293f1c314bc8acde9d313dba82ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/766 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_runtime': 26.5478, 'eval_samples_per_second': 28.854, 'eval_steps_per_second': 28.854, 'epoch': 0.89}\n",
      "{'loss': 0.4573, 'learning_rate': 1.2371134020618557e-06, 'epoch': 0.96}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11a9d63f8cc7421ca6064c0b18b02bc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/766 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_runtime': 26.5341, 'eval_samples_per_second': 28.868, 'eval_steps_per_second': 28.868, 'epoch': 0.96}\n",
      "{'train_runtime': 2408.7444, 'train_samples_per_second': 6.041, 'train_steps_per_second': 0.604, 'train_loss': 0.7171860921014216, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1455, training_loss=0.7171860921014216, metrics={'train_runtime': 2408.7444, 'train_samples_per_second': 6.041, 'train_steps_per_second': 0.604, 'train_loss': 0.7171860921014216, 'epoch': 1.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77766c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('./RoBertAAida')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71805d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b3f211-be57-4c73-9a66-1d65c7dade62",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3e969f",
   "metadata": {},
   "source": [
    "## Prediction and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f99facfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alzenau'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_brackets(input_string):\n",
    "    stack = []\n",
    "    brackets = {\"(\": \")\", \"{\": \"}\", \"[\": \"]\"}\n",
    "    for char in input_string:\n",
    "        if char in brackets.keys():\n",
    "            stack.append(char)\n",
    "        elif char in brackets.values():\n",
    "            if not stack or brackets[stack.pop()] != char:\n",
    "                return False\n",
    "    return not stack\n",
    "\n",
    "\n",
    "def process_answer(answer, candidates):\n",
    "    if answer == \"Not In Candidates\":\n",
    "        return answer\n",
    "    else:\n",
    "        modified_answer = answer.split(\"</ec>\")[0]\n",
    "        modified_answer = modified_answer.split(\": instance of \")[0]\n",
    "        modified_answer = modified_answer.split(\": instance\")[0]\n",
    "        modified_answer = modified_answer.replace(\"<s>\", \"\")\n",
    "        modified_answer = modified_answer.replace(\"</ec\", \"\")\n",
    "        modified_answer = modified_answer.replace(\"</s>\", \"\")\n",
    "        modified_answer = modified_answer.replace(\"<s\", \"\")\n",
    "        modified_answer = modified_answer.replace(\"</\", \"\")\n",
    "        modified_answer = modified_answer.replace(\" ec\", \"\")\n",
    "        modified_answer = modified_answer.replace(\">\", \"\")\n",
    "        modified_answer = modified_answer.strip()\n",
    "        if not check_brackets(modified_answer):\n",
    "            modified_answer = modified_answer.replace(\"(\", \"\")\n",
    "            modified_answer = modified_answer.replace(\")\", \"\")\n",
    "        if modified_answer == \"Not In Candidates\":\n",
    "            modified_answer = \"NIL\"\n",
    "        if modified_answer == \"\":\n",
    "            modified_answer = \"Not In Candidates\"\n",
    "        if modified_answer not in candidates:\n",
    "            modified_answer = \"Not In Candidates\"\n",
    "        return modified_answer\n",
    "\n",
    "\n",
    "process_answer(\"> Alzenau </\", [\"Alzenau\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1555cdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def make_prediction(data_entry, nil_prediction):\n",
    "    with torch.no_grad():\n",
    "        question = data_entry[\"input\"]\n",
    "        context = \"\"\n",
    "        # if nil_prediction:\n",
    "        #     context += \" Not In Candidates </ec> \"\n",
    "        index = 0\n",
    "        added = False\n",
    "        for item in data_entry[\"candidates\"]:\n",
    "            context += item + f\" </ec> \"\n",
    "            index += 1\n",
    "            if index == 1 and nil_prediction:\n",
    "                context += \" Not In Candidates : instance of Unknown </ec> \"\n",
    "                added = True\n",
    "        if not added and nil_prediction:\n",
    "            context = \" Not In Candidates : instance of Unknown </ec> \" + context\n",
    "        input_pairs = [question, context]\n",
    "\n",
    "        encodings = tokenizer.encode_plus(\n",
    "            input_pairs, return_tensors=\"pt\", truncation=\"only_second\"\n",
    "        ).to(\"cuda\")\n",
    "        start_scores, end_scores = model(\n",
    "            encodings[\"input_ids\"], attention_mask=encodings[\"attention_mask\"]\n",
    "        )\n",
    "        start_scores = F.softmax(start_scores, dim=1)\n",
    "        end_scores = F.softmax(end_scores, dim=1)\n",
    "        # start_scores.to(\"cpu\")\n",
    "        # end_scores.to(\"cpu\")\n",
    "        start = torch.argmax(\n",
    "            start_scores\n",
    "        )  # Get the most likely beginning of answer with the argmax of the score\n",
    "        end = (\n",
    "            torch.argmax(end_scores) + 1\n",
    "        )  # Get the most likely end of answer with the argmax of the score\n",
    "        mean_score = 0\n",
    "        try:\n",
    "            mean_score = end_scores[0][end.item()] + start_scores[0][start.item()]\n",
    "        except:\n",
    "            None\n",
    "        answer_tokens = encodings[\"input_ids\"][0, start.item() : end.item() + 1]\n",
    "        answer = \"\"\n",
    "\n",
    "        answer = process_answer(tokenizer.decode(answer_tokens), context)\n",
    "        classified = 0\n",
    "        if (mean_score < 0.9) and nil_prediction:\n",
    "            answer = \"Not In Candidates\"\n",
    "        else:\n",
    "            if answer == \"\":\n",
    "                answer = \"Not In Candidates\"\n",
    "        del encodings\n",
    "        del end_scores\n",
    "        del start_scores\n",
    "        del start\n",
    "        del end\n",
    "        score = float(mean_score)\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        return {\n",
    "            \"correct\": data_entry[\"output\"],\n",
    "            \"non_processed\": tokenizer.decode(answer_tokens),\n",
    "            \"predicted\": answer,\n",
    "            \"input_phrase\": question,\n",
    "            \"scores\": score,\n",
    "            \"candidates\": context,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255c023f-a73d-4d5e-a753-418ff74c0e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3ee371f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/656 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 656/656 [01:23<00:00,  7.83it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "import torch\n",
    "\n",
    "results = []\n",
    "file_path = \"./Datasets/InstanceOf/msnbc_test_instanceof-nil.jsonl\"\n",
    "dataset = []\n",
    "with open(file_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        # Load each line as a JSON object\n",
    "        data_line = json.loads(line)\n",
    "\n",
    "        dataset.append(data_line)\n",
    "\n",
    "for item in tqdm(dataset):\n",
    "    try:\n",
    "        pred = make_prediction(item, True)\n",
    "        results.append(pred)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "daca2118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "549"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b5ba25a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 656/656 [00:00<00:00, 183504.30it/s]\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "correct_trace = []\n",
    "wrong = []\n",
    "correct_nil = 0\n",
    "wrong_nil = 0\n",
    "full_data = []\n",
    "for result in tqdm(results):\n",
    "  processed = process_answer(result['non_processed'], result['candidates'])\n",
    "  if processed == result['correct'][0]['answer']:\n",
    "    correct += 1\n",
    "    correct_trace.append(result)\n",
    "    if processed == 'Not In Candidates':\n",
    "        correct_nil += 1\n",
    "  else:\n",
    "    if result['correct'][0]['answer'] == 'Not In Candidates':\n",
    "        wrong_nil += 1\n",
    "    wrong.append(result)\n",
    "  full_data.append({'score': result['scores'], 'nil': int(result['correct'][0]['answer'] == 'Not In Candidates')} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d68f13ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of the model is 0.8414634146341463\n",
      "accuracy in nil prediction is: 0.6903225806451613\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy of the model is {correct/len(results)}\")\n",
    "print(f\"accuracy in nil prediction is: {correct_nil/(correct_nil+wrong_nil)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2024e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of the model is 0.836890243902439\n",
      "accuracy in nil prediction is: 0.5935483870967742\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy of the model is {correct/len(results)}\")\n",
    "print(f\"accuracy in nil prediction is: {correct_nil/(correct_nil+wrong_nil)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b285bc-679f-41a8-9859-3f594a4b276c",
   "metadata": {},
   "source": [
    "-  Base: 65.9%\n",
    "-  Large: 73.21%(enriched) - 74.65% (standard)\n",
    "-  Large EN: 76.66%\n",
    "-  large NER: 75.5%\n",
    "-  Base-NER: 66.35%\n",
    "\n",
    "## MSNBC large\n",
    "-  No EN: 91.92%\n",
    "-  No Instance Of: 92.83% - Yes: 93.44%\n",
    "-  EN: 92.8\n",
    "-  Large NER: 92.83%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b365299e-e8f0-4651-b752-ff7d6adcb5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1197d1e9-3e52-4ca4-a959-57f6751f6c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "full_df = pd.DataFrame(full_data)\n",
    "df = pd.DataFrame(wrong)\n",
    "df_c = pd.DataFrame(correct_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "878b3a7c-82f8-434d-8767-739109fcf191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Iraq War : instance of military offensive </ec> Iraq national football team : instance of national association football team </ec> Anglo-Iraqi War : instance of war </ec> Iraq Football Association : instance of association football federation </ec> Iraqi Army : instance of Unknown </ec> Iraq at the 2004 Summer Olympics : instance of Olympic delegation </ec> Iraq at the 2008 Summer Olympics : instance of Olympic delegation </ec> Iraq at the 1960 Summer Olympics : instance of Olympic delegation </ec> Iraq prison abuse scandals : instance of scandal </ec> Iraq at the Asian Games : instance of nation at sport competition </ec> Iraq national basketball team : instance of Unknown </ec> History of Iraq (2003–11) : instance of Unknown </ec> Iraq at the 2006 Asian Games : instance of nation at sport competition </ec> Iraqi Premier League : instance of Unknown </ec> Iraq national futsal team : instance of national futsal team </ec> Afro-Iraqi : instance of Unknown </ec> Iraq–Turkey relations : instance of bilateral relation </ec> Gulf War : instance of war </ec> Iraqi Canadian : instance of Unknown </ec> Iraqi Kurdistan : instance of geographic location </ec> Security Detachment Iraq (Australia) : instance of military unit </ec> Iraq al-Manshiyya : instance of village </ec> Opposition to the Iraq War : instance of political position </ec> Supreme Iraqi Criminal Tribunal : instance of Wikimedia list article </ec> Iraqi Air Force : instance of air force </ec> Embassy of Iraq in Washington, D.C. : instance of Unknown </ec> Operation Telic : instance of Unknown </ec> Iraq Liberation Act : instance of Act of Congress in the United States </ec> United States support for Iraq during the Iran–Iraq war : instance of Unknown </ec> Saddam Hussein : instance of human </ec> Iraq War troop surge of 2007 : instance of war </ec> Open for Business (album) : instance of Unknown </ec> `Iraq al Amir : instance of Unknown </ec> Iraq Suwaydan : instance of village </ec> Mesopotamian Arabic : instance of modern language </ec> Iraq national baseball team : instance of national sports team </ec> Iraqi revolt against the British : instance of Unknown </ec> RAF Iraq Command : instance of military unit </ec> Iraq Family Health Survey : instance of Unknown </ec> Iraqi records in athletics : instance of Unknown </ec> Iraqi people : instance of Unknown </ec> 2011 Iraqi protests : instance of protest </ec> Iraqi passport : instance of Unknown </ec> Iraq–United States relations : instance of bilateral relation </ec> Iraqi Communist Party : instance of communist party </ec> Iraq at the 1996 Summer Olympics : instance of Olympic delegation </ec> Iraqi security forces : instance of Security Forces </ec> China–Iraq relations : instance of bilateral relation </ec> Iraq at the Olympics : instance of Olympic delegation </ec> Iraqi dinar : instance of dinar </ec> Iraq at the 2009 World Championships in Athletics : instance of nation at the World Athletics Championships </ec> Denmark–Iraq relations : instance of bilateral relation </ec> Iraq and weapons of mass destruction : instance of aspect in a geographic region </ec> Iraq–Russia relations : instance of bilateral relation </ec> Iraq at the Paralympics : instance of Paralympics delegation </ec> Academi : instance of Unknown </ec> Iraq at the 2011 World Aquatics Championships : instance of nation at sport competition </ec> Iraqi Republic Railways : instance of national railway </ec> Iraq at the 1964 Summer Olympics : instance of Olympic delegation </ec> Iraq at the 2008 Summer Paralympics : instance of Paralympics delegation </ec> Iraq at the 2000 Summer Olympics : instance of Olympic delegation </ec> Iraqi Army Ranks Insignia : instance of Unknown </ec> Iraq–Pakistan relations : instance of bilateral relation </ec> Iraq at the 2010 Summer Youth Olympics : instance of nation at sport competition </ec> Iraq at the 1988 Summer Olympics : instance of Olympic delegation </ec> Iraq at the 1992 Summer Olympics : instance of Olympic delegation </ec> Iraq–United Kingdom relations : instance of bilateral relation </ec> Iraqi cuisine : instance of national cuisine </ec> Iraq–Israel relations : instance of bilateral relation </ec> Mawtini : instance of national anthem </ec> Not In Candidates </ec>  Not In Candidates </ec> '"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]['candidates']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "43c3df84-751a-4ade-9364-a9078478e2a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correct</th>\n",
       "      <th>non_processed</th>\n",
       "      <th>predicted</th>\n",
       "      <th>input_phrase</th>\n",
       "      <th>scores</th>\n",
       "      <th>candidates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[{'answer': 'Not In Candidates', 'provenance':...</td>\n",
       "      <td>&lt;/s&gt;Tuscaloosa County, Alabama : instance of c...</td>\n",
       "      <td>Tuscaloosa County, Alabama</td>\n",
       "      <td>[START_ENT] Tuscaloosa [END_ENT] Ala Nick Saba...</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>Tuscaloosa County, Alabama : instance of count...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[{'answer': 'University of Alabama', 'provenan...</td>\n",
       "      <td>&lt;/s&gt;Alabama : instance of U.S. state &lt;/ec</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>TUSCALOOSA Ala Nick Saban couldn t escape his ...</td>\n",
       "      <td>0.957006</td>\n",
       "      <td>Alabama : instance of U.S. state &lt;/ec&gt; Univers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[{'answer': 'Not In Candidates', 'provenance':...</td>\n",
       "      <td>&lt;/s&gt;Miami : instance of city in the United Sta...</td>\n",
       "      <td>Miami</td>\n",
       "      <td>TUSCALOOSA Ala Nick Saban couldn t escape his ...</td>\n",
       "      <td>0.994787</td>\n",
       "      <td>Miami : instance of city in the United States ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[{'answer': 'Alabama Crimson Tide', 'provenanc...</td>\n",
       "      <td>&gt; Alabama Crimson Tide football : instance of ...</td>\n",
       "      <td>Alabama Crimson Tide football</td>\n",
       "      <td>TUSCALOOSA Ala Nick Saban couldn t escape his ...</td>\n",
       "      <td>0.999967</td>\n",
       "      <td>Crimson Tide (film) : instance of film &lt;/ec&gt; A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[{'answer': 'Alabama', 'provenance': [{'title'...</td>\n",
       "      <td>&gt; Alabama Crimson Tide football : instance of ...</td>\n",
       "      <td>Alabama Crimson Tide football</td>\n",
       "      <td>to leave the Miami Dolphins In other words he ...</td>\n",
       "      <td>0.981053</td>\n",
       "      <td>Alabama : instance of U.S. state &lt;/ec&gt; Univers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[{'answer': 'Alabama Crimson Tide', 'provenanc...</td>\n",
       "      <td>&gt; Alabama Crimson Tide football : instance of ...</td>\n",
       "      <td>Alabama Crimson Tide football</td>\n",
       "      <td>s here I love it here he said I like to affect...</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>Tide : instance of Unknown &lt;/ec&gt; Tide (brand) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[{'answer': 'University of Alabama', 'provenan...</td>\n",
       "      <td>&gt; Alabama Crimson Tide football : instance of ...</td>\n",
       "      <td>Alabama Crimson Tide football</td>\n",
       "      <td>in the past It s what you do now he said His r...</td>\n",
       "      <td>0.999062</td>\n",
       "      <td>Alabama : instance of U.S. state &lt;/ec&gt; Univers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[{'answer': 'Louisiana State University', 'pro...</td>\n",
       "      <td>&gt; LSU Tigers football : instance of college sp...</td>\n",
       "      <td>LSU Tigers football</td>\n",
       "      <td>It s what you do now he said His resume featur...</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>Louisiana State University : instance of publi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[{'answer': 'Not In Candidates', 'provenance':...</td>\n",
       "      <td>&lt;/s&gt;Miami : instance of city in the United Sta...</td>\n",
       "      <td>Miami</td>\n",
       "      <td>Alabama fans hunger for most A national title ...</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>Miami : instance of city in the United States ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[{'answer': 'University of Alabama', 'provenan...</td>\n",
       "      <td>&gt; Alabama Crimson Tide football : instance of ...</td>\n",
       "      <td>Alabama Crimson Tide football</td>\n",
       "      <td>fans hunger for most A national title His cham...</td>\n",
       "      <td>0.999566</td>\n",
       "      <td>Alabama : instance of U.S. state &lt;/ec&gt; Univers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[{'answer': 'Alabama Crimson Tide', 'provenanc...</td>\n",
       "      <td>&gt; Alabama Crimson Tide football : instance of ...</td>\n",
       "      <td>Alabama Crimson Tide football</td>\n",
       "      <td>are why the Tide was willing to offer a report...</td>\n",
       "      <td>0.998872</td>\n",
       "      <td>Tide : instance of Unknown &lt;/ec&gt; Tide (brand) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[{'answer': 'University of Alabama', 'provenan...</td>\n",
       "      <td>&gt; Alabama Crimson Tide football : instance of ...</td>\n",
       "      <td>Alabama Crimson Tide football</td>\n",
       "      <td>worth an estimated 32 million plus incentives ...</td>\n",
       "      <td>0.943538</td>\n",
       "      <td>Alabama : instance of U.S. state &lt;/ec&gt; Univers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[{'answer': 'University of Alabama', 'provenan...</td>\n",
       "      <td>&lt;/s&gt;Alabama : instance of U.S. state &lt;/ec</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>1999 under Mike DuBose The Tide s latest natio...</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>Alabama : instance of U.S. state &lt;/ec&gt; Univers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[{'answer': 'Alabama Crimson Tide', 'provenanc...</td>\n",
       "      <td>&gt; Alabama Crimson Tide football : instance of ...</td>\n",
       "      <td>Alabama Crimson Tide football</td>\n",
       "      <td>president chemistry professor John Vincent had...</td>\n",
       "      <td>0.997708</td>\n",
       "      <td>Tide : instance of Unknown &lt;/ec&gt; Tide (brand) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[{'answer': 'Not In Candidates', 'provenance':...</td>\n",
       "      <td>&lt;/s&gt;Florida State Seminoles football : instanc...</td>\n",
       "      <td>Florida State Seminoles football</td>\n",
       "      <td>positive view The money doesn t come out of th...</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>Florida State Seminoles football : instance of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[{'answer': 'Kevin Steele', 'provenance': [{'t...</td>\n",
       "      <td></td>\n",
       "      <td>Not In Candidates</td>\n",
       "      <td>money doesn t come out of the academic side of...</td>\n",
       "      <td>0.917817</td>\n",
       "      <td>Not In Candidates : instance of Unknown &lt;/ec&gt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[{'answer': 'Not In Candidates', 'provenance':...</td>\n",
       "      <td>&gt; Florida State Seminoles : instance of univer...</td>\n",
       "      <td>Florida State Seminoles</td>\n",
       "      <td>of the university The academic side is self su...</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>Seminole : instance of ethnic group &lt;/ec&gt; Flor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[{'answer': 'Louisiana State University', 'pro...</td>\n",
       "      <td>&gt; LSU Tigers football : instance of college sp...</td>\n",
       "      <td>LSU Tigers football</td>\n",
       "      <td>hiring offers much to the university Vincent s...</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>Louisiana State University : instance of publi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[{'answer': 'Not In Candidates', 'provenance':...</td>\n",
       "      <td>&gt; Notre Dame Fighting Irish : instance of univ...</td>\n",
       "      <td>Not In Candidates</td>\n",
       "      <td>having a high profile coach like Saban would c...</td>\n",
       "      <td>0.490671</td>\n",
       "      <td>University of Notre Dame : instance of academi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[{'answer': 'Not In Candidates', 'provenance':...</td>\n",
       "      <td>&lt;/s&gt;Tuscaloosa County, Alabama : instance of c...</td>\n",
       "      <td>Tuscaloosa County, Alabama</td>\n",
       "      <td>and money to the academic side Saban said his ...</td>\n",
       "      <td>0.999973</td>\n",
       "      <td>Tuscaloosa County, Alabama : instance of count...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              correct  \\\n",
       "20  [{'answer': 'Not In Candidates', 'provenance':...   \n",
       "21  [{'answer': 'University of Alabama', 'provenan...   \n",
       "22  [{'answer': 'Not In Candidates', 'provenance':...   \n",
       "23  [{'answer': 'Alabama Crimson Tide', 'provenanc...   \n",
       "24  [{'answer': 'Alabama', 'provenance': [{'title'...   \n",
       "25  [{'answer': 'Alabama Crimson Tide', 'provenanc...   \n",
       "26  [{'answer': 'University of Alabama', 'provenan...   \n",
       "27  [{'answer': 'Louisiana State University', 'pro...   \n",
       "28  [{'answer': 'Not In Candidates', 'provenance':...   \n",
       "29  [{'answer': 'University of Alabama', 'provenan...   \n",
       "30  [{'answer': 'Alabama Crimson Tide', 'provenanc...   \n",
       "31  [{'answer': 'University of Alabama', 'provenan...   \n",
       "32  [{'answer': 'University of Alabama', 'provenan...   \n",
       "33  [{'answer': 'Alabama Crimson Tide', 'provenanc...   \n",
       "34  [{'answer': 'Not In Candidates', 'provenance':...   \n",
       "35  [{'answer': 'Kevin Steele', 'provenance': [{'t...   \n",
       "36  [{'answer': 'Not In Candidates', 'provenance':...   \n",
       "37  [{'answer': 'Louisiana State University', 'pro...   \n",
       "38  [{'answer': 'Not In Candidates', 'provenance':...   \n",
       "39  [{'answer': 'Not In Candidates', 'provenance':...   \n",
       "\n",
       "                                        non_processed  \\\n",
       "20  </s>Tuscaloosa County, Alabama : instance of c...   \n",
       "21          </s>Alabama : instance of U.S. state </ec   \n",
       "22  </s>Miami : instance of city in the United Sta...   \n",
       "23  > Alabama Crimson Tide football : instance of ...   \n",
       "24  > Alabama Crimson Tide football : instance of ...   \n",
       "25  > Alabama Crimson Tide football : instance of ...   \n",
       "26  > Alabama Crimson Tide football : instance of ...   \n",
       "27  > LSU Tigers football : instance of college sp...   \n",
       "28  </s>Miami : instance of city in the United Sta...   \n",
       "29  > Alabama Crimson Tide football : instance of ...   \n",
       "30  > Alabama Crimson Tide football : instance of ...   \n",
       "31  > Alabama Crimson Tide football : instance of ...   \n",
       "32          </s>Alabama : instance of U.S. state </ec   \n",
       "33  > Alabama Crimson Tide football : instance of ...   \n",
       "34  </s>Florida State Seminoles football : instanc...   \n",
       "35                                                      \n",
       "36  > Florida State Seminoles : instance of univer...   \n",
       "37  > LSU Tigers football : instance of college sp...   \n",
       "38  > Notre Dame Fighting Irish : instance of univ...   \n",
       "39  </s>Tuscaloosa County, Alabama : instance of c...   \n",
       "\n",
       "                           predicted  \\\n",
       "20        Tuscaloosa County, Alabama   \n",
       "21                           Alabama   \n",
       "22                             Miami   \n",
       "23     Alabama Crimson Tide football   \n",
       "24     Alabama Crimson Tide football   \n",
       "25     Alabama Crimson Tide football   \n",
       "26     Alabama Crimson Tide football   \n",
       "27               LSU Tigers football   \n",
       "28                             Miami   \n",
       "29     Alabama Crimson Tide football   \n",
       "30     Alabama Crimson Tide football   \n",
       "31     Alabama Crimson Tide football   \n",
       "32                           Alabama   \n",
       "33     Alabama Crimson Tide football   \n",
       "34  Florida State Seminoles football   \n",
       "35                 Not In Candidates   \n",
       "36           Florida State Seminoles   \n",
       "37               LSU Tigers football   \n",
       "38                 Not In Candidates   \n",
       "39        Tuscaloosa County, Alabama   \n",
       "\n",
       "                                         input_phrase    scores  \\\n",
       "20  [START_ENT] Tuscaloosa [END_ENT] Ala Nick Saba...  0.999993   \n",
       "21  TUSCALOOSA Ala Nick Saban couldn t escape his ...  0.957006   \n",
       "22  TUSCALOOSA Ala Nick Saban couldn t escape his ...  0.994787   \n",
       "23  TUSCALOOSA Ala Nick Saban couldn t escape his ...  0.999967   \n",
       "24  to leave the Miami Dolphins In other words he ...  0.981053   \n",
       "25  s here I love it here he said I like to affect...  0.999995   \n",
       "26  in the past It s what you do now he said His r...  0.999062   \n",
       "27  It s what you do now he said His resume featur...  0.999994   \n",
       "28  Alabama fans hunger for most A national title ...  0.999993   \n",
       "29  fans hunger for most A national title His cham...  0.999566   \n",
       "30  are why the Tide was willing to offer a report...  0.998872   \n",
       "31  worth an estimated 32 million plus incentives ...  0.943538   \n",
       "32  1999 under Mike DuBose The Tide s latest natio...  0.999995   \n",
       "33  president chemistry professor John Vincent had...  0.997708   \n",
       "34  positive view The money doesn t come out of th...  0.999995   \n",
       "35  money doesn t come out of the academic side of...  0.917817   \n",
       "36  of the university The academic side is self su...  0.999995   \n",
       "37  hiring offers much to the university Vincent s...  0.999996   \n",
       "38  having a high profile coach like Saban would c...  0.490671   \n",
       "39  and money to the academic side Saban said his ...  0.999973   \n",
       "\n",
       "                                           candidates  \n",
       "20  Tuscaloosa County, Alabama : instance of count...  \n",
       "21  Alabama : instance of U.S. state </ec> Univers...  \n",
       "22  Miami : instance of city in the United States ...  \n",
       "23  Crimson Tide (film) : instance of film </ec> A...  \n",
       "24  Alabama : instance of U.S. state </ec> Univers...  \n",
       "25  Tide : instance of Unknown </ec> Tide (brand) ...  \n",
       "26  Alabama : instance of U.S. state </ec> Univers...  \n",
       "27  Louisiana State University : instance of publi...  \n",
       "28  Miami : instance of city in the United States ...  \n",
       "29  Alabama : instance of U.S. state </ec> Univers...  \n",
       "30  Tide : instance of Unknown </ec> Tide (brand) ...  \n",
       "31  Alabama : instance of U.S. state </ec> Univers...  \n",
       "32  Alabama : instance of U.S. state </ec> Univers...  \n",
       "33  Tide : instance of Unknown </ec> Tide (brand) ...  \n",
       "34  Florida State Seminoles football : instance of...  \n",
       "35   Not In Candidates : instance of Unknown </ec>...  \n",
       "36  Seminole : instance of ethnic group </ec> Flor...  \n",
       "37  Louisiana State University : instance of publi...  \n",
       "38  University of Notre Dame : instance of academi...  \n",
       "39  Tuscaloosa County, Alabama : instance of count...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 20\n",
    "offset=1\n",
    "df.iloc[offset *n : offset*n + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "be007697-5520-4786-b03a-4fa61bf30b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./errorsNil.csv\")\n",
    "df_c.to_csv(\"./correctNil.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67de164f-a8a4-4241-b9a6-3b88323bd64a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
