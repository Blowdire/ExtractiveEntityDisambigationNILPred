{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.0.4) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy_entity_linker.EntityLinker.EntityLinker at 0x7fa57ff167a0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.pipeline.entity_linker import DEFAULT_NEL_MODEL\n",
    "from spacy.kb import KnowledgeBase\n",
    "\n",
    "nlp.add_pipe(\"entityLinker\", last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "def get_wikidata_id_from_url(url):\n",
    "    path = urlparse(url).path\n",
    "    wikidata_id = path.split(\"/\")[-1]\n",
    "    return wikidata_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7939 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7939/7939 [01:30<00:00, 87.81it/s] \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "filepath = \"./train_converted2.jsonl\"\n",
    "\n",
    "processed = []\n",
    "\n",
    "with open(filepath, \"r\") as file:\n",
    "    for line in file:\n",
    "        # Load each line as a JSON object\n",
    "        data_line = json.loads(line)\n",
    "        # Extract the text and the annotations\n",
    "        for item in tqdm(data_line):\n",
    "            text = item[\"context\"]\n",
    "            doc = nlp(text)\n",
    "            linked_ents = []\n",
    "            ents = doc._.linkedEntities\n",
    "            for ent in ents:\n",
    "                linked_ents.append(\n",
    "                    (\n",
    "                        ent.original_alias,\n",
    "                        ent.url,\n",
    "                        ent.description,\n",
    "                        get_wikidata_id_from_url(ent.url),\n",
    "                    )\n",
    "                )\n",
    "            item[\"linked_ents\"] = linked_ents\n",
    "            processed.append(item)\n",
    "\n",
    "# save to file\n",
    "with open(\"./train2.jsonl\", \"w\") as file:\n",
    "    for line in processed:\n",
    "        json.dump(line, file)\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rub/.local/lib/python3.10/site-packages/spacy/displacy/__init__.py:106: UserWarning: [W011] It looks like you're calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you're already running a local web server, so there's no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization.\n",
      "  warnings.warn(Warnings.W011)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Eu\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " rejects [\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    START_ENT] German\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " [END_ENT] call to boycott \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    British\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " lamb \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Peter Blackburn\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Brussels\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    1996 08\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    22\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    The European Commission\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " said on \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Thursday\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " it disagreed with \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    German\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " advice to consumers to shun \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    British\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " lamb until scientists determine whether mad cow disease can be transmitted to sheep \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Germany\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " s representative to \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the European Union\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " s veterinary committee \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Werner Zwingmann\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " said on \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Wednesday\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " consumers should buy sheepmeat from countries other than \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Britain\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " until the scientific advice was clearer We do n t support any such recommendation because we do n t see any grounds for it the \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Commission\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " s chief spokesman \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Nikolaus van der\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #f0d0ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Pas\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">WORK_OF_ART</span>\n",
       "</mark>\n",
       " told a news</div>\n",
       "</figure>\n",
       "</body>\n",
       "</html></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'ent' visualizer\n",
      "Serving on http://0.0.0.0:5001 ...\n",
      "\n",
      "Shutting down server on port 5001.\n"
     ]
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "doc = nlp(processed[0]['context'])\n",
    "displacy.serve(doc, style=\"ent\", port=5001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "\n",
    "def compute_clusters(vectors):\n",
    "    try:\n",
    "        labels = DBSCAN(eps=1, min_samples=2).fit_predict(vectors)\n",
    "        unique_labels = np.unique(labels)\n",
    "        vectors = np.array(vectors)\n",
    "        selected_label = unique_labels[0]\n",
    "        cluster_points = vectors[labels == selected_label]\n",
    "        intra_cluster_distances = pairwise_distances(cluster_points)\n",
    "        average_intra_cluster_distance = np.sum(intra_cluster_distances) / (\n",
    "            len(cluster_points) * (len(cluster_points) - 1)\n",
    "        )\n",
    "        min_cohesion = average_intra_cluster_distance\n",
    "        # Calculate intra-cluster distance for each cluster\n",
    "        for cluster_label in unique_labels:\n",
    "            if cluster_label == -1:\n",
    "                cluster_points = vectors[labels == cluster_label]\n",
    "                if (\n",
    "                    len(cluster_points) > 1\n",
    "                ):  # Ensure there are at least two points in the cluster for distance calculation\n",
    "                    intra_cluster_distances = pairwise_distances(cluster_points)\n",
    "                    average_intra_cluster_distance = np.sum(intra_cluster_distances) / (\n",
    "                        len(cluster_points) * (len(cluster_points) - 1)\n",
    "                    )\n",
    "                    if average_intra_cluster_distance < min_cohesion:\n",
    "                        min_cohesion = average_intra_cluster_distance\n",
    "                        selected_label = cluster_label\n",
    "                    # print(\n",
    "                    #     f\"Cluster {cluster_label}: Average Intra-Cluster Distance = {average_intra_cluster_distance}\"\n",
    "                    # )\n",
    "\n",
    "                    # print(\n",
    "                    #     f\"Cluster {cluster_label} has too few points for intra-cluster distance calculation.\"\n",
    "                    # )\n",
    "\n",
    "        # Get the indices of the items in the most common cluster\n",
    "        indices = np.arange(len(labels))  # Array of indices\n",
    "        most_common_cluster_indices = indices[labels == selected_label]\n",
    "        return most_common_cluster_indices\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from multiprocessing import Pool\n",
    "\n",
    "filepath = \"./train2.jsonl\"\n",
    "\n",
    "processed = []\n",
    "\n",
    "\n",
    "def get_wikidata_embedding(wikidata_id):\n",
    "    url = \"http://localhost:5000/api/vector/\"\n",
    "    response = requests.get(url + wikidata_id)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"vector\"]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "def getRelatedEntities(entry):\n",
    "    embeddings = []\n",
    "    for linked in entry[\"linked_ents\"]:\n",
    "        embedding = get_wikidata_embedding(linked[3])\n",
    "        if len(embedding) > 0:\n",
    "            embeddings.append(embedding)\n",
    "    related = compute_clusters(embeddings)\n",
    "    most_related = []\n",
    "    for index in related:\n",
    "        most_related.append(entry[\"linked_ents\"][index])\n",
    "    entry[\"most_related\"] = most_related\n",
    "    return entry\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with open(filepath, \"r\") as file:\n",
    "        dataset = []\n",
    "        processed = []\n",
    "        for line in tqdm(file):\n",
    "            dataset.append(json.loads(line))\n",
    "        \n",
    "        # with Pool(2) as p:\n",
    "        #     processed = list(tqdm(p.imap(getRelatedEntities, dataset), total=len(dataset)))\n",
    "        for line in tqdm(dataset):\n",
    "            processed.append(getRelatedEntities(line))\n",
    "        \n",
    "        \n",
    "with open(\"./train4.jsonl\", \"w\") as file:\n",
    "    for line in processed:\n",
    "        json.dump(line, file)\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getRelatedEntities(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 3, 4, 8])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_clusters(test_vects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative for non converted DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from spacy.pipeline.entity_linker import DEFAULT_NEL_MODEL\n",
    "from spacy.kb import KnowledgeBase\n",
    "\n",
    "nlp.add_pipe(\"entityLinker\", last=True)\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "def get_wikidata_id_from_url(url):\n",
    "    path = urlparse(url).path\n",
    "    wikidata_id = path.split(\"/\")[-1]\n",
    "    return wikidata_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [00:08, 32.04it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "filepath = \"./ace2004-test-kilt.jsonl\"\n",
    "\n",
    "processed = []\n",
    "\n",
    "with open(filepath, \"r\") as file:\n",
    "    for line in tqdm(file):\n",
    "        # Load each line as a JSON object\n",
    "        data_line = json.loads(line)\n",
    "\n",
    "        # Extract the text and the annotations\n",
    "        text = data_line[\"input\"]\n",
    "        doc = nlp(text)\n",
    "        linked_ents = []\n",
    "        ents = doc._.linkedEntities\n",
    "        for ent in ents:\n",
    "            linked_ents.append(\n",
    "                (\n",
    "                    ent.original_alias,\n",
    "                    ent.url,\n",
    "                    ent.description,\n",
    "                    get_wikidata_id_from_url(ent.url),\n",
    "                )\n",
    "            )\n",
    "        data_line[\"linked_ents\"] = linked_ents\n",
    "        processed.append(data_line)\n",
    "\n",
    "# save to file\n",
    "with open(\"./ace2004-nerjsonl\", \"w\") as file:\n",
    "    for line in processed:\n",
    "        json.dump(line, file)\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 652/656 [00:44<00:00, 13.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  0  1  2]\n",
      "21\n",
      "[-1  0  1  2]\n",
      "20\n",
      "[-1  0  1  2  3]\n",
      "19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 656/656 [00:44<00:00, 14.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  0  1  2]\n",
      "16\n",
      "[-1  0  1]\n",
      "17\n",
      "[-1  0  1]\n",
      "15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from multiprocessing import Pool\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import numpy as np\n",
    "\n",
    "filepath = \"./msnbc-test-kilt-ner.jsonl\"\n",
    "\n",
    "processed = []\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_clusters(vectors):\n",
    "    try:\n",
    "        labels = DBSCAN(eps=1, min_samples=2).fit_predict(vectors)\n",
    "        unique_labels = np.unique(labels)\n",
    "        print(unique_labels)\n",
    "        vectors = np.array(vectors)\n",
    "        selected_label = unique_labels[0]\n",
    "        cluster_points = vectors[labels == selected_label]\n",
    "        intra_cluster_distances = pairwise_distances(cluster_points)\n",
    "        average_intra_cluster_distance = np.sum(intra_cluster_distances) / (\n",
    "            len(cluster_points) * (len(cluster_points) - 1)\n",
    "        )\n",
    "        min_cohesion = average_intra_cluster_distance\n",
    "        # Calculate intra-cluster distance for each cluster\n",
    "        for cluster_label in unique_labels:\n",
    "            if cluster_label == -1:\n",
    "                cluster_points = vectors[labels == cluster_label]\n",
    "                print(len(cluster_points))\n",
    "                if (\n",
    "                    len(cluster_points) > 1\n",
    "                ):  # Ensure there are at least two points in the cluster for distance calculation\n",
    "                    intra_cluster_distances = pairwise_distances(cluster_points)\n",
    "                    average_intra_cluster_distance = np.sum(intra_cluster_distances) / (\n",
    "                        len(cluster_points) * (len(cluster_points) - 1)\n",
    "                    )\n",
    "                    if average_intra_cluster_distance < min_cohesion:\n",
    "                        min_cohesion = average_intra_cluster_distance\n",
    "                        selected_label = cluster_label\n",
    "                    # print(\n",
    "                    #     f\"Cluster {cluster_label}: Average Intra-Cluster Distance = {average_intra_cluster_distance}\"\n",
    "                    # )\n",
    "\n",
    "                    # print(\n",
    "                    #     f\"Cluster {cluster_label} has too few points for intra-cluster distance calculation.\"\n",
    "                    # )\n",
    "\n",
    "        # Get the indices of the items in the most common cluster\n",
    "        indices = np.arange(len(labels))  # Array of indices\n",
    "        most_common_cluster_indices = indices[labels == selected_label]\n",
    "        return most_common_cluster_indices\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []\n",
    "\n",
    "\n",
    "def get_wikidata_embedding(wikidata_id):\n",
    "    url = \"http://localhost:5000/api/vector/\"\n",
    "    response = requests.get(url + wikidata_id)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"vector\"]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "def getRelatedEntities(entry):\n",
    "    embeddings = []\n",
    "    for linked in entry[\"linked_ents\"]:\n",
    "        embedding = get_wikidata_embedding(linked[3])\n",
    "        if len(embedding) > 0:\n",
    "            embeddings.append(embedding)\n",
    "    related = compute_clusters(embeddings)\n",
    "    most_related = []\n",
    "    for index in related:\n",
    "        most_related.append(entry[\"linked_ents\"][index])\n",
    "    entry[\"most_related\"] = most_related\n",
    "    related_string = \"<additional> \"\n",
    "    for related in entry[\"linked_ents\"][:10]:\n",
    "        if len(related) == 4:\n",
    "            if related[2] is not None:\n",
    "                related_string += related[2] + \" \"\n",
    "    related_string += \"</additional>\"\n",
    "    entry[\"input\"] = entry[\"input\"] + related_string\n",
    "    return entry\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with open(filepath, \"r\") as file:\n",
    "        dataset = []\n",
    "        processed = []\n",
    "        for line in tqdm(file):\n",
    "            dataset.append(json.loads(line))\n",
    "\n",
    "        # with Pool(2) as p:\n",
    "        #     processed = list(tqdm(p.imap(getRelatedEntities, dataset), total=len(dataset)))\n",
    "        for line in tqdm(dataset):\n",
    "            processed.append(getRelatedEntities(line))\n",
    "\n",
    "\n",
    "with open(\"./msnbc-ner2.jsonl\", \"w\") as file:\n",
    "    for line in processed:\n",
    "        json.dump(line, file)\n",
    "        file.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
