{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d01515e8-c2c5-4f7e-bcae-6d8559f41239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # or \"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63af2596-b907-49cc-b6c3-2190e8af746a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wandb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtesi\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrobertaEnrichedLarge\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# args = TrainingArguments(\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wandb' is not defined"
     ]
    }
   ],
   "source": [
    "wandb.init(project=\"tesi\", name=\"robertaEnrichedLarge\")  # args = TrainingArguments(\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2de9e6eb-0958-42eb-807b-04e51fd71df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.0.4) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-large\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "909f890a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.0.4) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./robertaLargeInstanceOf/checkpoint-17500/\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "    \"./robertaLargeInstanceOf/checkpoint-17500/\",\n",
    "    return_dict=False,\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ec104d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.0.4) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "file_path = \"./train_converted.jsonl\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d7c8d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend 'Venite Ad Me Omnes'. Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9eb6616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n"
     ]
    }
   ],
   "source": [
    "print(test[514:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1362618-8cce-4759-8214-6f77876c33d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7939\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "tokenized = []\n",
    "file_path = \"./train4.jsonl\"\n",
    "from datasets import Dataset\n",
    "\n",
    "original_data = []\n",
    "with open(file_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        # Load each line as a JSON object\n",
    "        data_line = json.loads(line)\n",
    "        additional_question = \"<additional> \"\n",
    "        try:\n",
    "            for related in data_line[\"most_related\"]:\n",
    "                length = len(related)\n",
    "                if len(related) == 4:\n",
    "                    additional_question += related[2]\n",
    "                    additional_question += \" \"\n",
    "        except:\n",
    "            None\n",
    "        additional_question += \"</additional>\"\n",
    "        data_line[\"context\"] += additional_question\n",
    "        \n",
    "        original_data.append(data_line)\n",
    "print(len(original_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f04504f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18448\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "tokenized = []\n",
    "file_path = \"./aida_train_instanceof.jsonl\"\n",
    "from datasets import Dataset\n",
    "\n",
    "original_data_aida = []\n",
    "with open(file_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        # Load each line as a JSON object\n",
    "        data_line = json.loads(line)\n",
    "        # additional_question = \"<additional> \"\n",
    "        # try:\n",
    "        #     for related in data_line[\"most_related\"]:\n",
    "        #         length = len(related)\n",
    "        #         if len(related) == 4:\n",
    "        #             additional_question += related[2]\n",
    "        #             additional_question += \" \"\n",
    "        # except:\n",
    "        #     None\n",
    "        # additional_question += \"</additional>\"\n",
    "        # data_line[\"context\"] += additional_question\n",
    "        \n",
    "        original_data_aida.append(data_line)\n",
    "print(len(original_data_aida))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db6f78a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18448\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# random.shuffle(original_data_aida)\n",
    "# concat =original_data_aida[:int(len(original_data_aida)* 0.5)] + original_data\n",
    "# random.shuffle(concat)\n",
    "concat = [] + original_data_aida\n",
    "print(len(concat))\n",
    "train_original = concat[: int(len(concat) * 0.9)]\n",
    "eval_original = concat[int(len(concat) * 0.9) :]\n",
    "dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"context\": [item[\"question\"] for item in train_original],\n",
    "        \"question\": [item[\"context\"] for item in train_original],\n",
    "        \"answers\": [item[\"answers\"] for item in train_original],\n",
    "    }\n",
    ")\n",
    "dataset_ev = Dataset.from_dict(\n",
    "    {\n",
    "        \"context\": [item[\"question\"] for item in eval_original],\n",
    "        \"question\": [item[\"context\"] for item in eval_original],\n",
    "        \"answers\": [item[\"answers\"] for item in eval_original],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af2fb8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.0.4) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f94614f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-large\")\n",
    "\n",
    "\n",
    "def get_correct_alignement(context, answer):\n",
    "    \"\"\"Some original examples in SQuAD have indices wrong by 1 or 2 character. We test and fix this here.\"\"\"\n",
    "    gold_text = answer[\"text\"][0]\n",
    "\n",
    "    start_idx = context.find(gold_text)\n",
    "    end_idx = context.find(\"</ec>\", start_idx)\n",
    "    return start_idx, end_idx\n",
    "    # if context[start_idx:end_idx] == gold_text:\n",
    "    #     return start_idx, end_idx       # When the gold label position is good\n",
    "    # elif context[start_idx-1:end_idx-1] == gold_text:\n",
    "    #     return start_idx-1, end_idx-1   # When the gold label is off by one character\n",
    "    # elif context[start_idx-2:end_idx-2] == gold_text:\n",
    "    #     return start_idx-2, end_idx-2   # When the gold label is off by two character\n",
    "    # else:\n",
    "    #     raise ValueError()\n",
    "\n",
    "\n",
    "# Tokenize our training dataset\n",
    "def convert_to_features(example):\n",
    "    try:\n",
    "        # Tokenize contexts and questions (as pairs of inputs)\n",
    "\n",
    "        input_pairs = [example[\"question\"], example[\"context\"]]\n",
    "        encodings = tokenizer.encode_plus(\n",
    "            input_pairs, pad_to_max_length=True, truncation=\"only_second\"\n",
    "        )\n",
    "        context_encodings = tokenizer.encode_plus(example[\"context\"])\n",
    "\n",
    "        # Compute start and end tokens for labels using Transformers's fast tokenizers alignement methodes.\n",
    "        # this will give us the position of answer span in the context text\n",
    "\n",
    "        start_idx, end_idx = get_correct_alignement(\n",
    "            example[\"context\"], example[\"answers\"]\n",
    "        )\n",
    "        if end_idx != -1 and start_idx != -1:\n",
    "            try:\n",
    "                start_positions_context = context_encodings.char_to_token(start_idx)\n",
    "                end_positions_context = context_encodings.char_to_token(end_idx)\n",
    "\n",
    "                # here we will compute the start and end position of the answer in the whole example\n",
    "                # as the example is encoded like this <s> question</s></s> context</s>\n",
    "                # and we know the postion of the answer in the context\n",
    "                # we can just find out the index of the sep token and then add that to position + 1 (+1 because there are two sep tokens)\n",
    "                # this will give us the position of the answer span in whole example\n",
    "                sep_idx = encodings[\"input_ids\"].index(tokenizer.sep_token_id)\n",
    "                start_positions = start_positions_context + sep_idx + 1\n",
    "                end_positions = end_positions_context + sep_idx + 1\n",
    "               \n",
    "                encodings.update(\n",
    "                    {\n",
    "                        \"start_positions\": start_positions,\n",
    "                        \"ids\": encodings[\"input_ids\"],\n",
    "                        \"end_positions\": end_positions,\n",
    "                        \"attention_mask\": encodings[\"attention_mask\"],\n",
    "                        \"targets\": 1\n",
    "                        if example[\"answers\"][\"text\"][0] == \"Not In Candidates\"\n",
    "                        else 0,\n",
    "                    }\n",
    "                )\n",
    "                return encodings\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        else:\n",
    "            print(\"qu\")\n",
    "            encodings.update(\n",
    "                {\n",
    "                    \"start_positions\": start_idx,\n",
    "                    \"ids\": encodings[\"input_ids\"],\n",
    "                    \"end_positions\": end_idx,\n",
    "                    \"attention_mask\": encodings[\"attention_mask\"],\n",
    "                    \"targets\": 1\n",
    "                    if example[\"answers\"][\"text\"][0] == \"Not In Candidates\"\n",
    "                    else 0,\n",
    "                }\n",
    "            )\n",
    "            return encodings\n",
    "    except Exception as e:\n",
    "        print(\"errors\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "352ecd7d-1d89-4113-bb3c-0fd8837334a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08d6cc4223184feaa54d6847f27f8649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16603 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rub/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2393: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1216 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qu\n",
      "qu\n",
      "qu\n",
      "qu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76eca3cba5914af988ae88c147e7906b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1845 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_ds = dataset.map(convert_to_features)\n",
    "eval_ds = dataset_ev.map(convert_to_features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa934870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    RobertaForQuestionAnswering,\n",
    ")\n",
    "from transformers import Trainer\n",
    "\n",
    "import torch\n",
    "from torch.nn import BCELoss\n",
    "\n",
    "\n",
    "class CustomRobertaForQuestionAnswering(RobertaForQuestionAnswering):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.additional_output1 = nn.Linear(1024, 1024)\n",
    "        self.additional_output2 = nn.Linear(1024, 512)\n",
    "        self.additional_output3 = nn.Linear(512, 512)\n",
    "        self.additional_output4 = nn.Linear(512, 256)\n",
    "        self.additional_output5 = nn.Linear(256, 256)\n",
    "        self.additional_output6 = nn.Linear(256, 128)\n",
    "        self.additional_output7 = nn.Linear(128, 128)\n",
    "        self.additional_output8 = nn.Linear(128, 64)\n",
    "        self.additional_output9 = nn.Linear(64, 64)\n",
    "        self.additional_output10 = nn.Linear(64, 32)\n",
    "        self.additional_output11 = nn.Linear(32, 32)\n",
    "        self.additional_output12 = nn.Linear(32, 16)\n",
    "        self.additional_output13 = nn.Linear(16, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.additional_output1.weight.data.normal_(\n",
    "            mean=0.0, std=config.initializer_range\n",
    "        )\n",
    "        self.additional_output2.weight.data.normal_(\n",
    "            mean=0.0, std=config.initializer_range\n",
    "        )\n",
    "        self.additional_output3.weight.data.normal_(\n",
    "            mean=0.0, std=config.initializer_range\n",
    "        )\n",
    "        self.additional_output4.weight.data.normal_(\n",
    "            mean=0.0, std=config.initializer_range\n",
    "        )\n",
    "        self.additional_output5.weight.data.normal_(\n",
    "            mean=0.0, std=config.initializer_range\n",
    "        )\n",
    "        self.additional_output6.weight.data.normal_(\n",
    "            mean=0.0, std=config.initializer_range\n",
    "        )\n",
    "        if self.additional_output1.bias is not None:\n",
    "            self.additional_output1.bias.data.zero_()\n",
    "        if self.additional_output2.bias is not None:\n",
    "            self.additional_output2.bias.data.zero_()\n",
    "        if self.additional_output3.bias is not None:\n",
    "            self.additional_output3.bias.data.zero_()\n",
    "        if self.additional_output4.bias is not None:\n",
    "            self.additional_output4.bias.data.zero_()\n",
    "        if self.additional_output5.bias is not None:\n",
    "            self.additional_output5.bias.data.zero_()\n",
    "        if self.additional_output6.bias is not None:\n",
    "            self.additional_output6.bias.data.zero_()\n",
    "        if self.additional_output7.bias is not None:\n",
    "            self.additional_output7.bias.data.zero_()\n",
    "        if self.additional_output8.bias is not None:\n",
    "            self.additional_output8.bias.data.zero_()\n",
    "        if self.additional_output9.bias is not None:\n",
    "            self.additional_output9.bias.data.zero_()\n",
    "        if self.additional_output10.bias is not None:\n",
    "            self.additional_output10.bias.data.zero_()\n",
    "        if self.additional_output11.bias is not None:\n",
    "            self.additional_output11.bias.data.zero_()\n",
    "        if self.additional_output12.bias is not None:\n",
    "            self.additional_output12.bias.data.zero_()\n",
    "        if self.additional_output13.bias is not None:\n",
    "            self.additional_output13.bias.data.zero_()\n",
    "        self.additional_output1.weight.data.normal_(\n",
    "            mean=0.0, std=config.initializer_range\n",
    "        )\n",
    "        self.additional_output2.weight.data.normal_(\n",
    "            mean=0.0, std=config.initializer_range\n",
    "        )\n",
    "        self.additional_output3.weight.data.normal_(\n",
    "            mean=0.0, std=config.initializer_range\n",
    "        )\n",
    "        self.additional_output4.weight.data.normal_(\n",
    "            mean=0.0, std=config.initializer_range\n",
    "        )\n",
    "        self.additional_output5.weight.data.normal_(\n",
    "            mean=0.0, std=config.initializer_range\n",
    "        )\n",
    "        self.additional_output6.weight.data.normal_(\n",
    "            mean=0.0, std=config.initializer_range\n",
    "        )\n",
    "        self.additional_output7.weight.data.normal_(\n",
    "            mean=0.0, std=config.initializer_range\n",
    "        )\n",
    "        self.additional_output8.weight.data.normal_(\n",
    "            mean=0.0, std=config.initializer_range\n",
    "        )\n",
    "        self.additional_output9.weight.data.normal_(\n",
    "            mean=0.0, std=config.initializer_range\n",
    "        )\n",
    "        self.additional_output10.weight.data.normal_(\n",
    "            mean=0.0, std=config.initializer_range\n",
    "        )\n",
    "        self.additional_output11.weight.data.normal_(\n",
    "            mean=0.0, std=config.initializer_range\n",
    "        )\n",
    "        self.additional_output12.weight.data.normal_(\n",
    "            mean=0.0, std=config.initializer_range\n",
    "        )\n",
    "        self.additional_output13.weight.data.normal_(\n",
    "            mean=0.0, std=config.initializer_range\n",
    "        )\n",
    "       \n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        start_positions=None,\n",
    "        is_nil=None,\n",
    "        end_positions=None,\n",
    "    ):\n",
    "        outputs = super().forward(\n",
    "            input_ids,\n",
    "            attention_mask,\n",
    "            token_type_ids,\n",
    "            position_ids,\n",
    "            head_mask,\n",
    "            inputs_embeds,\n",
    "            start_positions,\n",
    "            end_positions,\n",
    "            output_hidden_states=True,\n",
    "        )\n",
    "        # print(outputs)\n",
    "        hidden_states = outputs.hidden_states\n",
    "        last_hidden_state = hidden_states[-1]\n",
    "        cls_last_hidden_state = last_hidden_state[:, 0, :]\n",
    "        additional_output = self.additional_output1(cls_last_hidden_state)\n",
    "        additional_output = self.relu(additional_output)\n",
    "        additional_output = self.additional_output2(additional_output)\n",
    "        additional_output = self.relu(additional_output)\n",
    "        additional_output = self.additional_output3(additional_output)\n",
    "        additional_output = self.relu(additional_output)\n",
    "        additional_output = self.additional_output4(additional_output)\n",
    "        additional_output = self.relu(additional_output)\n",
    "        additional_output = self.additional_output5(additional_output)\n",
    "        additional_output = self.relu(additional_output)\n",
    "        additional_output = self.additional_output6(additional_output)\n",
    "        additional_output = self.relu(additional_output)\n",
    "        additional_output = self.additional_output7(additional_output)\n",
    "        additional_output = self.relu(additional_output)\n",
    "        additional_output = self.additional_output8(additional_output)\n",
    "        additional_output = self.relu(additional_output)\n",
    "        additional_output = self.additional_output9(additional_output)\n",
    "        additional_output = self.relu(additional_output)\n",
    "        additional_output = self.additional_output10(additional_output)\n",
    "        additional_output = self.relu(additional_output)\n",
    "        additional_output = self.additional_output11(additional_output)\n",
    "        additional_output = self.relu(additional_output)\n",
    "        additional_output = self.additional_output12(additional_output)\n",
    "        additional_output = self.relu(additional_output)\n",
    "        additional_output = self.sigmoid(self.additional_output13(additional_output)).squeeze(-1)\n",
    "        \n",
    "\n",
    "        return (outputs, additional_output)\n",
    "\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        is_nil = inputs.pop(\"is_nil\").float()\n",
    "        outputs, additional_output = model(**inputs)\n",
    "        # Compute the original loss\n",
    "        loss = outputs.loss.mean()\n",
    "        # Compute the additional loss\n",
    "        additional_loss = nn.functional.binary_cross_entropy(\n",
    "            additional_output, is_nil, reduction=\"mean\", reduce=True\n",
    "        )\n",
    "\n",
    "        # Combine the losses\n",
    "        total_loss = loss + additional_loss\n",
    "        return (total_loss, outputs) if return_outputs else total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca745971-3963-42de-af93-1282e75daab2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "from transformers import default_data_collator\n",
    "\n",
    "import wandb\n",
    "\n",
    "# wandb.init(project=\"tesi\", name=\"robertaAidaLargeNERNIL\")  # args = TrainingArguments(\n",
    "#     f\"longformer-finetuned-squad\",\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=8,\n",
    "#     num_train_epochs=3,\n",
    "#     weight_decay=0.01,\n",
    "# )\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"robertaLargeInstanceOfBigBS\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    report_to=\"wandb\",\n",
    "    load_best_model_at_end=False,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=20,\n",
    "    eval_steps=20,\n",
    "    save_steps=20,\n",
    "    save_total_limit=1,\n",
    "    learning_rate=3e-5,\n",
    "    per_device_eval_batch_size=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=100,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.02,\n",
    "    fp16=True,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed30d3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d2e693",
   "metadata": {},
   "outputs": [],
   "source": [
    "!shutdown -h now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77766c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./RoBertAAida\n",
      "Configuration saved in ./RoBertAAida/config.json\n",
      "Model weights saved in ./RoBertAAida/pytorch_model.bin\n",
      "tokenizer config file saved in ./RoBertAAida/tokenizer_config.json\n",
      "Special tokens file saved in ./RoBertAAida/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('./RoBertAAida')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71805d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c6b3f211-be57-4c73-9a66-1d65c7dade62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': 'Give It All (Train song) </ec> Give It All (film) </ec> Give It All </ec>  NIL </ec>',\n",
       " 'question': \"t==Helen McNichol (Tania Raymonde), a high school senior, is in love with Stanford Prescott (Ryan Merriman), her jock boyfriend, and has decided to  [START_ENT] give it all [END_ENT] up to him. At least until she learns that her name is written in the high school football team's secret Bang Book and it is Stanford's j\",\n",
       " 'answers': {'answer_start': [75], 'text': ['NIL']}}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3e969f",
   "metadata": {},
   "source": [
    "## Prediction and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f99facfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_brackets(input_string):\n",
    "    stack = []\n",
    "    brackets = {\"(\": \")\", \"{\": \"}\", \"[\": \"]\"}\n",
    "    for char in input_string:\n",
    "        if char in brackets.keys():\n",
    "            stack.append(char)\n",
    "        elif char in brackets.values():\n",
    "            if not stack or brackets[stack.pop()] != char:\n",
    "                return False\n",
    "    return not stack\n",
    "\n",
    "\n",
    "def process_answer(answer, candidates):\n",
    "    if answer == \"Not In Candidates\":\n",
    "        return answer\n",
    "    else:\n",
    "        modified_answer = answer.split(\"</ec>\")[0]\n",
    "        modified_answer = answer.split(\": instance of \")[0]\n",
    "        modified_answer = answer.split(\": instance\")[0]\n",
    "        modified_answer = modified_answer.replace(\"<s>\", \"\")\n",
    "        modified_answer = modified_answer.replace(\"</s>\", \"\")\n",
    "        modified_answer = modified_answer.replace(\"<s\", \"\")\n",
    "        modified_answer = modified_answer.replace(\"</\", \"\")\n",
    "        modified_answer = modified_answer.replace(\" ec\", \"\")\n",
    "        modified_answer = modified_answer.replace(\">\", \"\")\n",
    "        modified_answer = modified_answer.strip()\n",
    "        if not check_brackets(modified_answer):\n",
    "            modified_answer = modified_answer.replace(\"(\", \"\")\n",
    "            modified_answer = modified_answer.replace(\")\", \"\")\n",
    "        if modified_answer == \"Not In Candidates\":\n",
    "            modified_answer = \"NIL\"\n",
    "        if modified_answer == \"\":\n",
    "            modified_answer = \"Not In Candidates\"\n",
    "        if modified_answer not in candidates:\n",
    "            modified_answer = \"Not In Candidates\"\n",
    "        return modified_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a64e8bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "tokenizer_classfier = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "classifier = RobertaForSequenceClassification.from_pretrained(\n",
    "    \"./robertaClassifier2/checkpoint-100/\", return_dict=False, num_labels=2\n",
    ").to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1555cdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def check_is_nil(input_pairs):\n",
    "    encodings_c = tokenizer.encode_plus(\n",
    "        input_pairs, return_tensors=\"pt\", truncation=\"only_second\"\n",
    "    ).to(\"cuda\")\n",
    "    outs = classifier(\n",
    "        encodings_c[\"input_ids\"], attention_mask=encodings_c[\"attention_mask\"]\n",
    "    )\n",
    "    outs = outs[0].cpu().detach().numpy()\n",
    "    if outs[0][0] < outs[0][1]:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def make_prediction(data_entry, nil_prediction):\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            question = data_entry[\"input\"]\n",
    "            context = \"\"\n",
    "            for item in data_entry[\"candidates\"]:\n",
    "                context += item + f\" </ec> \"\n",
    "            if nil_prediction:\n",
    "                context += \"Not In Candidates </ec>\"\n",
    "            input_pairs = [question, context]\n",
    "            encodings = tokenizer.encode_plus(\n",
    "                input_pairs, return_tensors=\"pt\", truncation=\"only_second\"\n",
    "            ).to(\"cuda\")\n",
    "            encodings_classifier = tokenizer_classfier.encode_plus(\n",
    "                input_pairs, return_tensors=\"pt\", truncation=\"only_second\"\n",
    "            ).to(\"cuda\")\n",
    "            is_nil = False\n",
    "            if nil_prediction:\n",
    "                is_nil = check_is_nil(input_pairs)\n",
    "\n",
    "                if is_nil:\n",
    "                    return {\n",
    "                        \"correct\": data_entry[\"output\"],\n",
    "                        \"non_processed\": \"Classified Not In Candidates\",\n",
    "                        \"predicted\": \"Not In Candidates\",\n",
    "                        \"input_phrase\": question,\n",
    "                        \"scores\": -1,\n",
    "                        \"candidates\": context,\n",
    "                    }\n",
    "            answer = \"\"\n",
    "            mean_score = 0\n",
    "\n",
    "            start_scores, end_scores = model(\n",
    "                encodings[\"input_ids\"], attention_mask=encodings[\"attention_mask\"]\n",
    "            )\n",
    "            start_scores = F.softmax(start_scores, dim=1)\n",
    "            end_scores = F.softmax(end_scores, dim=1)\n",
    "            # start_scores.to(\"cpu\")\n",
    "            # end_scores.to(\"cpu\")\n",
    "            start = torch.argmax(\n",
    "                start_scores\n",
    "            )  # Get the most likely beginning of answer with the argmax of the score\n",
    "            end = (\n",
    "                torch.argmax(end_scores) + 1\n",
    "            )  # Get the most likely end of answer with the argmax of the score\n",
    "            mean_score = 0\n",
    "            try:\n",
    "                mean_score = end_scores[0][end.item()] + start_scores[0][start.item()]\n",
    "            except:\n",
    "                None\n",
    "            answer_tokens = encodings[\"input_ids\"][0, start.item() : end.item() + 1]\n",
    "            answer = \"\"\n",
    "\n",
    "            answer = process_answer(tokenizer.decode(answer_tokens), context)\n",
    "\n",
    "            del encodings\n",
    "            del encodings_classifier\n",
    "            if not is_nil:\n",
    "                del start_scores\n",
    "                del end_scores\n",
    "                del start\n",
    "                del end\n",
    "\n",
    "            score = float(mean_score)\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            return {\n",
    "                \"correct\": data_entry[\"output\"],\n",
    "                \"non_processed\": tokenizer.decode(answer_tokens)\n",
    "                if not is_nil\n",
    "                else None,\n",
    "                \"predicted\": answer,\n",
    "                \"input_phrase\": question,\n",
    "                \"scores\": score,\n",
    "                \"candidates\": context,\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return {\n",
    "                \"correct\": data_entry[\"output\"],\n",
    "                \"non_processed\": None,\n",
    "                \"predicted\": \"Not In Candidates\",\n",
    "                \"input_phrase\": question,\n",
    "                \"scores\": 0,\n",
    "                \"candidates\": context,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7871c50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.0.4) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-large\")\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"./robertaAidaLargeNERNIL_SpecialNIL2/checkpoint-600\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3ee371f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 16/656 [00:01<00:36, 17.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truncation error: Sequence to truncate too short to respect the provided max_length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 395/656 [00:24<00:16, 15.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truncation error: Sequence to truncate too short to respect the provided max_length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 656/656 [00:41<00:00, 15.81it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "import torch\n",
    "\n",
    "results = []\n",
    "file_path = \"./msnbc_nilled.jsonl\"\n",
    "dataset = []\n",
    "with open(file_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        # Load each line as a JSON object\n",
    "        data_line = json.loads(line)\n",
    "\n",
    "        dataset.append(data_line)\n",
    "\n",
    "for item in tqdm(dataset):\n",
    "    pred = make_prediction(item, True)\n",
    "    results.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b5ba25a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 656/656 [00:00<00:00, 1068529.49it/s]\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "correct_trace = []\n",
    "wrong = []\n",
    "correct_nil = 0\n",
    "wrong_nil = 0\n",
    "full_data = []\n",
    "for result in tqdm(results):\n",
    "    processed = result[\"predicted\"]\n",
    "    if processed == result[\"correct\"][0][\"answer\"]:\n",
    "        correct += 1\n",
    "        correct_trace.append(result)\n",
    "        if processed == \"Not In Candidates\":\n",
    "            correct_nil += 1\n",
    "\n",
    "    else:\n",
    "        if result[\"correct\"][0][\"answer\"] == \"Not In Candidates\":\n",
    "            wrong_nil += 1\n",
    "        wrong.append(result)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b2024e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of the model is 0.41615853658536583\n",
      "accuracy in nil prediction is: 0.9496124031007752\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy of the model is {correct/len(results)}\")\n",
    "print(f\"accuracy in nil prediction is: {correct_nil/(correct_nil+wrong_nil)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b285bc-679f-41a8-9859-3f594a4b276c",
   "metadata": {},
   "source": [
    "- Base: 65.9%\n",
    "- Large: 73.21%(enriched) - 74.65% (standard)\n",
    "- Large EN: 76.66%\n",
    "- large NER: 75.5%\n",
    "- Base-NER: 66.35%\n",
    "\n",
    "## MSNBC large\n",
    "- No EN: 91.92%\n",
    "- Large NIL-NER: 87.65% / 88.87%\n",
    "- RobertaAidaLargeNER: 93% on base dataset\n",
    "- RobertaAidaLargeNER finetuned on instance of: 92.98% - 92.37 on base dataset\n",
    "\n",
    "## NIL EL\n",
    "- NER + AIDA + Clustering  : 63.10%\n",
    "- NER + AIDA + Clustering OpenAi  : 64.61%\n",
    "\n",
    "## ace2004\n",
    "- NO NER: 79.76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba3a0e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1197d1e9-3e52-4ca4-a959-57f6751f6c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(wrong)\n",
    "df_c = pd.DataFrame(correct_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "43c3df84-751a-4ade-9364-a9078478e2a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correct</th>\n",
       "      <th>non_processed</th>\n",
       "      <th>predicted</th>\n",
       "      <th>input_phrase</th>\n",
       "      <th>scores</th>\n",
       "      <th>candidates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'answer': 'Not In Candidates', 'provenance':...</td>\n",
       "      <td>&lt;/s&gt;Celebrity Fit Club (U.S. TV series) &lt;/ec</td>\n",
       "      <td>Celebrity Fit Club (U.S. TV series)</td>\n",
       "      <td>Timberlake Diaz reportedly break up Former N S...</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>Celebrity Fit Club (U.S. TV series) &lt;/ec&gt; Not ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'answer': 'Not In Candidates', 'provenance':...</td>\n",
       "      <td>&lt;/s&gt;Celebrity Fit Club (U.S. TV series) &lt;/ec</td>\n",
       "      <td>Celebrity Fit Club (U.S. TV series)</td>\n",
       "      <td>Timberlake Diaz reportedly break up Former N S...</td>\n",
       "      <td>0.999944</td>\n",
       "      <td>Celebrity Fit Club (U.S. TV series) &lt;/ec&gt; Not ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'answer': 'Not In Candidates', 'provenance':...</td>\n",
       "      <td>&lt;/s&gt;Iraqi Armed Forces &lt;/ec</td>\n",
       "      <td>Iraqi Armed Forces</td>\n",
       "      <td>execution that was leaked I can officially now...</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>Iraqi Armed Forces &lt;/ec&gt; Iraqi security forces...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'answer': 'Not In Candidates', 'provenance':...</td>\n",
       "      <td>&lt;/s&gt;New York &lt;/ec</td>\n",
       "      <td>New York</td>\n",
       "      <td>Barbara Walters stands by Rosie O Donnell View...</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>New York &lt;/ec&gt; New York (magazine) &lt;/ec&gt; Unite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'answer': 'Not In Candidates', 'provenance':...</td>\n",
       "      <td>&lt;/s&gt;Las Vegas–Paradise, NV MSA &lt;/ec</td>\n",
       "      <td>Las Vegas–Paradise, NV MSA</td>\n",
       "      <td>Muslim with U S family is held turned away Aut...</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>Las Vegas–Paradise, NV MSA &lt;/ec&gt; Las Vegas Str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[{'answer': 'Not In Candidates', 'provenance':...</td>\n",
       "      <td>&gt; California State University, Bakersfield &lt;/ec</td>\n",
       "      <td>California State University, Bakersfield</td>\n",
       "      <td>Muslim with U S family is held turned away Aut...</td>\n",
       "      <td>0.999889</td>\n",
       "      <td>Bakersfield, Vermont &lt;/ec&gt; California State Un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[{'answer': 'West Virginia', 'provenance': [{'...</td>\n",
       "      <td>Classified Not In Candidates</td>\n",
       "      <td>Not In Candidates</td>\n",
       "      <td>U S mines still not safe enough experts say On...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>Not In Candidates &lt;/ec&gt; Not In Candidates &lt;/ec&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[{'answer': 'Not In Candidates', 'provenance':...</td>\n",
       "      <td>&gt; Alabama Crimson Tide football &lt;/ec</td>\n",
       "      <td>Alabama Crimson Tide football</td>\n",
       "      <td>worth an estimated 32 million plus incentives ...</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>Alabama &lt;/ec&gt; Alabama (band) &lt;/ec&gt; Alabama Cri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[{'answer': 'West Virginia', 'provenance': [{'...</td>\n",
       "      <td>Classified Not In Candidates</td>\n",
       "      <td>Not In Candidates</td>\n",
       "      <td>U S mines still not safe enough experts say On...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>Not In Candidates &lt;/ec&gt; Not In Candidates &lt;/ec&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[{'answer': 'Not In Candidates', 'provenance':...</td>\n",
       "      <td>&lt;/s&gt;Star &lt;/ec</td>\n",
       "      <td>Star</td>\n",
       "      <td>Timberlake Diaz reportedly break up Former N S...</td>\n",
       "      <td>0.617034</td>\n",
       "      <td>Star &lt;/ec&gt; Star (sailboat) &lt;/ec&gt; Star, Idaho &lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[{'answer': 'Innosense', 'provenance': [{'titl...</td>\n",
       "      <td>&gt; Finland &lt;/ec&gt; Finns &lt;/ec</td>\n",
       "      <td>Not In Candidates</td>\n",
       "      <td>with her family in Vail Colo while Timberlake ...</td>\n",
       "      <td>0.998689</td>\n",
       "      <td>Finn (dinghy) &lt;/ec&gt; Finn Hudson &lt;/ec&gt; Finland ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[{'answer': 'Not In Candidates', 'provenance':...</td>\n",
       "      <td>&lt;/s&gt;Germany national football team &lt;/ec</td>\n",
       "      <td>Germany national football team</td>\n",
       "      <td>cents or 2 3 percent to 42 30 Home Depot fell ...</td>\n",
       "      <td>0.177958</td>\n",
       "      <td>Germany national football team &lt;/ec&gt; Nazi Germ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[{'answer': 'Christmas Holiday', 'provenance':...</td>\n",
       "      <td>Classified Not In Candidates</td>\n",
       "      <td>Not In Candidates</td>\n",
       "      <td>Thousands of airline passengers still have not...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>Christmas &lt;/ec&gt; Christmas and holiday season &lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[{'answer': 'Not In Candidates', 'provenance':...</td>\n",
       "      <td>&lt;/s&gt;Celebrity Fit Club (U.S. TV series) &lt;/ec</td>\n",
       "      <td>Celebrity Fit Club (U.S. TV series)</td>\n",
       "      <td>Timberlake [START_ENT] Diaz [END_ENT] reported...</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>Celebrity Fit Club (U.S. TV series) &lt;/ec&gt; Not ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[{'answer': 'Not In Candidates', 'provenance':...</td>\n",
       "      <td>&gt; Justin Timberlake discography &lt;/ec</td>\n",
       "      <td>Justin Timberlake discography</td>\n",
       "      <td>Timberlake Diaz reportedly break up Former N S...</td>\n",
       "      <td>0.303225</td>\n",
       "      <td>The 2007 FutureSex/LoveShow &lt;/ec&gt; Justin Timbe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[{'answer': 'Not In Candidates', 'provenance':...</td>\n",
       "      <td>&lt;/s&gt;Michigan State University &lt;/ec</td>\n",
       "      <td>Michigan State University</td>\n",
       "      <td>be his defensive coordinator according to Semi...</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>Michigan State University &lt;/ec&gt; Michigan State...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[{'answer': 'Not In Candidates', 'provenance':...</td>\n",
       "      <td>&lt;/s&gt;Asshole &lt;/ec</td>\n",
       "      <td>Asshole</td>\n",
       "      <td>Oil rose 24 cents to settle at 79 14 per barre...</td>\n",
       "      <td>0.932826</td>\n",
       "      <td>Asshole &lt;/ec&gt; Not In Candidates &lt;/ec&gt; Not In C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[{'answer': 'Detroit Pistons', 'provenance': [...</td>\n",
       "      <td>Classified Not In Candidates</td>\n",
       "      <td>Not In Candidates</td>\n",
       "      <td>following a loss in Chicago What I do in the l...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>Detroit Pistons &lt;/ec&gt; 2003–04 Detroit Pistons ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[{'answer': 'Mars', 'provenance': [{'title': '...</td>\n",
       "      <td>Classified Not In Candidates</td>\n",
       "      <td>Not In Candidates</td>\n",
       "      <td>ahead when faced with an obstacle allowing the...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>Mars &lt;/ec&gt; Mars (mythology) &lt;/ec&gt; Planets in a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[{'answer': 'Mike DuBose', 'provenance': [{'ti...</td>\n",
       "      <td>Classified Not In Candidates</td>\n",
       "      <td>Not In Candidates</td>\n",
       "      <td>Moore put it are why the Tide was willing to o...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>Mike DuBose &lt;/ec&gt; Not In Candidates &lt;/ec&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              correct  \\\n",
       "0   [{'answer': 'Not In Candidates', 'provenance':...   \n",
       "1   [{'answer': 'Not In Candidates', 'provenance':...   \n",
       "2   [{'answer': 'Not In Candidates', 'provenance':...   \n",
       "3   [{'answer': 'Not In Candidates', 'provenance':...   \n",
       "4   [{'answer': 'Not In Candidates', 'provenance':...   \n",
       "5   [{'answer': 'Not In Candidates', 'provenance':...   \n",
       "6   [{'answer': 'West Virginia', 'provenance': [{'...   \n",
       "7   [{'answer': 'Not In Candidates', 'provenance':...   \n",
       "8   [{'answer': 'West Virginia', 'provenance': [{'...   \n",
       "9   [{'answer': 'Not In Candidates', 'provenance':...   \n",
       "10  [{'answer': 'Innosense', 'provenance': [{'titl...   \n",
       "11  [{'answer': 'Not In Candidates', 'provenance':...   \n",
       "12  [{'answer': 'Christmas Holiday', 'provenance':...   \n",
       "13  [{'answer': 'Not In Candidates', 'provenance':...   \n",
       "14  [{'answer': 'Not In Candidates', 'provenance':...   \n",
       "15  [{'answer': 'Not In Candidates', 'provenance':...   \n",
       "16  [{'answer': 'Not In Candidates', 'provenance':...   \n",
       "17  [{'answer': 'Detroit Pistons', 'provenance': [...   \n",
       "18  [{'answer': 'Mars', 'provenance': [{'title': '...   \n",
       "19  [{'answer': 'Mike DuBose', 'provenance': [{'ti...   \n",
       "\n",
       "                                      non_processed  \\\n",
       "0      </s>Celebrity Fit Club (U.S. TV series) </ec   \n",
       "1      </s>Celebrity Fit Club (U.S. TV series) </ec   \n",
       "2                       </s>Iraqi Armed Forces </ec   \n",
       "3                                 </s>New York </ec   \n",
       "4               </s>Las Vegas–Paradise, NV MSA </ec   \n",
       "5   > California State University, Bakersfield </ec   \n",
       "6                      Classified Not In Candidates   \n",
       "7              > Alabama Crimson Tide football </ec   \n",
       "8                      Classified Not In Candidates   \n",
       "9                                     </s>Star </ec   \n",
       "10                       > Finland </ec> Finns </ec   \n",
       "11          </s>Germany national football team </ec   \n",
       "12                     Classified Not In Candidates   \n",
       "13     </s>Celebrity Fit Club (U.S. TV series) </ec   \n",
       "14             > Justin Timberlake discography </ec   \n",
       "15               </s>Michigan State University </ec   \n",
       "16                                 </s>Asshole </ec   \n",
       "17                     Classified Not In Candidates   \n",
       "18                     Classified Not In Candidates   \n",
       "19                     Classified Not In Candidates   \n",
       "\n",
       "                                   predicted  \\\n",
       "0        Celebrity Fit Club (U.S. TV series)   \n",
       "1        Celebrity Fit Club (U.S. TV series)   \n",
       "2                         Iraqi Armed Forces   \n",
       "3                                   New York   \n",
       "4                 Las Vegas–Paradise, NV MSA   \n",
       "5   California State University, Bakersfield   \n",
       "6                          Not In Candidates   \n",
       "7              Alabama Crimson Tide football   \n",
       "8                          Not In Candidates   \n",
       "9                                       Star   \n",
       "10                         Not In Candidates   \n",
       "11            Germany national football team   \n",
       "12                         Not In Candidates   \n",
       "13       Celebrity Fit Club (U.S. TV series)   \n",
       "14             Justin Timberlake discography   \n",
       "15                 Michigan State University   \n",
       "16                                   Asshole   \n",
       "17                         Not In Candidates   \n",
       "18                         Not In Candidates   \n",
       "19                         Not In Candidates   \n",
       "\n",
       "                                         input_phrase    scores  \\\n",
       "0   Timberlake Diaz reportedly break up Former N S...  0.999980   \n",
       "1   Timberlake Diaz reportedly break up Former N S...  0.999944   \n",
       "2   execution that was leaked I can officially now...  0.999996   \n",
       "3   Barbara Walters stands by Rosie O Donnell View...  0.999996   \n",
       "4   Muslim with U S family is held turned away Aut...  0.999996   \n",
       "5   Muslim with U S family is held turned away Aut...  0.999889   \n",
       "6   U S mines still not safe enough experts say On... -1.000000   \n",
       "7   worth an estimated 32 million plus incentives ...  0.999997   \n",
       "8   U S mines still not safe enough experts say On... -1.000000   \n",
       "9   Timberlake Diaz reportedly break up Former N S...  0.617034   \n",
       "10  with her family in Vail Colo while Timberlake ...  0.998689   \n",
       "11  cents or 2 3 percent to 42 30 Home Depot fell ...  0.177958   \n",
       "12  Thousands of airline passengers still have not... -1.000000   \n",
       "13  Timberlake [START_ENT] Diaz [END_ENT] reported...  0.999989   \n",
       "14  Timberlake Diaz reportedly break up Former N S...  0.303225   \n",
       "15  be his defensive coordinator according to Semi...  0.999997   \n",
       "16  Oil rose 24 cents to settle at 79 14 per barre...  0.932826   \n",
       "17  following a loss in Chicago What I do in the l... -1.000000   \n",
       "18  ahead when faced with an obstacle allowing the... -1.000000   \n",
       "19  Moore put it are why the Tide was willing to o... -1.000000   \n",
       "\n",
       "                                           candidates  \n",
       "0   Celebrity Fit Club (U.S. TV series) </ec> Not ...  \n",
       "1   Celebrity Fit Club (U.S. TV series) </ec> Not ...  \n",
       "2   Iraqi Armed Forces </ec> Iraqi security forces...  \n",
       "3   New York </ec> New York (magazine) </ec> Unite...  \n",
       "4   Las Vegas–Paradise, NV MSA </ec> Las Vegas Str...  \n",
       "5   Bakersfield, Vermont </ec> California State Un...  \n",
       "6     Not In Candidates </ec> Not In Candidates </ec>  \n",
       "7   Alabama </ec> Alabama (band) </ec> Alabama Cri...  \n",
       "8     Not In Candidates </ec> Not In Candidates </ec>  \n",
       "9   Star </ec> Star (sailboat) </ec> Star, Idaho <...  \n",
       "10  Finn (dinghy) </ec> Finn Hudson </ec> Finland ...  \n",
       "11  Germany national football team </ec> Nazi Germ...  \n",
       "12  Christmas </ec> Christmas and holiday season <...  \n",
       "13  Celebrity Fit Club (U.S. TV series) </ec> Not ...  \n",
       "14  The 2007 FutureSex/LoveShow </ec> Justin Timbe...  \n",
       "15  Michigan State University </ec> Michigan State...  \n",
       "16  Asshole </ec> Not In Candidates </ec> Not In C...  \n",
       "17  Detroit Pistons </ec> 2003–04 Detroit Pistons ...  \n",
       "18  Mars </ec> Mars (mythology) </ec> Planets in a...  \n",
       "19          Mike DuBose </ec> Not In Candidates </ec>  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 20\n",
    "offset = 0\n",
    "df.iloc[offset * n : offset * n + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "67de164f-a8a4-4241-b9a6-3b88323bd64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ConnectionError), entering retry loop.\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(\"./errorsNil.csv\")\n",
    "df_c.to_csv(\"./correctNil.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "391ade97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.0.4) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "Couldn't reach http://www.derczynski.com/resources/ipm_nel.tar.gz (ConnectTimeout(MaxRetryError(\"HTTPConnectionPool(host='www.derczynski.com', port=80): Max retries exceeded with url: /resources/ipm_nel.tar.gz (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7f654873f220>, 'Connection to www.derczynski.com timed out. (connect timeout=100)'))\")))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_dataset\n\u001b[0;32m----> 3\u001b[0m dataset \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39;49m\u001b[39mstrombergnlp/ipm_nel\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/load.py:2130\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   2127\u001b[0m try_from_hf_gcs \u001b[39m=\u001b[39m path \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[1;32m   2129\u001b[0m \u001b[39m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 2130\u001b[0m builder_instance\u001b[39m.\u001b[39;49mdownload_and_prepare(\n\u001b[1;32m   2131\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   2132\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   2133\u001b[0m     verification_mode\u001b[39m=\u001b[39;49mverification_mode,\n\u001b[1;32m   2134\u001b[0m     try_from_hf_gcs\u001b[39m=\u001b[39;49mtry_from_hf_gcs,\n\u001b[1;32m   2135\u001b[0m     num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[1;32m   2136\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   2137\u001b[0m )\n\u001b[1;32m   2139\u001b[0m \u001b[39m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   2140\u001b[0m keep_in_memory \u001b[39m=\u001b[39m (\n\u001b[1;32m   2141\u001b[0m     keep_in_memory \u001b[39mif\u001b[39;00m keep_in_memory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m is_small_dataset(builder_instance\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size)\n\u001b[1;32m   2142\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/builder.py:954\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    952\u001b[0m     \u001b[39mif\u001b[39;00m num_proc \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    953\u001b[0m         prepare_split_kwargs[\u001b[39m\"\u001b[39m\u001b[39mnum_proc\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m num_proc\n\u001b[0;32m--> 954\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_download_and_prepare(\n\u001b[1;32m    955\u001b[0m         dl_manager\u001b[39m=\u001b[39;49mdl_manager,\n\u001b[1;32m    956\u001b[0m         verification_mode\u001b[39m=\u001b[39;49mverification_mode,\n\u001b[1;32m    957\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mprepare_split_kwargs,\n\u001b[1;32m    958\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdownload_and_prepare_kwargs,\n\u001b[1;32m    959\u001b[0m     )\n\u001b[1;32m    960\u001b[0m \u001b[39m# Sync info\u001b[39;00m\n\u001b[1;32m    961\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(split\u001b[39m.\u001b[39mnum_bytes \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39msplits\u001b[39m.\u001b[39mvalues())\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/builder.py:1717\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_splits_kwargs)\u001b[0m\n\u001b[1;32m   1716\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_download_and_prepare\u001b[39m(\u001b[39mself\u001b[39m, dl_manager, verification_mode, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprepare_splits_kwargs):\n\u001b[0;32m-> 1717\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_download_and_prepare(\n\u001b[1;32m   1718\u001b[0m         dl_manager,\n\u001b[1;32m   1719\u001b[0m         verification_mode,\n\u001b[1;32m   1720\u001b[0m         check_duplicate_keys\u001b[39m=\u001b[39;49mverification_mode \u001b[39m==\u001b[39;49m VerificationMode\u001b[39m.\u001b[39;49mBASIC_CHECKS\n\u001b[1;32m   1721\u001b[0m         \u001b[39mor\u001b[39;49;00m verification_mode \u001b[39m==\u001b[39;49m VerificationMode\u001b[39m.\u001b[39;49mALL_CHECKS,\n\u001b[1;32m   1722\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mprepare_splits_kwargs,\n\u001b[1;32m   1723\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/builder.py:1027\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m   1025\u001b[0m split_dict \u001b[39m=\u001b[39m SplitDict(dataset_name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset_name)\n\u001b[1;32m   1026\u001b[0m split_generators_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_split_generators_kwargs(prepare_split_kwargs)\n\u001b[0;32m-> 1027\u001b[0m split_generators \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_split_generators(dl_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msplit_generators_kwargs)\n\u001b[1;32m   1029\u001b[0m \u001b[39m# Checksums verification\u001b[39;00m\n\u001b[1;32m   1030\u001b[0m \u001b[39mif\u001b[39;00m verification_mode \u001b[39m==\u001b[39m VerificationMode\u001b[39m.\u001b[39mALL_CHECKS \u001b[39mand\u001b[39;00m dl_manager\u001b[39m.\u001b[39mrecord_checksums:\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/datasets_modules/datasets/strombergnlp--ipm_nel/c92a92b7e511bb25ace34b25db0e6d95db94fda1bf36f1333b0c1a13303d6f14/ipm_nel.py:127\u001b[0m, in \u001b[0;36mIpmNel2003._split_generators\u001b[0;34m(self, dl_manager)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_split_generators\u001b[39m(\u001b[39mself\u001b[39m, dl_manager):\n\u001b[1;32m    126\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Returns SplitGenerators.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m     downloaded_file \u001b[39m=\u001b[39m dl_manager\u001b[39m.\u001b[39;49mdownload_and_extract(_URL)\n\u001b[1;32m    128\u001b[0m     data_files \u001b[39m=\u001b[39m {\n\u001b[1;32m    129\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m: os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(downloaded_file, _TRAINING_FILE),\n\u001b[1;32m    130\u001b[0m     }\n\u001b[1;32m    132\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m    133\u001b[0m         datasets\u001b[39m.\u001b[39mSplitGenerator(name\u001b[39m=\u001b[39mdatasets\u001b[39m.\u001b[39mSplit\u001b[39m.\u001b[39mTRAIN, gen_kwargs\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mfilepath\u001b[39m\u001b[39m\"\u001b[39m: data_files[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m]}),\n\u001b[1;32m    134\u001b[0m     ]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/download/download_manager.py:564\u001b[0m, in \u001b[0;36mDownloadManager.download_and_extract\u001b[0;34m(self, url_or_urls)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdownload_and_extract\u001b[39m(\u001b[39mself\u001b[39m, url_or_urls):\n\u001b[1;32m    549\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Download and extract given `url_or_urls`.\u001b[39;00m\n\u001b[1;32m    550\u001b[0m \n\u001b[1;32m    551\u001b[0m \u001b[39m    Is roughly equivalent to:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[39m        extracted_path(s): `str`, extracted paths of given URL(s).\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 564\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mextract(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdownload(url_or_urls))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/download/download_manager.py:427\u001b[0m, in \u001b[0;36mDownloadManager.download\u001b[0;34m(self, url_or_urls)\u001b[0m\n\u001b[1;32m    424\u001b[0m download_func \u001b[39m=\u001b[39m partial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_download, download_config\u001b[39m=\u001b[39mdownload_config)\n\u001b[1;32m    426\u001b[0m start_time \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow()\n\u001b[0;32m--> 427\u001b[0m downloaded_path_or_paths \u001b[39m=\u001b[39m map_nested(\n\u001b[1;32m    428\u001b[0m     download_func,\n\u001b[1;32m    429\u001b[0m     url_or_urls,\n\u001b[1;32m    430\u001b[0m     map_tuple\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    431\u001b[0m     num_proc\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mnum_proc,\n\u001b[1;32m    432\u001b[0m     disable_tqdm\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m is_progress_bar_enabled(),\n\u001b[1;32m    433\u001b[0m     desc\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mDownloading data files\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    434\u001b[0m )\n\u001b[1;32m    435\u001b[0m duration \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow() \u001b[39m-\u001b[39m start_time\n\u001b[1;32m    436\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDownloading took \u001b[39m\u001b[39m{\u001b[39;00mduration\u001b[39m.\u001b[39mtotal_seconds()\u001b[39m \u001b[39m\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m \u001b[39m\u001b[39m60\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m min\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/utils/py_utils.py:455\u001b[0m, in \u001b[0;36mmap_nested\u001b[0;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, types, disable_tqdm, desc)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[39m# Singleton\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data_struct, \u001b[39mdict\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data_struct, types):\n\u001b[0;32m--> 455\u001b[0m     \u001b[39mreturn\u001b[39;00m function(data_struct)\n\u001b[1;32m    457\u001b[0m disable_tqdm \u001b[39m=\u001b[39m disable_tqdm \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled()\n\u001b[1;32m    458\u001b[0m iterable \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(data_struct\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_struct, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m data_struct\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/download/download_manager.py:453\u001b[0m, in \u001b[0;36mDownloadManager._download\u001b[0;34m(self, url_or_filename, download_config)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[39mif\u001b[39;00m is_relative_path(url_or_filename):\n\u001b[1;32m    451\u001b[0m     \u001b[39m# append the relative path to the base_path\u001b[39;00m\n\u001b[1;32m    452\u001b[0m     url_or_filename \u001b[39m=\u001b[39m url_or_path_join(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_base_path, url_or_filename)\n\u001b[0;32m--> 453\u001b[0m \u001b[39mreturn\u001b[39;00m cached_path(url_or_filename, download_config\u001b[39m=\u001b[39;49mdownload_config)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/utils/file_utils.py:181\u001b[0m, in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, download_config, **download_kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m     url_or_filename \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(url_or_filename)\n\u001b[1;32m    179\u001b[0m \u001b[39mif\u001b[39;00m is_remote_url(url_or_filename):\n\u001b[1;32m    180\u001b[0m     \u001b[39m# URL, so get it from the cache (downloading if necessary)\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m     output_path \u001b[39m=\u001b[39m get_from_cache(\n\u001b[1;32m    182\u001b[0m         url_or_filename,\n\u001b[1;32m    183\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    184\u001b[0m         force_download\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mforce_download,\n\u001b[1;32m    185\u001b[0m         proxies\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mproxies,\n\u001b[1;32m    186\u001b[0m         resume_download\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mresume_download,\n\u001b[1;32m    187\u001b[0m         user_agent\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49muser_agent,\n\u001b[1;32m    188\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mlocal_files_only,\n\u001b[1;32m    189\u001b[0m         use_etag\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49muse_etag,\n\u001b[1;32m    190\u001b[0m         max_retries\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    191\u001b[0m         token\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mtoken,\n\u001b[1;32m    192\u001b[0m         ignore_url_params\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mignore_url_params,\n\u001b[1;32m    193\u001b[0m         storage_options\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mstorage_options,\n\u001b[1;32m    194\u001b[0m         download_desc\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mdownload_desc,\n\u001b[1;32m    195\u001b[0m     )\n\u001b[1;32m    196\u001b[0m \u001b[39melif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(url_or_filename):\n\u001b[1;32m    197\u001b[0m     \u001b[39m# File, and it exists.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m     output_path \u001b[39m=\u001b[39m url_or_filename\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/utils/file_utils.py:572\u001b[0m, in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag, max_retries, token, ignore_url_params, storage_options, download_desc)\u001b[0m\n\u001b[1;32m    570\u001b[0m _raise_if_offline_mode_is_enabled(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTried to reach \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    571\u001b[0m \u001b[39mif\u001b[39;00m head_error \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 572\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt reach \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(head_error)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    573\u001b[0m \u001b[39melif\u001b[39;00m response \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    574\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt reach \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m (error \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mConnectionError\u001b[0m: Couldn't reach http://www.derczynski.com/resources/ipm_nel.tar.gz (ConnectTimeout(MaxRetryError(\"HTTPConnectionPool(host='www.derczynski.com', port=80): Max retries exceeded with url: /resources/ipm_nel.tar.gz (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7f654873f220>, 'Connection to www.derczynski.com timed out. (connect timeout=100)'))\")))"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"strombergnlp/ipm_nel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f3e7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
